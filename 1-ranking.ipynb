{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use LearnBPR user ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import struct\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basedir = '/home/tvromen/research/subtitles'\n",
    "\n",
    "class Flags(object):\n",
    "    def __init__(self):\n",
    "        # Data loading params\n",
    "        self.val_sample_percentage = .1 # Percentage of the training data to use for validation\n",
    "        self.ratings_file = os.path.join(basedir, 'ml-20m/ratings.csv') # Data source for the ratings\n",
    "        self.text_data_file = os.path.join(basedir, 'movielens-subtitles-1024.txt') # Data source\n",
    "\n",
    "        self.max_lines = 100000\n",
    "\n",
    "        # Model Hyperparameters\n",
    "        self.embedding_dim = 64 # Dimensionality of user & movie vectors (default: 128)\n",
    "\n",
    "        #self.max_vocab_size = 100000\n",
    "        #self.vocab_embedding_dim = 300 # Dimensionality of character embedding (default: 128)\n",
    "        #self.filter_sizes = \"3,4,5\" # Comma-separated filter sizes (default: '3,4,5')\n",
    "        #self.num_filters = 128 # Number of filters per filter size (default: 128)\n",
    "        #self.dropout_keep_prob = 0.5 # Dropout keep probability (default: 0.5)\n",
    "\n",
    "        #self.words_in_scene = 64\n",
    "        #self.num_scenes = 16\n",
    "\n",
    "        # Training parameters\n",
    "        self.batch_size = 128 # Batch Size (default: 64)\n",
    "        self.num_epochs = 10 # Number of training epochs (default: 8)\n",
    "        self.summary_every = 100\n",
    "        self.evaluate_every = 1000 # Evaluate model on val set after this many steps (default: 100)\n",
    "        self.checkpoint_every = 2000 # Save model after this many steps (default: 100)\n",
    "        self.num_checkpoints = 3 # Number of checkpoints to store (default: 5)\n",
    "        # Misc Parameters\n",
    "        self.allow_soft_placement = True # Allow device soft device placement\n",
    "        self.log_device_placement = True # Log placement of ops on devices\n",
    "\n",
    "FLAGS = Flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data helpers\n",
    "\n",
    "class IdAssigner:\n",
    "    def __init__(self):\n",
    "        self.forward = dict()\n",
    "        self.reverse = dict()\n",
    "        self.next_id = 0\n",
    "    def get_id(self, x):\n",
    "        if x not in self.forward:\n",
    "            self.forward[x] = self.next_id\n",
    "            self.reverse[self.next_id] = x\n",
    "            self.next_id += 1\n",
    "        return self.forward[x]\n",
    "    def get_reverse_id(self, id_):\n",
    "        return self.reverse[id_]\n",
    "    def get_next_id(self):\n",
    "        return self.next_id\n",
    "\n",
    "class Subtitles:\n",
    "    \"\"\"\n",
    "    Class that is in charge of subtitles\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file):\n",
    "        samples = list(open(data_file, 'r').readlines())\n",
    "        samples = [s.strip() for s in samples]\n",
    "        ids = [int(s.split()[0]) for s in samples]\n",
    "        x_text = [' '.join(s.split()[1:]) for s in samples]\n",
    "        self.subs = dict()\n",
    "        for id_,txt in zip(ids, x_text):\n",
    "            self.subs[id_] = txt\n",
    "\n",
    "\n",
    "class RatingsData:\n",
    "    \"\"\"\n",
    "    Loads the ratings from the file. Returns an array x where each row is [user_id, movie_id]\n",
    "    The movie_id is not the original movie_id, but rather a new id which is allocated densely (no skips)\n",
    "    Only movies with rating >= 3.0 are considered watched (TODO)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_file, max_lines, subs):\n",
    "        self.id_assigner = IdAssigner()\n",
    "        self.movies_per_user = defaultdict(list) # map user_id -> [movie_id_1, movie_id_2, ...]\n",
    "        self.movie_watch_count = defaultdict(int)   # number of times watched per new_movie_id\n",
    "        # load file\n",
    "        x = self._load_file(data_file, max_lines, subs)\n",
    "        x = self._sort_dataset(x)\n",
    "        x = x[['user_id', 'movie_id', 'rating']]\n",
    "        # split training/validation:\n",
    "        # the training set is all movies for each user except the last (in chronological order)\n",
    "        # validation set is the last movie for each user\n",
    "        user_ids = x['user_id']\n",
    "        is_last = (user_ids != np.append(user_ids[1:], -1))\n",
    "        self.train = x[~is_last]\n",
    "        self.val = x[is_last]\n",
    "\n",
    "    def _load_file(self, data_file, max_lines, subs):\n",
    "        print(\"Loading data...\")\n",
    "        x = np.zeros(\n",
    "            max_lines,\n",
    "            dtype=[('valid',np.bool), ('user_id',np.int32), ('movie_id', np.int32), ('rating', np.float32), ('timestamp', np.int32)]\n",
    "        )\n",
    "        with open(data_file) as f:\n",
    "            _ = f.readline() # skip first line\n",
    "            for i,line in enumerate(f.readlines()):\n",
    "                if i % 1000000 == 0:\n",
    "                    print('{}...'.format(i))\n",
    "                if i == max_lines:\n",
    "                    break\n",
    "                words = line.split(',')\n",
    "                user_id  = int(words[0])\n",
    "                movie_id = int(words[1])\n",
    "                rating   = float(words[2])\n",
    "                timestamp = int(words[3])\n",
    "                # if rating < 3.0:\n",
    "                #     # we count this as not-watched (for now...)\n",
    "                #     continue\n",
    "                if movie_id not in subs.subs:\n",
    "                    # movie doesn't have subtitles\n",
    "                    continue\n",
    "                new_movie_id = self.id_assigner.get_id(movie_id)\n",
    "                x[i] = (True, user_id, new_movie_id, rating, timestamp)\n",
    "                self.movies_per_user[user_id].append(new_movie_id)\n",
    "                self.movie_watch_count[new_movie_id] += 1\n",
    "        valid = (x['valid'] != 0)\n",
    "        x = x[valid]\n",
    "        return x[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "\n",
    "    def _sort_dataset(self, x):\n",
    "        # sort by user and timestamp - use stable sorting algorithm\n",
    "        x = x[x['timestamp'].argsort(kind='mergesort')]  # secondary sort key\n",
    "        x = x[x['user_id'].argsort(kind='mergesort')]  # primary sort key\n",
    "        return x\n",
    "\n",
    "    def get_num_users(self):\n",
    "        return max(self.movies_per_user.keys()) + 1 # starts from 1\n",
    "\n",
    "    def get_num_movies(self):\n",
    "        return self.id_assigner.get_next_id()\n",
    "\n",
    "    def get_train(self, shuffle=True):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(len(self.train)))\n",
    "            return self.train[shuffle_indices]\n",
    "        else:\n",
    "            return self.train\n",
    "\n",
    "    def get_val(self, shuffle=True):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(len(self.val)))\n",
    "            return self.val[shuffle_indices]\n",
    "        else:\n",
    "            return self.val\n",
    "\n",
    "    ## TODO: generate negative with equal probability\n",
    "    ## TODO: this doesn't really use movie_id...\n",
    "    def _batch_iter_generate_neg(self, user_id, movie_id):\n",
    "        \"\"\"\n",
    "        Generates a batch res of same length as the inputs, such that for each tuple in (user_id, movie_id, res),\n",
    "        the item movie_id is ranked same or higher (by the user) than the corresponding item in res.\n",
    "        Also includes movies that haven't been watched. The movies are chosen according to popularity.\n",
    "        inputs:\n",
    "            user_id is array of length n\n",
    "            movie_id is array of length n\n",
    "        output:\n",
    "            res is array of length n\n",
    "        \"\"\"\n",
    "        n = len(user_id)\n",
    "        assert len(movie_id) == n, len(movie_id)\n",
    "        # count number of watches\n",
    "        num_movies = self.id_assigner.get_next_id()\n",
    "        watch_counts = np.array([self.movie_watch_count[i] for i in range(num_movies)], dtype=np.float32)\n",
    "        total_watches = np.sum(watch_counts)\n",
    "        assert(total_watches > 0)\n",
    "        res = np.zeros([n], dtype=np.int32)\n",
    "        # do it per user to speed up things\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            curr_user_id = user_id[i]\n",
    "            user_start_idx = i\n",
    "            while i < n and user_id[i] == curr_user_id:\n",
    "                i += 1\n",
    "                if i % 1000000 == 0:\n",
    "                    print('{}...'.format(i))\n",
    "            if i < n:\n",
    "                assert user_id[i] > curr_user_id, 'input needs to be sorted by user, otherwise this code it really inefficient'\n",
    "            user_end_idx = i # after-last\n",
    "            # update watch counts: subtract this user's watches\n",
    "            to_mask = np.zeros([num_movies], dtype=np.bool)\n",
    "            for j in self.movies_per_user[curr_user_id]:\n",
    "                to_mask[j] = True\n",
    "            total_masked = np.sum(watch_counts[to_mask])\n",
    "            p = (watch_counts * (1-to_mask)) / (total_watches - total_masked)\n",
    "            res[user_start_idx:user_end_idx] = \\\n",
    "                np.random.choice(num_movies, size=[user_end_idx-user_start_idx], p=p)\n",
    "            if False: # too slow\n",
    "                for j in range(user_start_idx, user_end_idx):\n",
    "                    assert res[j] not in self.movies_per_user[curr_user_id]\n",
    "        return res\n",
    "\n",
    "    def batch_iter(self, x, batch_size, num_epochs, shuffle=True):\n",
    "        \"\"\"\n",
    "        Generates the pair for each datapoint, and then\n",
    "        generates a batch iterator.\n",
    "        \"\"\"\n",
    "        assert type(x) == np.ndarray, type(x)\n",
    "        data_size = len(x)\n",
    "        # split to batches\n",
    "        num_batches_per_epoch = ((data_size - 1) // batch_size) + 1\n",
    "        for epoch in range(num_epochs):\n",
    "            # generate the pair\n",
    "            neg = self._batch_iter_generate_neg(x['user_id'], x['movie_id'])\n",
    "            # Shuffle the data\n",
    "            shuffle_indices = np.arange(data_size)\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "            x_shuffled = x[shuffle_indices]\n",
    "            neg_shuffled = neg[shuffle_indices]\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                x_batch = x_shuffled[start_index:end_index]\n",
    "                batch_neg = neg_shuffled[start_index:end_index]\n",
    "                batch_user_id = x_batch['user_id']\n",
    "                batch_pos = x_batch['movie_id']\n",
    "                batch_pos_neg = np.stack([batch_pos,batch_neg], axis=-1)\n",
    "                batch_rating = x_batch['rating']\n",
    "                yield (batch_user_id,batch_pos_neg,batch_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "0...\n",
      "Train/Val split: 97629/702\n",
      "Num users: 703\n",
      "Num movies: 7494\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "subs = Subtitles(FLAGS.text_data_file)\n",
    "ratings = RatingsData(FLAGS.ratings_file, FLAGS.max_lines, subs)\n",
    "#ratings = RatingsData(FLAGS.ratings_file, 10000, subs)\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "x_train = ratings.get_train(shuffle=False)\n",
    "x_val = ratings.get_val(shuffle=False)\n",
    "\n",
    "print(\"Train/Val split: {:d}/{:d}\".format(len(x_train), len(x_val)))\n",
    "\n",
    "num_users = ratings.get_num_users()\n",
    "num_movies = ratings.get_num_movies()\n",
    "\n",
    "print('Num users: {}'.format(num_users))\n",
    "print('Num movies: {}'.format(num_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dynamic_tensor_shape(x):\n",
    "    \"\"\"\n",
    "    Calculate the tensor shape. Use a plain number where possible and a tensor elsewhere.\n",
    "    x is a tensor of some shape.\n",
    "    returns a list with the dimensions of x.\n",
    "    \"\"\"\n",
    "    shape_tensor = tf.shape(x)\n",
    "    shape = list(x.get_shape())\n",
    "    for i in range(len(shape)):\n",
    "        shape[i] = shape[i].value\n",
    "        if shape[i] is None:\n",
    "            # use tensor to represent the dimension\n",
    "            shape[i] = shape_tensor[i]\n",
    "    return shape\n",
    "\n",
    "\n",
    "def embedding_lookup_layer(x, vocab_size, embedding_dim, variable_scope, reuse=False):\n",
    "    \"\"\"\n",
    "    Lookup embedding\n",
    "    x is tensor of shape (d_1, d_2, ..., d_n) and type int32\n",
    "    result is tensor of shape (d_1, d_2, ..., d_n, embedding_dim) of n+1 dimensions and type DT_FLOAT\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(variable_scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            'W',\n",
    "            shape=[vocab_size, embedding_dim],\n",
    "            initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            regularizer=tf.contrib.layers.l2_regularizer(1.)\n",
    "        )\n",
    "    x_embedded = tf.nn.embedding_lookup(W, x)\n",
    "    return x_embedded\n",
    "\n",
    "def bias_lookup_layer(x, vocab_size, variable_scope, reuse=False):\n",
    "    \"\"\"\n",
    "    Lookup embedding\n",
    "    x is tensor of shape (d_1, d_2, ..., d_n) and type int32\n",
    "    result is tensor of same shape in x and type DT_FLOAT\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(variable_scope, reuse=reuse):\n",
    "        b = tf.get_variable(\n",
    "            'b',\n",
    "            shape=[vocab_size, 1],\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            regularizer=tf.contrib.layers.l2_regularizer(1.)\n",
    "        )\n",
    "    x_bias = tf.squeeze(tf.nn.embedding_lookup(b, x), -1)\n",
    "    return x_bias\n",
    "\n",
    "def fc_layer(x, output_size, variable_scope, reuse=False):\n",
    "    \"\"\"\n",
    "    Fully-connected layer\n",
    "    x has shape (batch_size, d_2)\n",
    "    result has shape (batch_size, output_size)\n",
    "    \"\"\"\n",
    "    shape = get_dynamic_tensor_shape(x)\n",
    "    assert len(shape) == 2\n",
    "    ## TODO: regularization\n",
    "    with tf.variable_scope(variable_scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            \"W\",\n",
    "            shape=[shape[1], output_size],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable(\n",
    "            \"b\",\n",
    "            shape=[output_size],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "    result = tf.nn.xw_plus_b(x, W, b, name=\"fc\")\n",
    "    return result\n",
    "\n",
    "\n",
    "class PredictionModel(object):\n",
    "    \"\"\"\n",
    "    A neural network for predicting per-user movie ratings.\n",
    "    The input to the network is the user_id and movie_id.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_movies, embedding_dim, l2_reg_lambda):\n",
    "\n",
    "        assert num_users >= 1\n",
    "        self.num_users = num_users\n",
    "        assert num_movies >= 1\n",
    "        self.num_movies = num_movies\n",
    "        assert embedding_dim >= 1\n",
    "        self.embedding_dim = embedding_dim\n",
    "        assert l2_reg_lambda >= 0\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_user_id = tf.placeholder(tf.int32, [None], name=\"input_user_id\")\n",
    "        self.input_pos_neg = tf.placeholder(tf.int32, [None, 2], name=\"input_pos_neg\")\n",
    "        self.input_movie_id = tf.placeholder(tf.int32, [None], name=\"input_movie_id\")\n",
    "        #self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\") # TODO: do we need dropout?\n",
    "\n",
    "        asrt1 = tf.assert_equal(tf.shape(self.input_user_id)[0], tf.shape(self.input_pos_neg)[0])\n",
    "\n",
    "        # embedding lookup layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding_lookup'), tf.control_dependencies([asrt1]):\n",
    "            user_embedding = embedding_lookup_layer(self.input_user_id, num_users, embedding_dim, 'user_embedding')\n",
    "            movie_embedding = embedding_lookup_layer(self.input_pos_neg, num_movies, embedding_dim, 'movie_embedding')\n",
    "            movie_bias = bias_lookup_layer(self.input_pos_neg, num_movies, 'movie_embedding')\n",
    "\n",
    "        pos_embedding = movie_embedding[:,0,:]\n",
    "        neg_embedding = movie_embedding[:,1,:]\n",
    "        pos_bias = movie_bias[:,0]\n",
    "        neg_bias = movie_bias[:,1]\n",
    "\n",
    "        delta_embedding = pos_embedding - neg_embedding\n",
    "        delta_bias = pos_bias - neg_bias\n",
    "\n",
    "        # collaborative prediction layer\n",
    "        with tf.name_scope('collab_prediction'):        \n",
    "            collab_prediction = tf.reduce_sum(user_embedding * delta_embedding, axis=1) + delta_bias\n",
    "\n",
    "        # Calculate loss\n",
    "        with tf.name_scope('collab_loss'):\n",
    "            #losses = tf.log(tf.sigmoid(-collab_prediction) + 0.01)\n",
    "            losses = tf.sigmoid(-collab_prediction) ## TODO use log of sigmoid\n",
    "            self.collab_loss = tf.reduce_mean(losses) # TODO use built-in l2_loss functionality\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "            self.loss = self.collab_loss + l2_reg_lambda * sum(reg_losses)\n",
    "\n",
    "        # Collab accuracy = ranking accuracy\n",
    "        with tf.name_scope('collab_accuracy'):\n",
    "            self.collab_accuracy = tf.reduce_mean(tf.cast(collab_prediction > 0, tf.float32))\n",
    "\n",
    "    def get_predictions(self):\n",
    "        asrt1 = tf.assert_equal(tf.shape(self.input_user_id)[0], 1)\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding_lookup'), tf.control_dependencies([asrt1]):\n",
    "            user_embedding = embedding_lookup_layer(self.input_user_id, self.num_users, self.embedding_dim, 'user_embedding', True)\n",
    "            movie_embedding = embedding_lookup_layer(self.input_movie_id, self.num_movies, self.embedding_dim, 'movie_embedding', True)\n",
    "            movie_bias = bias_lookup_layer(self.input_movie_id, self.num_movies, 'movie_embedding', True)\n",
    "            prediction = tf.reduce_sum(user_embedding * movie_embedding, axis=1) + movie_bias\n",
    "            self.tmp1 = tf.reduce_sum(user_embedding * movie_embedding, axis=1)\n",
    "            self.tmp2 = movie_bias\n",
    "            return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "def train(\n",
    "    cnn, sess, starter_learning_rate, learning_rate_decay_every, learning_rate_decay_by\n",
    "):\n",
    "    last_accuracy = 0\n",
    "\n",
    "    # Define Training procedure\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    #optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        starter_learning_rate, global_step, learning_rate_decay_every,\n",
    "        learning_rate_decay_by, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "    # Keep track of gradient values and sparsity (optional)\n",
    "    grad_summaries = []\n",
    "    #for g, v in grads_and_vars:\n",
    "    for g,v in []:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    #grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    # Summaries for loss and accuracy\n",
    "    collab_loss_summary = tf.summary.scalar(\"collab_loss\", cnn.collab_loss)\n",
    "    loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", cnn.collab_accuracy)\n",
    "    learning_rate_summary = tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([collab_loss_summary, loss_summary, acc_summary, learning_rate_summary])#, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Val summaries\n",
    "    val_summary_op = tf.summary.merge([collab_loss_summary, loss_summary, acc_summary, learning_rate_summary])\n",
    "    val_summary_dir = os.path.join(out_dir, \"summaries\", \"val\")\n",
    "    val_summary_writer = tf.summary.FileWriter(val_summary_dir, sess.graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train_step(batch_user_id, batch_pos_neg, batch_rating):\n",
    "        \"\"\"\n",
    "        A single training step \n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            cnn.input_user_id: batch_user_id,\n",
    "            cnn.input_pos_neg: batch_pos_neg,\n",
    "            #cnn.input_rating: batch_rating,\n",
    "            #cnn.input_train_rating: 0.1,\n",
    "            #cnn.dropout_keep_prob: FLAGS.dropout_keep_prob,\n",
    "        }\n",
    "        (pretrain_collab_accuracy,) = sess.run([cnn.collab_accuracy], feed_dict)\n",
    "        sess.run(train_op, feed_dict)\n",
    "        step, loss, collab_accuracy, rate = sess.run(\n",
    "            [global_step, cnn.loss, cnn.collab_accuracy, learning_rate],\n",
    "            feed_dict)\n",
    "        if step % FLAGS.summary_every == 0:\n",
    "            summaries = sess.run(train_summary_op, feed_dict)\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        if step % FLAGS.summary_every == 0:\n",
    "            print(\"{}: step {}, loss {:g}, collab_acc {:g}->{:g}, rate {:g}\".format(\n",
    "                time_str, step, loss, pretrain_collab_accuracy, collab_accuracy, rate))\n",
    "        return collab_accuracy\n",
    "\n",
    "    def val_step(batch_user_id, batch_pos_neg, batch_rating, writer=None):\n",
    "        \"\"\"\n",
    "        Evaluates model on a val set\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            cnn.input_user_id: batch_user_id,\n",
    "            cnn.input_pos_neg: batch_pos_neg,\n",
    "            #cnn.input_rating: batch_rating,\n",
    "            #cnn.input_train_rating: 0.1,\n",
    "        }\n",
    "        step, summaries, loss, collab_accuracy = sess.run(\n",
    "            [global_step, val_summary_op, cnn.loss, cnn.collab_accuracy],\n",
    "            feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, collab_acc {:g}\".format(\n",
    "            time_str, step, loss, collab_accuracy))\n",
    "        if writer:\n",
    "            writer.add_summary(summaries, step)\n",
    "        return collab_accuracy\n",
    "\n",
    "    # Generate batches\n",
    "    batches = ratings.batch_iter(x_train, FLAGS.batch_size, FLAGS.num_epochs)\n",
    "    # Training loop. For each batch...\n",
    "    last_test_collab_accuracy = None\n",
    "    for batch_user_id, batch_pos_neg, batch_rating in batches:\n",
    "        last_collab_accuracy = train_step(batch_user_id, batch_pos_neg, batch_rating)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % FLAGS.evaluate_every == 0:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            ((val_user_id, val_pos_neg, val_rating),) = ratings.batch_iter(x_val, len(x_val), 1)\n",
    "            if len(x_val) > 1024:\n",
    "                val_user_id, val_pos_neg, val_rating = val_user_id[:1024], val_pos_neg[:1024], val_rating[:1024]\n",
    "            last_test_collab_accuracy = \\\n",
    "                val_step(val_user_id, val_pos_neg, val_rating, writer=val_summary_writer)\n",
    "            print(\"\")\n",
    "        if current_step % FLAGS.checkpoint_every == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "            pass\n",
    "    return (last_collab_accuracy, last_test_collab_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_precision(model, sess):\n",
    "    ranks = []\n",
    "    mrr = 0.\n",
    "    mrr_at_10 = 0\n",
    "    precision_at_10 = 0\n",
    "    n = 50 # calculate only on first n users in validation set\n",
    "    for i in range(n):\n",
    "        user_id, movie_id, rating = x_val[i]\n",
    "        if i % 50 == 0:\n",
    "            print('{}...'.format(i))\n",
    "        num_movies = ratings.id_assigner.get_next_id()\n",
    "        batch_movie_id = np.arange(num_movies)\n",
    "        #batch_user_id = np.ones_like(batch_movie_id) * user_id\n",
    "        #batch_rating = np.zeros_like(batch_movie_id)\n",
    "        feed_dict = {\n",
    "            model.input_user_id: [user_id],\n",
    "            model.input_movie_id: batch_movie_id,\n",
    "        }\n",
    "        scores = sess.run(model.get_predictions(), feed_dict=feed_dict)\n",
    "        s = scores[movie_id] # the score for the correct movie\n",
    "        train_movies = x_train[x_train['user_id'] == user_id]['movie_id']\n",
    "        not_watched = (scores == scores) # all True\n",
    "        not_watched[train_movies] = False\n",
    "        higher_scores = (scores > s)    \n",
    "        rank = np.sum(higher_scores & not_watched) + 1\n",
    "        ranks.append(rank)\n",
    "        mrr += 1. / rank\n",
    "        if rank <= 10:\n",
    "            mrr_at_10 += 1. / rank\n",
    "            precision_at_10 += 1\n",
    "    mrr /= n\n",
    "    mrr_at_10 /= n\n",
    "    precision_at_10 /= n\n",
    "    return mrr, mrr_at_10, precision_at_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runall():\n",
    "    res = defaultdict(list)\n",
    "    res_mrr = defaultdict(list)\n",
    "    res_mrr10 = defaultdict(list)\n",
    "    res_p10 = defaultdict(list)\n",
    "    with open('results.txt', 'a') as f:\n",
    "        for l2_reg_lambda in [0.]:\n",
    "            with tf.Graph().as_default():\n",
    "                session_conf = tf.ConfigProto(\n",
    "                    allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "                    log_device_placement=FLAGS.log_device_placement)\n",
    "                session_conf.gpu_options.allow_growth=True\n",
    "                sess = tf.Session(config=session_conf)\n",
    "                with sess.as_default():\n",
    "                    model = PredictionModel(\n",
    "                        num_users=num_users,\n",
    "                        num_movies=num_movies,\n",
    "                        embedding_dim=FLAGS.embedding_dim,\n",
    "                        l2_reg_lambda=l2_reg_lambda)\n",
    "                    for i in range(1):\n",
    "                        f.write('lambda: {}\\n'.format(l2_reg_lambda))\n",
    "                        last_accuracy = train(model, sess, 3e-3, 20000, 0.5)\n",
    "                        f.write('accuracy: {}\\n'.format(last_accuracy))\n",
    "                        res[l2_reg_lambda].append(last_accuracy)\n",
    "                        mrr, mrr10, p10 = calc_precision(model, sess)\n",
    "                        f.write(repr((mrr, p10)) + '\\n')\n",
    "                        res_mrr[l2_reg_lambda] = mrr\n",
    "                        res_p10[l2_reg_lambda] = p10\n",
    "                        res_mrr10[l2_reg_lambda] = mrr10\n",
    "                        f.write('\\n')\n",
    "                        f.flush()\n",
    "    return res, res_mrr, res_mrr10, res_p10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/tvromen/research/subtitles2/runs/1510229389\n",
      "\n",
      "2017-11-09T14:09:51.013176: step 100, loss 0.496787, collab_acc 0.492188->0.648438, rate 0.003\n",
      "2017-11-09T14:09:52.030435: step 200, loss 0.488271, collab_acc 0.585938->0.679688, rate 0.003\n",
      "2017-11-09T14:09:53.028962: step 300, loss 0.473816, collab_acc 0.625->0.71875, rate 0.003\n",
      "2017-11-09T14:09:54.042243: step 400, loss 0.43884, collab_acc 0.726562->0.78125, rate 0.003\n",
      "2017-11-09T14:09:55.038706: step 500, loss 0.383724, collab_acc 0.695312->0.726562, rate 0.003\n",
      "2017-11-09T14:09:55.923697: step 600, loss 0.333434, collab_acc 0.78125->0.820312, rate 0.003\n",
      "2017-11-09T14:09:56.925698: step 700, loss 0.310399, collab_acc 0.757812->0.78125, rate 0.003\n",
      "2017-11-09T14:09:58.129416: step 800, loss 0.275027, collab_acc 0.796875->0.8125, rate 0.003\n",
      "2017-11-09T14:09:59.029466: step 900, loss 0.262865, collab_acc 0.796875->0.8125, rate 0.003\n",
      "2017-11-09T14:09:59.984243: step 1000, loss 0.233985, collab_acc 0.828125->0.828125, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:10:00.144420: step 1000, loss 0.334857, collab_acc 0.720798\n",
      "\n",
      "2017-11-09T14:10:01.160541: step 1100, loss 0.281763, collab_acc 0.75->0.773438, rate 0.003\n",
      "2017-11-09T14:10:02.301760: step 1200, loss 0.222236, collab_acc 0.820312->0.828125, rate 0.003\n",
      "2017-11-09T14:10:03.457867: step 1300, loss 0.226341, collab_acc 0.789062->0.8125, rate 0.003\n",
      "2017-11-09T14:10:04.615754: step 1400, loss 0.230135, collab_acc 0.78125->0.8125, rate 0.003\n",
      "2017-11-09T14:10:05.741025: step 1500, loss 0.235383, collab_acc 0.796875->0.804688, rate 0.003\n",
      "2017-11-09T14:10:07.123839: step 1600, loss 0.197795, collab_acc 0.867188->0.875, rate 0.003\n",
      "2017-11-09T14:10:08.298171: step 1700, loss 0.182218, collab_acc 0.8125->0.859375, rate 0.003\n",
      "2017-11-09T14:10:09.411128: step 1800, loss 0.176916, collab_acc 0.84375->0.84375, rate 0.003\n",
      "2017-11-09T14:10:10.561087: step 1900, loss 0.151649, collab_acc 0.875->0.882812, rate 0.003\n",
      "2017-11-09T14:10:11.710681: step 2000, loss 0.171776, collab_acc 0.851562->0.875, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:10:11.895063: step 2000, loss 0.283321, collab_acc 0.742165\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510229389/checkpoints/model-2000\n",
      "\n",
      "2017-11-09T14:10:13.135622: step 2100, loss 0.188857, collab_acc 0.8125->0.828125, rate 0.003\n",
      "2017-11-09T14:10:14.272934: step 2200, loss 0.212463, collab_acc 0.8125->0.820312, rate 0.003\n",
      "2017-11-09T14:10:15.644812: step 2300, loss 0.133554, collab_acc 0.875->0.890625, rate 0.003\n",
      "2017-11-09T14:10:16.737683: step 2400, loss 0.143515, collab_acc 0.851562->0.882812, rate 0.003\n",
      "2017-11-09T14:10:17.894921: step 2500, loss 0.148915, collab_acc 0.859375->0.882812, rate 0.003\n",
      "2017-11-09T14:10:19.282318: step 2600, loss 0.117014, collab_acc 0.898438->0.898438, rate 0.003\n",
      "2017-11-09T14:10:20.665225: step 2700, loss 0.130481, collab_acc 0.890625->0.90625, rate 0.003\n",
      "2017-11-09T14:10:22.074381: step 2800, loss 0.126199, collab_acc 0.898438->0.914062, rate 0.003\n",
      "2017-11-09T14:10:23.511379: step 2900, loss 0.119633, collab_acc 0.898438->0.914062, rate 0.003\n",
      "2017-11-09T14:10:24.934556: step 3000, loss 0.160657, collab_acc 0.851562->0.867188, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:10:25.140323: step 3000, loss 0.249511, collab_acc 0.782051\n",
      "\n",
      "2017-11-09T14:10:26.850462: step 3100, loss 0.0821777, collab_acc 0.945312->0.953125, rate 0.003\n",
      "2017-11-09T14:10:28.325221: step 3200, loss 0.130647, collab_acc 0.875->0.90625, rate 0.003\n",
      "2017-11-09T14:10:29.683673: step 3300, loss 0.123104, collab_acc 0.90625->0.914062, rate 0.003\n",
      "2017-11-09T14:10:31.114919: step 3400, loss 0.102591, collab_acc 0.914062->0.921875, rate 0.003\n",
      "2017-11-09T14:10:32.566244: step 3500, loss 0.0734119, collab_acc 0.921875->0.929688, rate 0.003\n",
      "2017-11-09T14:10:33.939411: step 3600, loss 0.113535, collab_acc 0.898438->0.898438, rate 0.003\n",
      "2017-11-09T14:10:35.407632: step 3700, loss 0.072011, collab_acc 0.945312->0.945312, rate 0.003\n",
      "2017-11-09T14:10:36.858904: step 3800, loss 0.10766, collab_acc 0.890625->0.914062, rate 0.003\n",
      "2017-11-09T14:10:38.524449: step 3900, loss 0.0805179, collab_acc 0.9375->0.953125, rate 0.003\n",
      "2017-11-09T14:10:39.939343: step 4000, loss 0.0932791, collab_acc 0.90625->0.921875, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:10:40.140403: step 4000, loss 0.249505, collab_acc 0.763533\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510229389/checkpoints/model-4000\n",
      "\n",
      "2017-11-09T14:10:41.558201: step 4100, loss 0.126079, collab_acc 0.890625->0.890625, rate 0.003\n",
      "2017-11-09T14:10:42.965494: step 4200, loss 0.0966616, collab_acc 0.898438->0.914062, rate 0.003\n",
      "2017-11-09T14:10:44.385764: step 4300, loss 0.120499, collab_acc 0.882812->0.890625, rate 0.003\n",
      "2017-11-09T14:10:45.810035: step 4400, loss 0.099005, collab_acc 0.90625->0.921875, rate 0.003\n",
      "2017-11-09T14:10:47.217650: step 4500, loss 0.0687184, collab_acc 0.953125->0.960938, rate 0.003\n",
      "2017-11-09T14:10:48.838515: step 4600, loss 0.0850443, collab_acc 0.9375->0.9375, rate 0.003\n",
      "2017-11-09T14:10:50.271863: step 4700, loss 0.109985, collab_acc 0.898438->0.90625, rate 0.003\n",
      "2017-11-09T14:10:51.682081: step 4800, loss 0.0940922, collab_acc 0.914062->0.921875, rate 0.003\n",
      "2017-11-09T14:10:53.057366: step 4900, loss 0.0624364, collab_acc 0.945312->0.953125, rate 0.003\n",
      "2017-11-09T14:10:54.350942: step 5000, loss 0.0486859, collab_acc 0.953125->0.96875, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:10:54.555255: step 5000, loss 0.222736, collab_acc 0.792023\n",
      "\n",
      "2017-11-09T14:10:55.921725: step 5100, loss 0.0570942, collab_acc 0.96875->0.96875, rate 0.003\n",
      "2017-11-09T14:10:57.332481: step 5200, loss 0.109337, collab_acc 0.890625->0.898438, rate 0.003\n",
      "2017-11-09T14:10:58.709080: step 5300, loss 0.0800969, collab_acc 0.945312->0.960938, rate 0.003\n",
      "2017-11-09T14:11:00.399646: step 5400, loss 0.0564837, collab_acc 0.945312->0.960938, rate 0.003\n",
      "2017-11-09T14:11:01.831955: step 5500, loss 0.0438596, collab_acc 0.976562->0.984375, rate 0.003\n",
      "2017-11-09T14:11:03.279568: step 5600, loss 0.0661756, collab_acc 0.945312->0.960938, rate 0.003\n",
      "2017-11-09T14:11:04.722132: step 5700, loss 0.0474966, collab_acc 0.960938->0.96875, rate 0.003\n",
      "2017-11-09T14:11:06.074672: step 5800, loss 0.0445322, collab_acc 0.976562->0.976562, rate 0.003\n",
      "2017-11-09T14:11:07.499944: step 5900, loss 0.092242, collab_acc 0.921875->0.921875, rate 0.003\n",
      "2017-11-09T14:11:08.915543: step 6000, loss 0.0641006, collab_acc 0.945312->0.96875, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:11:09.124257: step 6000, loss 0.213977, collab_acc 0.816239\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510229389/checkpoints/model-6000\n",
      "\n",
      "2017-11-09T14:11:10.625296: step 6100, loss 0.0799032, collab_acc 0.914062->0.9375, rate 0.003\n",
      "2017-11-09T14:11:12.286498: step 6200, loss 0.0499001, collab_acc 0.976562->0.984375, rate 0.003\n",
      "2017-11-09T14:11:13.682092: step 6300, loss 0.0654278, collab_acc 0.9375->0.945312, rate 0.003\n",
      "2017-11-09T14:11:15.127731: step 6400, loss 0.0544744, collab_acc 0.96875->0.96875, rate 0.003\n",
      "2017-11-09T14:11:16.518313: step 6500, loss 0.0651698, collab_acc 0.953125->0.953125, rate 0.003\n",
      "2017-11-09T14:11:17.817413: step 6600, loss 0.0497518, collab_acc 0.960938->0.960938, rate 0.003\n",
      "2017-11-09T14:11:19.226721: step 6700, loss 0.0858635, collab_acc 0.921875->0.9375, rate 0.003\n",
      "2017-11-09T14:11:20.637389: step 6800, loss 0.073772, collab_acc 0.945312->0.945312, rate 0.003\n",
      "2017-11-09T14:11:22.346223: step 6900, loss 0.0383028, collab_acc 0.96875->0.976562, rate 0.003\n",
      "2017-11-09T14:11:23.766275: step 7000, loss 0.0627266, collab_acc 0.953125->0.953125, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:11:23.967929: step 7000, loss 0.216785, collab_acc 0.806268\n",
      "\n",
      "2017-11-09T14:11:25.395607: step 7100, loss 0.0684208, collab_acc 0.9375->0.953125, rate 0.003\n",
      "2017-11-09T14:11:26.815149: step 7200, loss 0.0657256, collab_acc 0.953125->0.953125, rate 0.003\n",
      "2017-11-09T14:11:28.272748: step 7300, loss 0.0445625, collab_acc 0.96875->0.96875, rate 0.003\n",
      "2017-11-09T14:11:29.650016: step 7400, loss 0.0464762, collab_acc 0.984375->0.984375, rate 0.003\n",
      "2017-11-09T14:11:31.119178: step 7500, loss 0.0997459, collab_acc 0.914062->0.914062, rate 0.003\n",
      "2017-11-09T14:11:32.561503: step 7600, loss 0.036822, collab_acc 0.96875->0.96875, rate 0.003\n",
      "0...\n"
     ]
    }
   ],
   "source": [
    "res = runall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(defaultdict(<class 'list'>, {0.0: [(0.94623655, 0.8062678)]}), defaultdict(<class 'list'>, {0.0: 0.054445933646771388}), defaultdict(<class 'list'>, {0.0: 0.044857142857142859}), defaultdict(<class 'list'>, {0.0: 0.14}))\n"
     ]
    }
   ],
   "source": [
    "# 100K lines\n",
    "# all star ratings are considered watched\n",
    "# with regularization\n",
    "# update code to support calculation of MRR and precision@10\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(defaultdict(<class 'list'>, {0.0: [(0.83673465, 0.77148438)]}), defaultdict(<class 'list'>, {0.0: 0.048712822395037852}), defaultdict(<class 'list'>, {0.0: 0.035190476190476189}), defaultdict(<class 'list'>, {0.0: 0.12}))\n"
     ]
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "# with regularization\n",
    "# update code to support calculation of MRR and precision@10\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0.0: [(0.97192383, 0.75740814)], 1e-06: [(0.9397583, 0.7156477)]})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "# with regularization\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {2e-06: [(0.824646, 0.71223354)], 2e-05: [(0.546875, 0.46242237)]})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# only star ratings of 3.0 and above are considered watched\n",
    "# added regularization\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0.0: [(0.86505127, 0.7810421)],\n",
       "             0.0001: [(0.53125, 0.48535061)],\n",
       "             0.0002: [(0.5234375, 0.4638834)],\n",
       "             0.0005: [(0.5703125, 0.48535156)],\n",
       "             0.001: [(0.53106689, 0.50292969)],\n",
       "             0.002: [(0.5234375, 0.46480083)],\n",
       "             0.005: [(0.5, 0.46386719)],\n",
       "             0.01: [(0.5234375, 0.48632812)],\n",
       "             0.02: [(0.5546875, 0.50780296)],\n",
       "             0.05: [(0.59375, 0.50976562)],\n",
       "             0.1: [(0.5703125, 0.5092907)]})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# only star ratings of 3.0 and above are considered watched\n",
    "# added regularization\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {0: [(0.8729248, 0.78455317)]})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# only star ratings of 3.0 and above are considered watched\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {0: [(0.96618652, 0.77198738)]})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
