{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use \"SVD\" to predict user rating of each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import struct\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basedir = '/home/tvromen/research/subtitles'\n",
    "\n",
    "class Flags(object):\n",
    "    def __init__(self):\n",
    "        # Data loading params\n",
    "        self.val_sample_percentage = .1 # Percentage of the training data to use for validation\n",
    "        self.ratings_file = os.path.join(basedir, 'ml-20m/ratings.csv') # Data source for the ratings\n",
    "        self.text_data_file = os.path.join(basedir, 'movielens-subtitles-1024.txt') # Data source\n",
    "\n",
    "        self.max_lines = 100000\n",
    "\n",
    "        # Model Hyperparameters\n",
    "        self.embedding_dim = 64 # Dimensionality of user & movie vectors (default: 128)\n",
    "\n",
    "        #self.max_vocab_size = 100000\n",
    "        #self.vocab_embedding_dim = 300 # Dimensionality of character embedding (default: 128)\n",
    "        #self.filter_sizes = \"3,4,5\" # Comma-separated filter sizes (default: '3,4,5')\n",
    "        #self.num_filters = 128 # Number of filters per filter size (default: 128)\n",
    "        #self.dropout_keep_prob = 0.5 # Dropout keep probability (default: 0.5)\n",
    "\n",
    "        #self.words_in_scene = 64\n",
    "        #self.num_scenes = 16\n",
    "\n",
    "        # Training parameters\n",
    "        self.batch_size = 128 # Batch Size (default: 128)\n",
    "        self.num_epochs = 10 # Number of training epochs (default: 8)\n",
    "        self.summary_every = 100\n",
    "        self.evaluate_every = 1000 # Evaluate model on val set after this many steps (default: 100)\n",
    "        self.checkpoint_every = 2000 # Save model after this many steps (default: 100)\n",
    "        self.num_checkpoints = 3 # Number of checkpoints to store (default: 5)\n",
    "        # Misc Parameters\n",
    "        self.allow_soft_placement = True # Allow device soft device placement\n",
    "        self.log_device_placement = True # Log placement of ops on devices\n",
    "\n",
    "FLAGS = Flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data helpers\n",
    "\n",
    "class IdAssigner:\n",
    "    def __init__(self):\n",
    "        self.forward = dict()\n",
    "        self.reverse = dict()\n",
    "        self.next_id = 0\n",
    "    def get_id(self, x):\n",
    "        if x not in self.forward:\n",
    "            self.forward[x] = self.next_id\n",
    "            self.reverse[self.next_id] = x\n",
    "            self.next_id += 1\n",
    "        return self.forward[x]\n",
    "    def get_reverse_id(self, id_):\n",
    "        return self.reverse[id_]\n",
    "    def get_next_id(self):\n",
    "        return self.next_id\n",
    "\n",
    "class Subtitles:\n",
    "    \"\"\"\n",
    "    Class that is in charge of subtitles\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file):\n",
    "        samples = list(open(data_file, 'r').readlines())\n",
    "        samples = [s.strip() for s in samples]\n",
    "        ids = [int(s.split()[0]) for s in samples]\n",
    "        x_text = [' '.join(s.split()[1:]) for s in samples]\n",
    "        self.subs = dict()\n",
    "        for id_,txt in zip(ids, x_text):\n",
    "            self.subs[id_] = txt\n",
    "\n",
    "\n",
    "class RatingsData:\n",
    "    \"\"\"\n",
    "    Loads the ratings from the file. Returns an array x where each row is [user_id, movie_id]\n",
    "    The movie_id is not the original movie_id, but rather a new id which is allocated densely (no skips)\n",
    "    Only movies with rating >= 3.0 are considered watched (TODO)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_file, max_lines, subs):\n",
    "        self.id_assigner = IdAssigner()\n",
    "        self.movies_per_user = defaultdict(list) # map user_id -> [movie_id_1, movie_id_2, ...]\n",
    "        self.movie_watch_count = defaultdict(int)   # number of times watched per new_movie_id\n",
    "        # load file\n",
    "        x = self._load_file(data_file, max_lines, subs)\n",
    "        x = self._sort_dataset(x)\n",
    "        x = x[['user_id', 'movie_id', 'rating']]\n",
    "        # split training/validation:\n",
    "        # the training set is all movies for each user except the last (in chronological order)\n",
    "        # validation set is the last movie for each user\n",
    "        user_ids = x['user_id']\n",
    "        is_last = (user_ids != np.append(user_ids[1:], -1))\n",
    "        self.train = x[~is_last]\n",
    "        self.val = x[is_last]\n",
    "\n",
    "    def _load_file(self, data_file, max_lines, subs):\n",
    "        print(\"Loading data...\")\n",
    "        x = np.zeros(\n",
    "            max_lines,\n",
    "            dtype=[('valid',np.bool), ('user_id',np.int32), ('movie_id', np.int32), ('rating', np.float32), ('timestamp', np.int32)]\n",
    "        )\n",
    "        with open(data_file) as f:\n",
    "            _ = f.readline() # skip first line\n",
    "            for i,line in enumerate(f.readlines()):\n",
    "                if i % 1000000 == 0:\n",
    "                    print('{}...'.format(i))\n",
    "                if i == max_lines:\n",
    "                    break\n",
    "                words = line.split(',')\n",
    "                user_id  = int(words[0])\n",
    "                movie_id = int(words[1])\n",
    "                rating   = float(words[2])\n",
    "                timestamp = int(words[3])\n",
    "                #if rating < 3.0:\n",
    "                #    # we count this as not-watched (for now...)\n",
    "                #    continue\n",
    "                if movie_id not in subs.subs:\n",
    "                    # movie doesn't have subtitles\n",
    "                    continue\n",
    "                new_movie_id = self.id_assigner.get_id(movie_id)\n",
    "                x[i] = (True, user_id, new_movie_id, rating, timestamp)\n",
    "                self.movies_per_user[user_id].append(new_movie_id)\n",
    "                self.movie_watch_count[new_movie_id] += 1\n",
    "        valid = (x['valid'] != 0)\n",
    "        x = x[valid]\n",
    "        # center the ratings around 0\n",
    "        x['rating'] = x['rating'] - np.mean(x['rating'])\n",
    "        return x[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "\n",
    "    def _sort_dataset(self, x):\n",
    "        # sort by user and timestamp - use stable sorting algorithm\n",
    "        x = x[x['timestamp'].argsort(kind='mergesort')]  # secondary sort key\n",
    "        x = x[x['user_id'].argsort(kind='mergesort')]  # primary sort key\n",
    "        return x\n",
    "\n",
    "    def get_num_users(self):\n",
    "        return max(self.movies_per_user.keys()) + 1 # starts from 1\n",
    "\n",
    "    def get_num_movies(self):\n",
    "        return self.id_assigner.get_next_id()\n",
    "\n",
    "    def get_train(self, shuffle=True):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(len(self.train)))\n",
    "            return self.train[shuffle_indices]\n",
    "        else:\n",
    "            return self.train\n",
    "\n",
    "    def get_val(self, shuffle=True):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(len(self.val)))\n",
    "            return self.val[shuffle_indices]\n",
    "        else:\n",
    "            return self.val\n",
    "\n",
    "    def batch_iter(self, x, batch_size, num_epochs, shuffle=True):\n",
    "        \"\"\"\n",
    "        Generates the pair for each datapoint, and then\n",
    "        generates a batch iterator.\n",
    "        \"\"\"\n",
    "        assert type(x) == np.ndarray, type(x)\n",
    "        data_size = len(x)\n",
    "        # split to batches\n",
    "        num_batches_per_epoch = ((data_size - 1) // batch_size) + 1\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the data\n",
    "            shuffle_indices = np.arange(data_size)\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "            x_shuffled = x[shuffle_indices]\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                x_batch = x_shuffled[start_index:end_index]\n",
    "                batch_user_id = x_batch['user_id']\n",
    "                batch_movie_id = x_batch['movie_id']\n",
    "                batch_rating = x_batch['rating']\n",
    "                yield (batch_user_id, batch_movie_id, batch_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "0...\n",
      "Train/Val split: 97629/702\n",
      "Num users: 703\n",
      "Num movies: 7494\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "subs = Subtitles(FLAGS.text_data_file)\n",
    "ratings = RatingsData(FLAGS.ratings_file, FLAGS.max_lines, subs)\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "x_train = ratings.get_train(shuffle=False)\n",
    "x_val = ratings.get_val(shuffle=False)\n",
    "\n",
    "print(\"Train/Val split: {:d}/{:d}\".format(len(x_train), len(x_val)))\n",
    "\n",
    "num_users = ratings.get_num_users()\n",
    "num_movies = ratings.get_num_movies()\n",
    "\n",
    "print('Num users: {}'.format(num_users))\n",
    "print('Num movies: {}'.format(num_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dynamic_tensor_shape(x):\n",
    "    \"\"\"\n",
    "    Calculate the tensor shape. Use a plain number where possible and a tensor elsewhere.\n",
    "    x is a tensor of some shape.\n",
    "    returns a list with the dimensions of x.\n",
    "    \"\"\"\n",
    "    shape_tensor = tf.shape(x)\n",
    "    shape = list(x.get_shape())\n",
    "    for i in range(len(shape)):\n",
    "        shape[i] = shape[i].value\n",
    "        if shape[i] is None:\n",
    "            # use tensor to represent the dimension\n",
    "            shape[i] = shape_tensor[i]\n",
    "    return shape\n",
    "\n",
    "\n",
    "def embedding_lookup_layer(x, vocab_size, embedding_dim, variable_scope, reuse=False):\n",
    "    \"\"\"\n",
    "    Lookup embedding\n",
    "    x is tensor of shape (d_1, d_2, ..., d_n) and type int32\n",
    "    result is tensor of shape (d_1, d_2, ..., d_n, embedding_dim) of n+1 dimensions and type DT_FLOAT\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(variable_scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            'W',\n",
    "            shape=[vocab_size, embedding_dim],\n",
    "            initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            regularizer=tf.contrib.layers.l2_regularizer(1.)\n",
    "        )\n",
    "    x_embedded = tf.nn.embedding_lookup(W, x)\n",
    "    return x_embedded\n",
    "\n",
    "def bias_lookup_layer(x, vocab_size, variable_scope, reuse=False):\n",
    "    \"\"\"\n",
    "    Lookup embedding\n",
    "    x is tensor of shape (d_1, d_2, ..., d_n) and type int32\n",
    "    result is tensor of same shape in x and type DT_FLOAT\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(variable_scope, reuse=reuse):\n",
    "        b = tf.get_variable(\n",
    "            'b',\n",
    "            shape=[vocab_size, 1],\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            regularizer=tf.contrib.layers.l2_regularizer(1.)\n",
    "        )\n",
    "    x_bias = tf.squeeze(tf.nn.embedding_lookup(b, x), -1)\n",
    "    return x_bias\n",
    "\n",
    "def fc_layer(x, output_size, variable_scope, reuse=False):\n",
    "    \"\"\"\n",
    "    Fully-connected layer\n",
    "    x has shape (batch_size, d_2)\n",
    "    result has shape (batch_size, output_size)\n",
    "    \"\"\"\n",
    "    shape = get_dynamic_tensor_shape(x)\n",
    "    assert len(shape) == 2\n",
    "    ## TODO: regularization\n",
    "    with tf.variable_scope(variable_scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            \"W\",\n",
    "            shape=[shape[1], output_size],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable(\n",
    "            \"b\",\n",
    "            shape=[output_size],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "    result = tf.nn.xw_plus_b(x, W, b, name=\"fc\")\n",
    "    return result\n",
    "\n",
    "\n",
    "class PredictionModel(object):\n",
    "    \"\"\"\n",
    "    A neural network for predicting per-user movie ratings.\n",
    "    The input to the network is the user_id and movie_id.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_movies, embedding_dim, l2_reg_lambda):\n",
    "\n",
    "        assert num_users >= 1\n",
    "        self.num_users = num_users\n",
    "        assert num_movies >= 1\n",
    "        self.num_movies = num_movies\n",
    "        assert embedding_dim >= 1\n",
    "        self.embedding_dim = embedding_dim\n",
    "        assert l2_reg_lambda >= 0\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_user_id = tf.placeholder(tf.int32, [None], name=\"input_user_id\")\n",
    "        self.input_movie_id = tf.placeholder(tf.int32, [None], name=\"input_movie_id\")\n",
    "        self.input_rating = tf.placeholder(tf.float32, [None], name=\"input_rating\")\n",
    "        #self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\") # TODO: do we need dropout?\n",
    "\n",
    "        asrt1 = tf.assert_equal(tf.shape(self.input_user_id)[0], tf.shape(self.input_movie_id)[0])\n",
    "        asrt2 = tf.assert_equal(tf.shape(self.input_user_id)[0], tf.shape(self.input_rating)[0])\n",
    "\n",
    "        # embedding lookup layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding_lookup'), tf.control_dependencies([asrt1]):\n",
    "            user_embedding = embedding_lookup_layer(self.input_user_id, self.num_users, self.embedding_dim, 'user_embedding')\n",
    "            user_bias = bias_lookup_layer(self.input_user_id, self.num_users, 'user_embedding')\n",
    "            movie_embedding = embedding_lookup_layer(self.input_movie_id, self.num_movies, self.embedding_dim, 'movie_embedding')\n",
    "            movie_bias = bias_lookup_layer(self.input_movie_id, self.num_movies, 'movie_embedding')\n",
    "            #user_embedding_fc = fc_layer(user_embedding, self.embedding_dim, 'embedding_fc', reuse=None)\n",
    "            #movie_embedding_fc = fc_layer(movie_embedding, self.embedding_dim, 'embedding_fc', reuse=True)\n",
    "            self.rating_prediction = tf.reduce_sum(user_embedding * movie_embedding, axis=1) + user_bias + movie_bias\n",
    "\n",
    "        with tf.name_scope('rating_loss'):\n",
    "            losses = tf.square(self.input_rating - self.rating_prediction)\n",
    "            self.rating_loss = tf.reduce_mean(losses)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "            self.loss = self.rating_loss + l2_reg_lambda * sum(reg_losses)\n",
    "\n",
    "        # Rating accuracy = RMSE (for now...)\n",
    "        # (this is actually error rate, and not accuracy, i.e. lower is better)\n",
    "        with tf.name_scope('rating_accuracy'):\n",
    "            self.rating_accuracy = tf.sqrt(self.rating_loss)\n",
    "\n",
    "    def get_predictions(self):\n",
    "        asrt1 = tf.assert_equal(tf.shape(self.input_user_id)[0], 1)\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding_lookup'), tf.control_dependencies([asrt1]):\n",
    "            user_embedding = embedding_lookup_layer(self.input_user_id, self.num_users, self.embedding_dim, 'user_embedding', True)\n",
    "            user_bias = bias_lookup_layer(self.input_user_id, self.num_users, 'user_embedding', True)\n",
    "            movie_embedding = embedding_lookup_layer(self.input_movie_id, self.num_movies, self.embedding_dim, 'movie_embedding', True)\n",
    "            movie_bias = bias_lookup_layer(self.input_movie_id, self.num_movies, 'movie_embedding', True)\n",
    "            prediction = tf.reduce_sum(user_embedding * movie_embedding, axis=1) + user_bias + movie_bias\n",
    "            self.tmp1 = tf.reduce_sum(user_embedding * movie_embedding, axis=1)\n",
    "            self.tmp2 = user_bias\n",
    "            self.tmp3 = movie_bias\n",
    "            return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "def train(\n",
    "    cnn, sess, starter_learning_rate, learning_rate_decay_every, learning_rate_decay_by\n",
    "):\n",
    "    last_accuracy = 0\n",
    "\n",
    "    # Define Training procedure\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    #optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        starter_learning_rate, global_step, learning_rate_decay_every,\n",
    "        learning_rate_decay_by, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "    # Keep track of gradient values and sparsity (optional)\n",
    "    grad_summaries = []\n",
    "    #for g, v in grads_and_vars:\n",
    "    for g,v in []:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    #grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    # Summaries for loss and accuracy\n",
    "    rating_loss_summary = tf.summary.scalar(\"rating_loss\", cnn.rating_loss)\n",
    "    loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", cnn.rating_accuracy)\n",
    "    learning_rate_summary = tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([rating_loss_summary, loss_summary, acc_summary, learning_rate_summary])#, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Val summaries\n",
    "    val_summary_op = tf.summary.merge([rating_loss_summary, loss_summary, acc_summary, learning_rate_summary])\n",
    "    val_summary_dir = os.path.join(out_dir, \"summaries\", \"val\")\n",
    "    val_summary_writer = tf.summary.FileWriter(val_summary_dir, sess.graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train_step(batch_user_id, batch_movie_id, batch_rating):\n",
    "        \"\"\"\n",
    "        A single training step \n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            cnn.input_user_id: batch_user_id,\n",
    "            cnn.input_movie_id: batch_movie_id,\n",
    "            cnn.input_rating: batch_rating\n",
    "        }\n",
    "        (pretrain_rating_accuracy,) = sess.run([cnn.rating_accuracy], feed_dict)\n",
    "        sess.run(train_op, feed_dict)\n",
    "        step, loss, rating_accuracy, rate = sess.run(\n",
    "            [global_step, cnn.loss, cnn.rating_accuracy, learning_rate],\n",
    "            feed_dict)\n",
    "        if step % FLAGS.summary_every == 0:\n",
    "            summaries = sess.run(train_summary_op, feed_dict)\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        if step % FLAGS.summary_every == 0:\n",
    "            print(\"{}: step {}, loss {:g}, rating_acc {:g}->{:g}, rate {:g}\".format(\n",
    "                time_str, step, loss, pretrain_rating_accuracy, rating_accuracy, rate))\n",
    "        return rating_accuracy\n",
    "\n",
    "    def val_step(batch_user_id, batch_movie_id, batch_rating, writer=None):\n",
    "        \"\"\"\n",
    "        Evaluates model on a val set\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            cnn.input_user_id: batch_user_id,\n",
    "            cnn.input_movie_id: batch_movie_id,\n",
    "            cnn.input_rating: batch_rating\n",
    "        }\n",
    "        step, summaries, loss, rating_accuracy = sess.run(\n",
    "            [global_step, val_summary_op, cnn.loss, cnn.rating_accuracy],\n",
    "            feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, rating_acc {:g}\".format(\n",
    "            time_str, step, loss, rating_accuracy))\n",
    "        if writer:\n",
    "            writer.add_summary(summaries, step)\n",
    "        return rating_accuracy\n",
    "\n",
    "    # Generate batches\n",
    "    batches = ratings.batch_iter(x_train, FLAGS.batch_size, FLAGS.num_epochs)\n",
    "    # Training loop. For each batch...\n",
    "    last_test_rating_accuracy = None\n",
    "    for batch_user_id, batch_movie_id, batch_rating in batches:\n",
    "        last_rating_accuracy = train_step(batch_user_id, batch_movie_id, batch_rating)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % FLAGS.evaluate_every == 0:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            ((val_user_id, val_movie_id, val_rating),) = ratings.batch_iter(x_val, len(x_val), 1)\n",
    "            if len(x_val) > 1024:\n",
    "                val_user_id, val_movie_id, val_rating = val_user_id[:1024], val_movie_id[:1024], val_rating[:1024]\n",
    "            last_test_rating_accuracy = \\\n",
    "                val_step(val_user_id, val_movie_id, val_rating, writer=val_summary_writer)\n",
    "            print(\"\")\n",
    "        if current_step % FLAGS.checkpoint_every == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "            pass\n",
    "    return (last_rating_accuracy, last_test_rating_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_precision(model, sess):\n",
    "    ranks = []\n",
    "    mrr = 0\n",
    "    precision_at_10 = 0\n",
    "    n = 50 # calculate only on first n users in validation set\n",
    "    for i in range(n):\n",
    "        user_id, movie_id, rating = x_val[i]\n",
    "        if i % 50 == 0:\n",
    "            print('{}...'.format(i))\n",
    "        num_movies = ratings.id_assigner.get_next_id()\n",
    "        batch_movie_id = np.arange(num_movies)\n",
    "        #batch_user_id = np.ones_like(batch_movie_id) * user_id\n",
    "        #batch_rating = np.zeros_like(batch_movie_id)\n",
    "        feed_dict = {\n",
    "            model.input_user_id: [user_id],\n",
    "            model.input_movie_id: batch_movie_id,\n",
    "        }\n",
    "        scores = sess.run(model.get_predictions(), feed_dict=feed_dict)\n",
    "        #print(scores)\n",
    "#         tmp1, tmp2, tmp3 = sess.run([model.tmp1, model.tmp2, model.tmp3], feed_dict=feed_dict)\n",
    "#         print(tmp1)\n",
    "#         print(tmp2)\n",
    "#         print(tmp3)\n",
    "        s = scores[movie_id] # the score for the correct movie\n",
    "        #print(s)\n",
    "        train_movies = x_train[x_train['user_id'] == user_id]['movie_id']\n",
    "        not_watched = (scores == scores) # all True\n",
    "        not_watched[train_movies] = False\n",
    "        higher_scores = (scores > s)    \n",
    "        rank = np.sum(higher_scores & not_watched) + 1\n",
    "        ranks.append(rank)\n",
    "        #print('for user_id {} the rank is {}'.format(user_id, rank))\n",
    "        mrr += 1. / rank\n",
    "        if rank <= 10:\n",
    "            precision_at_10 += 1\n",
    "    mrr /= n\n",
    "    precision_at_10 /= n\n",
    "    print('MRR is {}'.format(mrr))\n",
    "    print('Precision@10 is {}'.format(precision_at_10))\n",
    "    plt.hist(ranks, bins=np.logspace(0., np.log10(ratings.id_assigner.get_next_id()) , 20), normed=1)\n",
    "    plt.gca().set_xscale(\"log\")\n",
    "    plt.show()\n",
    "    return mrr, precision_at_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runall():\n",
    "    res = defaultdict(list)\n",
    "    with open('results.txt', 'a') as f:\n",
    "        for l2_reg_lambda in [1e-6]:\n",
    "            with tf.Graph().as_default():\n",
    "                session_conf = tf.ConfigProto(\n",
    "                    allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "                    log_device_placement=FLAGS.log_device_placement)\n",
    "                session_conf.gpu_options.allow_growth=True\n",
    "                sess = tf.Session(config=session_conf)\n",
    "                with sess.as_default():\n",
    "                    model = PredictionModel(\n",
    "                        num_users=num_users,\n",
    "                        num_movies=num_movies,\n",
    "                        embedding_dim=FLAGS.embedding_dim,\n",
    "                        l2_reg_lambda=l2_reg_lambda)\n",
    "                    for i in range(1):\n",
    "                        f.write('lambda: {}\\n'.format(l2_reg_lambda))\n",
    "                        last_accuracy = train(model, sess, 3e-3, 20000, 0.5)\n",
    "                        f.write('accuracy: {}\\n'.format(last_accuracy))\n",
    "                        res[l2_reg_lambda].append(last_accuracy)\n",
    "                        mrr, precision_at_10 = calc_precision(model, sess)\n",
    "                        f.write(repr((mrr, precision_at_10)) + '\\n')\n",
    "                        f.write('\\n')\n",
    "                        f.flush()\n",
    "                        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/tvromen/research/subtitles2/runs/1510230829\n",
      "\n",
      "2017-11-09T14:33:51.080313: step 100, loss 0.994387, rating_acc 1.00401->0.997121, rate 0.003\n",
      "2017-11-09T14:33:52.475220: step 200, loss 0.944222, rating_acc 0.981407->0.971563, rate 0.003\n",
      "2017-11-09T14:33:53.857085: step 300, loss 0.729766, rating_acc 0.866955->0.853975, rate 0.003\n",
      "2017-11-09T14:33:55.250120: step 400, loss 0.955757, rating_acc 0.99447->0.977245, rate 0.003\n",
      "2017-11-09T14:33:56.543749: step 500, loss 0.626912, rating_acc 0.809623->0.791128, rate 0.003\n",
      "2017-11-09T14:33:57.906578: step 600, loss 0.842457, rating_acc 0.937882->0.917147, rate 0.003\n",
      "2017-11-09T14:33:59.301737: step 700, loss 0.837352, rating_acc 0.935404->0.914214, rate 0.003\n",
      "2017-11-09T14:34:00.752518: step 800, loss 0.426112, rating_acc 0.670944->0.651355, rate 0.003\n",
      "2017-11-09T14:34:02.114719: step 900, loss 0.584721, rating_acc 0.785248->0.763231, rate 0.003\n",
      "2017-11-09T14:34:03.479200: step 1000, loss 0.45184, rating_acc 0.695668->0.670292, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:34:03.497833: step 1000, loss 0.816694, rating_acc 0.9023\n",
      "\n",
      "2017-11-09T14:34:04.886581: step 1100, loss 0.492411, rating_acc 0.729651->0.699635, rate 0.003\n",
      "2017-11-09T14:34:06.218264: step 1200, loss 0.529187, rating_acc 0.760164->0.725199, rate 0.003\n",
      "2017-11-09T14:34:07.354350: step 1300, loss 0.47577, rating_acc 0.723524->0.687129, rate 0.003\n",
      "2017-11-09T14:34:08.668802: step 1400, loss 0.387072, rating_acc 0.655806->0.618959, rate 0.003\n",
      "2017-11-09T14:34:09.979039: step 1500, loss 0.453236, rating_acc 0.704015->0.670054, rate 0.003\n",
      "2017-11-09T14:34:11.366461: step 1600, loss 0.233252, rating_acc 0.504849->0.478287, rate 0.003\n",
      "2017-11-09T14:34:12.759435: step 1700, loss 0.229584, rating_acc 0.506902->0.474273, rate 0.003\n",
      "2017-11-09T14:34:14.172836: step 1800, loss 0.251802, rating_acc 0.52477->0.496986, rate 0.003\n",
      "2017-11-09T14:34:15.633481: step 1900, loss 0.233882, rating_acc 0.51164->0.478448, rate 0.003\n",
      "2017-11-09T14:34:17.374442: step 2000, loss 0.249837, rating_acc 0.524466->0.49469, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:34:17.381926: step 2000, loss 0.832704, rating_acc 0.909717\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510230829/checkpoints/model-2000\n",
      "\n",
      "2017-11-09T14:34:19.266002: step 2100, loss 0.243835, rating_acc 0.524668->0.488425, rate 0.003\n",
      "2017-11-09T14:34:21.138212: step 2200, loss 0.280513, rating_acc 0.562261->0.524479, rate 0.003\n",
      "2017-11-09T14:34:22.948591: step 2300, loss 0.133686, rating_acc 0.38195->0.357905, rate 0.003\n",
      "2017-11-09T14:34:24.635423: step 2400, loss 0.130501, rating_acc 0.381021->0.353261, rate 0.003\n",
      "2017-11-09T14:34:26.401085: step 2500, loss 0.162326, rating_acc 0.427117->0.395599, rate 0.003\n",
      "2017-11-09T14:34:28.120592: step 2600, loss 0.15637, rating_acc 0.415741->0.38785, rate 0.003\n",
      "2017-11-09T14:34:29.945442: step 2700, loss 0.160551, rating_acc 0.429955->0.393051, rate 0.003\n",
      "2017-11-09T14:34:31.606684: step 2800, loss 0.167933, rating_acc 0.434089->0.402186, rate 0.003\n",
      "2017-11-09T14:34:33.490590: step 2900, loss 0.175781, rating_acc 0.448928->0.41169, rate 0.003\n",
      "2017-11-09T14:34:35.319356: step 3000, loss 0.213411, rating_acc 0.492198->0.454974, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:34:35.324031: step 3000, loss 0.869703, rating_acc 0.929136\n",
      "\n",
      "2017-11-09T14:34:37.044798: step 3100, loss 0.127315, rating_acc 0.37349->0.347595, rate 0.003\n",
      "2017-11-09T14:34:38.792229: step 3200, loss 0.0992025, rating_acc 0.335597->0.304437, rate 0.003\n",
      "2017-11-09T14:34:40.533123: step 3300, loss 0.0739444, rating_acc 0.291945->0.259581, rate 0.003\n",
      "2017-11-09T14:34:42.233696: step 3400, loss 0.092149, rating_acc 0.317221->0.292459, rate 0.003\n",
      "2017-11-09T14:34:43.993731: step 3500, loss 0.118597, rating_acc 0.370168->0.334512, rate 0.003\n",
      "2017-11-09T14:34:45.693148: step 3600, loss 0.0973233, rating_acc 0.333621->0.300913, rate 0.003\n",
      "2017-11-09T14:34:47.302020: step 3700, loss 0.0897598, rating_acc 0.318415->0.287932, rate 0.003\n",
      "2017-11-09T14:34:49.052066: step 3800, loss 0.0865285, rating_acc 0.315566->0.282122, rate 0.003\n",
      "2017-11-09T14:34:50.837278: step 3900, loss 0.0675505, rating_acc 0.275964->0.246134, rate 0.003\n",
      "2017-11-09T14:34:52.551759: step 4000, loss 0.0643309, rating_acc 0.272728->0.239417, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:34:52.556331: step 4000, loss 0.90979, rating_acc 0.950147\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510230829/checkpoints/model-4000\n",
      "\n",
      "2017-11-09T14:34:54.408604: step 4100, loss 0.0731951, rating_acc 0.286569->0.257162, rate 0.003\n",
      "2017-11-09T14:34:56.191614: step 4200, loss 0.0942369, rating_acc 0.33492->0.295137, rate 0.003\n",
      "2017-11-09T14:34:57.872870: step 4300, loss 0.111296, rating_acc 0.356296->0.322631, rate 0.003\n",
      "2017-11-09T14:34:59.641389: step 4400, loss 0.0939629, rating_acc 0.32854->0.294439, rate 0.003\n",
      "2017-11-09T14:35:01.360027: step 4500, loss 0.12171, rating_acc 0.374824->0.338169, rate 0.003\n",
      "2017-11-09T14:35:03.044140: step 4600, loss 0.0488636, rating_acc 0.232352->0.203568, rate 0.003\n",
      "2017-11-09T14:35:04.809043: step 4700, loss 0.0542252, rating_acc 0.255385->0.216358, rate 0.003\n",
      "2017-11-09T14:35:06.517307: step 4800, loss 0.0565, rating_acc 0.258759->0.221488, rate 0.003\n",
      "2017-11-09T14:35:08.272662: step 4900, loss 0.0600686, rating_acc 0.271115->0.229318, rate 0.003\n",
      "2017-11-09T14:35:10.026762: step 5000, loss 0.089079, rating_acc 0.327886->0.285556, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:35:10.036437: step 5000, loss 0.947525, rating_acc 0.96953\n",
      "\n",
      "2017-11-09T14:35:11.780993: step 5100, loss 0.0940572, rating_acc 0.330713->0.294018, rate 0.003\n",
      "2017-11-09T14:35:13.509129: step 5200, loss 0.0917485, rating_acc 0.329187->0.289954, rate 0.003\n",
      "2017-11-09T14:35:15.285737: step 5300, loss 0.100743, rating_acc 0.342188->0.304965, rate 0.003\n",
      "2017-11-09T14:35:17.028758: step 5400, loss 0.0873019, rating_acc 0.320201->0.282016, rate 0.003\n",
      "2017-11-09T14:35:18.667073: step 5500, loss 0.0580338, rating_acc 0.265007->0.224185, rate 0.003\n",
      "2017-11-09T14:35:20.450415: step 5600, loss 0.0635177, rating_acc 0.277256->0.236043, rate 0.003\n",
      "2017-11-09T14:35:22.164568: step 5700, loss 0.0744331, rating_acc 0.291403->0.258052, rate 0.003\n",
      "2017-11-09T14:35:23.932697: step 5800, loss 0.0634262, rating_acc 0.275047->0.235667, rate 0.003\n",
      "2017-11-09T14:35:25.660715: step 5900, loss 0.0681738, rating_acc 0.288467->0.245426, rate 0.003\n",
      "2017-11-09T14:35:27.423193: step 6000, loss 0.0851677, rating_acc 0.323063->0.27777, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:35:27.427637: step 6000, loss 0.981039, rating_acc 0.986422\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510230829/checkpoints/model-6000\n",
      "\n",
      "2017-11-09T14:35:30.383146: step 6100, loss 0.069244, rating_acc 0.288769->0.247282, rate 0.003\n",
      "2017-11-09T14:35:32.165473: step 6200, loss 0.0547401, rating_acc 0.251875->0.21606, rate 0.003\n",
      "2017-11-09T14:35:33.900695: step 6300, loss 0.0606091, rating_acc 0.265073->0.229238, rate 0.003\n",
      "2017-11-09T14:35:35.651780: step 6400, loss 0.0612276, rating_acc 0.267161->0.230575, rate 0.003\n",
      "2017-11-09T14:35:37.312641: step 6500, loss 0.0758396, rating_acc 0.303852->0.260268, rate 0.003\n",
      "2017-11-09T14:35:39.057025: step 6600, loss 0.0579916, rating_acc 0.261107->0.223236, rate 0.003\n",
      "2017-11-09T14:35:40.809263: step 6700, loss 0.0647455, rating_acc 0.282057->0.237764, rate 0.003\n",
      "2017-11-09T14:35:42.526677: step 6800, loss 0.0777948, rating_acc 0.306924->0.263682, rate 0.003\n",
      "2017-11-09T14:35:44.308723: step 6900, loss 0.0493885, rating_acc 0.238449->0.202694, rate 0.003\n",
      "2017-11-09T14:35:46.027578: step 7000, loss 0.0489332, rating_acc 0.239263->0.201625, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-09T14:35:46.036500: step 7000, loss 0.97743, rating_acc 0.984454\n",
      "\n",
      "2017-11-09T14:35:47.779050: step 7100, loss 0.0657462, rating_acc 0.279827->0.239711, rate 0.003\n",
      "2017-11-09T14:35:49.512651: step 7200, loss 0.0705803, rating_acc 0.288067->0.249538, rate 0.003\n",
      "2017-11-09T14:35:51.251901: step 7300, loss 0.0459918, rating_acc 0.226379->0.193999, rate 0.003\n",
      "2017-11-09T14:35:52.998971: step 7400, loss 0.0641963, rating_acc 0.277203->0.236184, rate 0.003\n",
      "2017-11-09T14:35:54.706341: step 7500, loss 0.0569567, rating_acc 0.259246->0.220175, rate 0.003\n",
      "2017-11-09T14:35:56.460654: step 7600, loss 0.0795595, rating_acc 0.31157->0.266474, rate 0.003\n",
      "0...\n",
      "MRR is 0.005763734532849677\n",
      "Precision@10 is 0.02\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feee5e697b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = runall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-09T14:36:08.580698\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {1e-06: [(0.23959386, 0.98445415)]})"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100K lines\n",
    "# all star ratings are considered watched\n",
    "# centered the rating data around 0\n",
    "# lambda = 5e-6\n",
    "# 10 epochs\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {5e-06: [(0.43487486, 0.86346918)]})"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "# centered the rating data around 0\n",
    "# lambda = 5e-6\n",
    "# 10 epochs\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/tvromen/research/subtitles2/runs/1510051919\n",
      "\n",
      "2017-11-07T12:52:01.503558: step 100, loss 0.907448, rating_acc 0.955825->0.952468, rate 0.003\n",
      "2017-11-07T12:52:03.152124: step 200, loss 0.367514, rating_acc 0.610292->0.605556, rate 0.003\n",
      "2017-11-07T12:52:04.849260: step 300, loss 2.61865, rating_acc 1.6325->1.61783, rate 0.003\n",
      "2017-11-07T12:52:06.514451: step 400, loss 0.977994, rating_acc 0.995856->0.987826, rate 0.003\n",
      "2017-11-07T12:52:08.200116: step 500, loss 1.25621, rating_acc 1.12934->1.11932, rate 0.003\n",
      "2017-11-07T12:52:09.872983: step 600, loss 0.892536, rating_acc 0.949428->0.942792, rate 0.003\n",
      "2017-11-07T12:52:11.562449: step 700, loss 0.880012, rating_acc 0.950038->0.935488, rate 0.003\n",
      "2017-11-07T12:52:13.192748: step 800, loss 0.890135, rating_acc 0.948482->0.940231, rate 0.003\n",
      "2017-11-07T12:52:14.856940: step 900, loss 0.825631, rating_acc 0.922051->0.904055, rate 0.003\n",
      "2017-11-07T12:52:15.954127: step 1000, loss 0.864978, rating_acc 0.93941->0.925224, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:52:15.963376: step 1000, loss 0.889722, rating_acc 0.938501\n",
      "\n",
      "2017-11-07T12:52:17.100005: step 1100, loss 0.540256, rating_acc 0.733922->0.727779, rate 0.003\n",
      "2017-11-07T12:52:18.769181: step 1200, loss 0.879147, rating_acc 0.960018->0.931773, rate 0.003\n",
      "2017-11-07T12:52:20.488511: step 1300, loss 1.06174, rating_acc 1.06964->1.02496, rate 0.003\n",
      "2017-11-07T12:52:22.167360: step 1400, loss 2.34802, rating_acc 1.55352->1.52859, rate 0.003\n",
      "2017-11-07T12:52:23.870527: step 1500, loss 0.63559, rating_acc 0.804757->0.789978, rate 0.003\n",
      "2017-11-07T12:52:25.551456: step 1600, loss 0.835437, rating_acc 0.915498->0.907498, rate 0.003\n",
      "2017-11-07T12:52:27.246920: step 1700, loss 0.811769, rating_acc 0.937163->0.893681, rate 0.003\n",
      "2017-11-07T12:52:28.934781: step 1800, loss 0.610977, rating_acc 0.791159->0.772724, rate 0.003\n",
      "2017-11-07T12:52:30.634980: step 1900, loss 1.80953, rating_acc 1.4216->1.33981, rate 0.003\n",
      "2017-11-07T12:52:32.322255: step 2000, loss 0.467254, rating_acc 0.691164->0.672221, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:52:32.327025: step 2000, loss 0.929763, rating_acc 0.956238\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-2000\n",
      "\n",
      "2017-11-07T12:52:34.078019: step 2100, loss 0.998826, rating_acc 1.05876->0.991773, rate 0.003\n",
      "2017-11-07T12:52:35.334222: step 2200, loss 0.565496, rating_acc 0.775669->0.741723, rate 0.003\n",
      "2017-11-07T12:52:36.445868: step 2300, loss 0.430297, rating_acc 0.659639->0.644241, rate 0.003\n",
      "2017-11-07T12:52:37.961541: step 2400, loss 0.523729, rating_acc 0.762378->0.712625, rate 0.003\n",
      "2017-11-07T12:52:39.606931: step 2500, loss 1.51773, rating_acc 1.2669->1.2255, rate 0.003\n",
      "2017-11-07T12:52:41.263626: step 2600, loss 0.920347, rating_acc 0.958699->0.950642, rate 0.003\n",
      "2017-11-07T12:52:42.937758: step 2700, loss 0.531425, rating_acc 0.722031->0.717127, rate 0.003\n",
      "2017-11-07T12:52:44.608508: step 2800, loss 0.684311, rating_acc 0.82004->0.81669, rate 0.003\n",
      "2017-11-07T12:52:46.267932: step 2900, loss 2.06472, rating_acc 1.43902->1.43074, rate 0.003\n",
      "2017-11-07T12:52:47.910033: step 3000, loss 0.363013, rating_acc 0.649238->0.587384, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:52:47.915606: step 3000, loss 0.937951, rating_acc 0.959144\n",
      "\n",
      "2017-11-07T12:52:49.557501: step 3100, loss 0.765697, rating_acc 0.925267->0.864519, rate 0.003\n",
      "2017-11-07T12:52:51.214014: step 3200, loss 1.38831, rating_acc 1.19645->1.17034, rate 0.003\n",
      "2017-11-07T12:52:52.851449: step 3300, loss 0.659515, rating_acc 0.830907->0.800347, rate 0.003\n",
      "2017-11-07T12:52:54.272778: step 3400, loss 0.716677, rating_acc 0.905736->0.835325, rate 0.003\n",
      "2017-11-07T12:52:55.379292: step 3500, loss 0.802255, rating_acc 0.918116->0.884836, rate 0.003\n",
      "2017-11-07T12:52:56.673611: step 3600, loss 0.591372, rating_acc 0.760914->0.75647, rate 0.003\n",
      "2017-11-07T12:52:58.339281: step 3700, loss 1.20377, rating_acc 1.09824->1.08846, rate 0.003\n",
      "2017-11-07T12:52:59.979500: step 3800, loss 0.608673, rating_acc 0.764245->0.767717, rate 0.003\n",
      "2017-11-07T12:53:01.622129: step 3900, loss 1.34591, rating_acc 1.19425->1.1514, rate 0.003\n",
      "2017-11-07T12:53:03.286000: step 4000, loss 0.572042, rating_acc 0.77725->0.742908, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:53:03.290424: step 4000, loss 0.931503, rating_acc 0.954658\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-4000\n",
      "\n",
      "2017-11-07T12:53:05.041230: step 4100, loss 0.533648, rating_acc 0.734233->0.716619, rate 0.003\n",
      "2017-11-07T12:53:06.706326: step 4200, loss 0.882641, rating_acc 0.981372->0.928481, rate 0.003\n",
      "2017-11-07T12:53:08.349886: step 4300, loss 1.31368, rating_acc 1.15753->1.13718, rate 0.003\n",
      "2017-11-07T12:53:09.997713: step 4400, loss 0.720729, rating_acc 0.855944->0.835865, rate 0.003\n",
      "2017-11-07T12:53:11.675903: step 4500, loss 0.45852, rating_acc 0.658077->0.660207, rate 0.003\n",
      "2017-11-07T12:53:13.224605: step 4600, loss 0.722376, rating_acc 0.852782->0.83645, rate 0.003\n",
      "2017-11-07T12:53:14.327227: step 4700, loss 0.407289, rating_acc 0.644759->0.620121, rate 0.003\n",
      "2017-11-07T12:53:15.542423: step 4800, loss 2.61089, rating_acc 1.6445->1.60875, rate 0.003\n",
      "2017-11-07T12:53:17.181896: step 4900, loss 1.08133, rating_acc 1.06227->1.02824, rate 0.003\n",
      "2017-11-07T12:53:18.850595: step 5000, loss 2.03389, rating_acc 1.47892->1.41766, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:53:18.854969: step 5000, loss 0.925298, rating_acc 0.949303\n",
      "\n",
      "2017-11-07T12:53:20.530941: step 5100, loss 0.735243, rating_acc 0.855989->0.843246, rate 0.003\n",
      "2017-11-07T12:53:22.192442: step 5200, loss 0.548036, rating_acc 0.750686->0.723624, rate 0.003\n",
      "2017-11-07T12:53:23.852259: step 5300, loss 0.542899, rating_acc 0.735406->0.720371, rate 0.003\n",
      "2017-11-07T12:53:25.508578: step 5400, loss 1.05286, rating_acc 1.0367->1.01452, rate 0.003\n",
      "2017-11-07T12:53:27.151694: step 5500, loss 0.711376, rating_acc 0.878279->0.829601, rate 0.003\n",
      "2017-11-07T12:53:28.813582: step 5600, loss 1.45328, rating_acc 1.28773->1.19585, rate 0.003\n",
      "2017-11-07T12:53:30.468314: step 5700, loss 0.601903, rating_acc 0.798064->0.760947, rate 0.003\n",
      "2017-11-07T12:53:32.118015: step 5800, loss 1.31141, rating_acc 1.15128->1.13497, rate 0.003\n",
      "2017-11-07T12:53:33.279510: step 5900, loss 1.02687, rating_acc 1.00996->1.00161, rate 0.003\n",
      "2017-11-07T12:53:34.385249: step 6000, loss 1.4742, rating_acc 1.24075->1.20445, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:53:34.387935: step 6000, loss 0.908672, rating_acc 0.940836\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-6000\n",
      "\n",
      "2017-11-07T12:53:36.223499: step 6100, loss 0.601312, rating_acc 0.803707->0.760308, rate 0.003\n",
      "2017-11-07T12:53:37.855309: step 6200, loss 0.389909, rating_acc 0.614415->0.605144, rate 0.003\n",
      "2017-11-07T12:53:39.539450: step 6300, loss 0.501283, rating_acc 0.698999->0.69123, rate 0.003\n",
      "2017-11-07T12:53:41.212785: step 6400, loss 0.816012, rating_acc 0.950476->0.889985, rate 0.003\n",
      "2017-11-07T12:53:42.874347: step 6500, loss 0.852691, rating_acc 0.954474->0.910191, rate 0.003\n",
      "2017-11-07T12:53:44.228723: step 6600, loss 0.722572, rating_acc 0.873115->0.835267, rate 0.003\n",
      "2017-11-07T12:53:45.359350: step 6700, loss 0.773543, rating_acc 0.976661->0.865422, rate 0.003\n",
      "2017-11-07T12:53:46.476057: step 6800, loss 1.44728, rating_acc 1.27121->1.19286, rate 0.003\n",
      "2017-11-07T12:53:48.132028: step 6900, loss 0.634353, rating_acc 0.817889->0.780938, rate 0.003\n",
      "2017-11-07T12:53:49.779395: step 7000, loss 1.186, rating_acc 1.09236->1.07784, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:53:49.784094: step 7000, loss 0.902607, rating_acc 0.937203\n",
      "\n",
      "2017-11-07T12:53:51.433283: step 7100, loss 1.36379, rating_acc 1.2065->1.15742, rate 0.003\n",
      "2017-11-07T12:53:53.084816: step 7200, loss 1.11512, rating_acc 1.05793->1.04464, rate 0.003\n",
      "2017-11-07T12:53:54.646333: step 7300, loss 0.658854, rating_acc 0.809332->0.796618, rate 0.003\n",
      "2017-11-07T12:53:55.750013: step 7400, loss 0.480453, rating_acc 0.709133->0.675425, rate 0.003\n",
      "2017-11-07T12:53:56.985233: step 7500, loss 0.559973, rating_acc 0.742006->0.731784, rate 0.003\n",
      "2017-11-07T12:53:58.644571: step 7600, loss 1.43001, rating_acc 1.20634->1.18568, rate 0.003\n",
      "2017-11-07T12:54:00.504399: step 7700, loss 0.932903, rating_acc 0.995304->0.953024, rate 0.003\n",
      "2017-11-07T12:54:02.167936: step 7800, loss 0.514331, rating_acc 0.708558->0.699126, rate 0.003\n",
      "2017-11-07T12:54:03.840619: step 7900, loss 0.810118, rating_acc 0.907208->0.8857, rate 0.003\n",
      "2017-11-07T12:54:05.513571: step 8000, loss 0.370045, rating_acc 0.589717->0.587257, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:54:05.517708: step 8000, loss 1.08192, rating_acc 1.02798\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-8000\n",
      "\n",
      "2017-11-07T12:54:07.273842: step 8100, loss 1.66485, rating_acc 1.31967->1.28032, rate 0.003\n",
      "2017-11-07T12:54:08.912424: step 8200, loss 1.63963, rating_acc 1.31708->1.27035, rate 0.003\n",
      "2017-11-07T12:54:10.579368: step 8300, loss 0.565961, rating_acc 0.735977->0.735031, rate 0.003\n",
      "2017-11-07T12:54:12.251077: step 8400, loss 1.14932, rating_acc 1.10731->1.06019, rate 0.003\n",
      "2017-11-07T12:54:13.755164: step 8500, loss 0.629638, rating_acc 0.812451->0.776393, rate 0.003\n",
      "2017-11-07T12:54:14.849557: step 8600, loss 1.3356, rating_acc 1.15219->1.14399, rate 0.003\n",
      "2017-11-07T12:54:16.089239: step 8700, loss 0.797829, rating_acc 0.891534->0.878138, rate 0.003\n",
      "2017-11-07T12:54:17.734993: step 8800, loss 0.755303, rating_acc 0.882023->0.853934, rate 0.003\n",
      "2017-11-07T12:54:19.369601: step 8900, loss 0.965557, rating_acc 0.980395->0.969473, rate 0.003\n",
      "2017-11-07T12:54:21.027870: step 9000, loss 0.977782, rating_acc 0.991422->0.976009, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:54:21.032644: step 9000, loss 0.993571, rating_acc 0.984065\n",
      "\n",
      "2017-11-07T12:54:22.703504: step 9100, loss 0.512704, rating_acc 0.706123->0.6984, rate 0.003\n",
      "2017-11-07T12:54:24.348821: step 9200, loss 0.404179, rating_acc 0.638991->0.615946, rate 0.003\n",
      "2017-11-07T12:54:26.004863: step 9300, loss 0.449244, rating_acc 0.669028->0.651593, rate 0.003\n",
      "2017-11-07T12:54:27.659396: step 9400, loss 0.609562, rating_acc 0.791344->0.764899, rate 0.003\n",
      "2017-11-07T12:54:29.310996: step 9500, loss 0.926215, rating_acc 0.965632->0.949622, rate 0.003\n",
      "2017-11-07T12:54:30.960388: step 9600, loss 0.949681, rating_acc 0.971631->0.961782, rate 0.003\n",
      "2017-11-07T12:54:32.593731: step 9700, loss 0.734855, rating_acc 0.864198->0.842944, rate 0.003\n",
      "2017-11-07T12:54:33.788763: step 9800, loss 0.605637, rating_acc 0.774703->0.762623, rate 0.003\n",
      "2017-11-07T12:54:34.901281: step 9900, loss 0.662556, rating_acc 0.821117->0.79929, rate 0.003\n",
      "2017-11-07T12:54:36.480932: step 10000, loss 1.05628, rating_acc 1.0534->1.01605, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:54:36.484942: step 10000, loss 0.92947, rating_acc 0.951602\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-10000\n",
      "\n",
      "2017-11-07T12:54:38.254843: step 10100, loss 0.696822, rating_acc 0.82571->0.820485, rate 0.003\n",
      "2017-11-07T12:54:39.950966: step 10200, loss 0.495668, rating_acc 0.734237->0.68715, rate 0.003\n",
      "2017-11-07T12:54:41.599988: step 10300, loss 1.35873, rating_acc 1.17544->1.15546, rate 0.003\n",
      "2017-11-07T12:54:43.277082: step 10400, loss 0.759732, rating_acc 0.882621->0.857976, rate 0.003\n",
      "2017-11-07T12:54:44.953600: step 10500, loss 0.998525, rating_acc 1.00742->0.987479, rate 0.003\n",
      "2017-11-07T12:54:46.596415: step 10600, loss 0.282696, rating_acc 0.509645->0.509161, rate 0.003\n",
      "2017-11-07T12:54:48.266558: step 10700, loss 0.486264, rating_acc 0.723589->0.680302, rate 0.003\n",
      "2017-11-07T12:54:49.948101: step 10800, loss 0.549577, rating_acc 0.727432->0.725408, rate 0.003\n",
      "2017-11-07T12:54:51.631857: step 10900, loss 0.442651, rating_acc 0.651393->0.647575, rate 0.003\n",
      "2017-11-07T12:54:52.885696: step 11000, loss 0.540058, rating_acc 0.737632->0.718757, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:54:52.888508: step 11000, loss 0.902492, rating_acc 0.937574\n",
      "\n",
      "2017-11-07T12:54:53.992580: step 11100, loss 0.669141, rating_acc 0.823449->0.803265, rate 0.003\n",
      "2017-11-07T12:54:55.392013: step 11200, loss 0.900685, rating_acc 0.954513->0.936452, rate 0.003\n",
      "2017-11-07T12:54:57.045442: step 11300, loss 1.15495, rating_acc 1.08157->1.06383, rate 0.003\n",
      "2017-11-07T12:54:58.700378: step 11400, loss 0.881367, rating_acc 0.941979->0.926398, rate 0.003\n",
      "2017-11-07T12:55:00.359188: step 11500, loss 1.32255, rating_acc 1.15736->1.13986, rate 0.003\n",
      "2017-11-07T12:55:02.037593: step 11600, loss 0.43906, rating_acc 0.650517->0.645014, rate 0.003\n",
      "2017-11-07T12:55:03.698204: step 11700, loss 0.84754, rating_acc 0.913627->0.908037, rate 0.003\n",
      "2017-11-07T12:55:05.379070: step 11800, loss 1.24806, rating_acc 1.12904->1.10672, rate 0.003\n",
      "2017-11-07T12:55:07.069166: step 11900, loss 0.96618, rating_acc 0.997943->0.971079, rate 0.003\n",
      "2017-11-07T12:55:08.732724: step 12000, loss 0.833554, rating_acc 0.914732->0.900103, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:55:08.738141: step 12000, loss 0.873805, rating_acc 0.922191\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-12000\n",
      "\n",
      "2017-11-07T12:55:10.511716: step 12100, loss 0.349729, rating_acc 0.589573->0.570634, rate 0.003\n",
      "2017-11-07T12:55:11.951954: step 12200, loss 0.666627, rating_acc 0.814533->0.801494, rate 0.003\n",
      "2017-11-07T12:55:13.061173: step 12300, loss 0.328641, rating_acc 0.581472->0.551823, rate 0.003\n",
      "2017-11-07T12:55:14.379425: step 12400, loss 0.843237, rating_acc 0.942736->0.905175, rate 0.003\n",
      "2017-11-07T12:55:16.025338: step 12500, loss 0.615352, rating_acc 0.804237->0.768904, rate 0.003\n",
      "2017-11-07T12:55:17.692984: step 12600, loss 0.553966, rating_acc 0.743871->0.727889, rate 0.003\n",
      "2017-11-07T12:55:19.370695: step 12700, loss 0.527115, rating_acc 0.727655->0.709243, rate 0.003\n",
      "2017-11-07T12:55:21.005209: step 12800, loss 0.59917, rating_acc 0.767526->0.758367, rate 0.003\n",
      "2017-11-07T12:55:22.672829: step 12900, loss 0.694231, rating_acc 0.835163->0.818684, rate 0.003\n",
      "2017-11-07T12:55:24.351103: step 13000, loss 0.786913, rating_acc 0.889838->0.87355, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:55:24.356389: step 13000, loss 0.867592, rating_acc 0.918569\n",
      "\n",
      "2017-11-07T12:55:25.999056: step 13100, loss 0.724346, rating_acc 0.864355->0.837114, rate 0.003\n",
      "2017-11-07T12:55:27.658950: step 13200, loss 0.548678, rating_acc 0.742042->0.724673, rate 0.003\n",
      "2017-11-07T12:55:29.333153: step 13300, loss 0.539822, rating_acc 0.733799->0.718579, rate 0.003\n",
      "2017-11-07T12:55:30.936064: step 13400, loss 0.894368, rating_acc 0.991327->0.933298, rate 0.003\n",
      "2017-11-07T12:55:32.052335: step 13500, loss 0.56753, rating_acc 0.746137->0.737708, rate 0.003\n",
      "2017-11-07T12:55:33.182198: step 13600, loss 1.15476, rating_acc 1.09445->1.06378, rate 0.003\n",
      "2017-11-07T12:55:34.864605: step 13700, loss 0.725238, rating_acc 0.854004->0.837946, rate 0.003\n",
      "2017-11-07T12:55:36.534261: step 13800, loss 0.669484, rating_acc 0.811177->0.80392, rate 0.003\n",
      "2017-11-07T12:55:38.190918: step 13900, loss 0.668918, rating_acc 0.810686->0.80357, rate 0.003\n",
      "2017-11-07T12:55:39.865936: step 14000, loss 0.555863, rating_acc 0.755913->0.729946, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:55:39.870323: step 14000, loss 0.862336, rating_acc 0.91613\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-14000\n",
      "\n",
      "2017-11-07T12:55:41.667102: step 14100, loss 0.52846, rating_acc 0.742342->0.710807, rate 0.003\n",
      "2017-11-07T12:55:43.334096: step 14200, loss 0.751406, rating_acc 0.870586->0.853316, rate 0.003\n",
      "2017-11-07T12:55:45.008022: step 14300, loss 0.965184, rating_acc 0.981199->0.970402, rate 0.003\n",
      "2017-11-07T12:55:46.691476: step 14400, loss 0.970179, rating_acc 0.994483->0.972957, rate 0.003\n",
      "2017-11-07T12:55:48.383525: step 14500, loss 1.08552, rating_acc 1.05047->1.0306, rate 0.003\n",
      "2017-11-07T12:55:50.056521: step 14600, loss 1.07717, rating_acc 1.03824->1.02657, rate 0.003\n",
      "2017-11-07T12:55:51.223510: step 14700, loss 0.706996, rating_acc 0.843371->0.826976, rate 0.003\n",
      "2017-11-07T12:55:52.336527: step 14800, loss 0.419944, rating_acc 0.659078->0.629969, rate 0.003\n",
      "2017-11-07T12:55:53.972585: step 14900, loss 0.653375, rating_acc 0.802718->0.79389, rate 0.003\n",
      "2017-11-07T12:55:55.660675: step 15000, loss 0.534335, rating_acc 0.728529->0.714699, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:55:55.664773: step 15000, loss 0.855027, rating_acc 0.911859\n",
      "\n",
      "2017-11-07T12:55:57.332363: step 15100, loss 1.22022, rating_acc 1.15832->1.09394, rate 0.003\n",
      "2017-11-07T12:55:58.993606: step 15200, loss 0.702805, rating_acc 0.838577->0.824247, rate 0.003\n",
      "2017-11-07T12:56:00.857712: step 15300, loss 0.909728, rating_acc 0.95355->0.941291, rate 0.003\n",
      "2017-11-07T12:56:02.550627: step 15400, loss 0.994299, rating_acc 1.00333->0.985159, rate 0.003\n",
      "2017-11-07T12:56:04.230991: step 15500, loss 0.643863, rating_acc 0.792826->0.787492, rate 0.003\n",
      "2017-11-07T12:56:05.888877: step 15600, loss 0.151659, rating_acc 0.3739->0.357861, rate 0.003\n",
      "2017-11-07T12:56:07.553495: step 15700, loss 0.445965, rating_acc 0.699931->0.6497, rate 0.003\n",
      "2017-11-07T12:56:09.232002: step 15800, loss 1.19732, rating_acc 1.10184->1.08308, rate 0.003\n",
      "2017-11-07T12:56:10.439776: step 15900, loss 1.03018, rating_acc 1.008->1.00293, rate 0.003\n",
      "2017-11-07T12:56:11.549663: step 16000, loss 1.58567, rating_acc 1.27998->1.24966, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:56:11.552347: step 16000, loss 0.903589, rating_acc 0.937857\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-16000\n",
      "\n",
      "2017-11-07T12:56:13.228411: step 16100, loss 0.907859, rating_acc 0.953301->0.939519, rate 0.003\n",
      "2017-11-07T12:56:14.819286: step 16200, loss 0.479707, rating_acc 0.684177->0.673569, rate 0.003\n",
      "2017-11-07T12:56:15.934203: step 16300, loss 0.692087, rating_acc 0.80984->0.815794, rate 0.003\n",
      "2017-11-07T12:56:17.055631: step 16400, loss 0.648197, rating_acc 0.810277->0.788423, rate 0.003\n",
      "2017-11-07T12:56:18.531461: step 16500, loss 0.789454, rating_acc 0.883432->0.873625, rate 0.003\n",
      "2017-11-07T12:56:20.185009: step 16600, loss 0.175909, rating_acc 0.396803->0.387132, rate 0.003\n",
      "2017-11-07T12:56:21.850335: step 16700, loss 0.472681, rating_acc 0.695767->0.668408, rate 0.003\n",
      "2017-11-07T12:56:23.473692: step 16800, loss 0.764584, rating_acc 0.881852->0.859553, rate 0.003\n",
      "2017-11-07T12:56:25.122082: step 16900, loss 0.884726, rating_acc 0.942877->0.926814, rate 0.003\n",
      "2017-11-07T12:56:26.775249: step 17000, loss 0.377196, rating_acc 0.63908->0.592652, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:56:26.780456: step 17000, loss 0.931747, rating_acc 0.951729\n",
      "\n",
      "2017-11-07T12:56:28.449168: step 17100, loss 0.654902, rating_acc 0.828748->0.792991, rate 0.003\n",
      "2017-11-07T12:56:30.135867: step 17200, loss 0.378137, rating_acc 0.639426->0.59318, rate 0.003\n",
      "2017-11-07T12:56:31.806333: step 17300, loss 0.905816, rating_acc 0.97713->0.937746, rate 0.003\n",
      "2017-11-07T12:56:32.929616: step 17400, loss 0.624706, rating_acc 0.795912->0.773657, rate 0.003\n",
      "2017-11-07T12:56:34.052901: step 17500, loss 1.16776, rating_acc 1.09015->1.06848, rate 0.003\n",
      "2017-11-07T12:56:35.721697: step 17600, loss 0.563693, rating_acc 0.750897->0.73321, rate 0.003\n",
      "2017-11-07T12:56:37.408885: step 17700, loss 0.885259, rating_acc 0.954158->0.926788, rate 0.003\n",
      "2017-11-07T12:56:39.094273: step 17800, loss 0.547124, rating_acc 0.764965->0.721776, rate 0.003\n",
      "2017-11-07T12:56:40.756330: step 17900, loss 1.30973, rating_acc 1.15111->1.13277, rate 0.003\n",
      "2017-11-07T12:56:42.416698: step 18000, loss 0.672549, rating_acc 0.816354->0.803706, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:56:42.420391: step 18000, loss 0.902211, rating_acc 0.935738\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-18000\n",
      "\n",
      "2017-11-07T12:56:44.213284: step 18100, loss 0.377574, rating_acc 0.608502->0.59225, rate 0.003\n",
      "2017-11-07T12:56:45.878953: step 18200, loss 0.84126, rating_acc 0.934855->0.902455, rate 0.003\n",
      "2017-11-07T12:56:47.553646: step 18300, loss 0.925551, rating_acc 0.997354->0.948064, rate 0.003\n",
      "2017-11-07T12:56:49.212598: step 18400, loss 0.348198, rating_acc 0.580117->0.566562, rate 0.003\n",
      "2017-11-07T12:56:50.899132: step 18500, loss 0.893076, rating_acc 0.948943->0.93044, rate 0.003\n",
      "2017-11-07T12:56:52.119518: step 18600, loss 0.750111, rating_acc 0.87489->0.849996, rate 0.003\n",
      "2017-11-07T12:56:53.237870: step 18700, loss 0.459621, rating_acc 0.691269->0.656834, rate 0.003\n",
      "2017-11-07T12:56:54.746402: step 18800, loss 0.32396, rating_acc 0.551757->0.543799, rate 0.003\n",
      "2017-11-07T12:56:56.416284: step 18900, loss 0.982913, rating_acc 1.01124->0.977251, rate 0.003\n",
      "2017-11-07T12:56:58.067545: step 19000, loss 0.609184, rating_acc 0.827972->0.762269, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:56:58.071784: step 19000, loss 0.899536, rating_acc 0.933491\n",
      "\n",
      "2017-11-07T12:56:59.739064: step 19100, loss 0.442839, rating_acc 0.688011->0.643848, rate 0.003\n",
      "2017-11-07T12:57:01.388265: step 19200, loss 0.55119, rating_acc 0.750672->0.72289, rate 0.003\n",
      "2017-11-07T12:57:03.100077: step 19300, loss 0.401743, rating_acc 0.643323->0.61093, rate 0.003\n",
      "2017-11-07T12:57:04.804065: step 19400, loss 1.16148, rating_acc 1.09318->1.06417, rate 0.003\n",
      "2017-11-07T12:57:06.472345: step 19500, loss 1.61034, rating_acc 1.27434->1.25736, rate 0.003\n",
      "2017-11-07T12:57:08.136777: step 19600, loss 0.659802, rating_acc 0.830326->0.793866, rate 0.003\n",
      "2017-11-07T12:57:09.828447: step 19700, loss 0.290735, rating_acc 0.537744->0.510352, rate 0.003\n",
      "2017-11-07T12:57:11.233300: step 19800, loss 1.55583, rating_acc 1.27887->1.2349, rate 0.003\n",
      "2017-11-07T12:57:12.342896: step 19900, loss 0.669317, rating_acc 0.821645->0.798989, rate 0.003\n",
      "2017-11-07T12:57:13.739405: step 20000, loss 0.925316, rating_acc 0.97094->0.945817, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:57:13.742710: step 20000, loss 0.884731, rating_acc 0.924113\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-20000\n",
      "\n",
      "2017-11-07T12:57:15.502570: step 20100, loss 0.695397, rating_acc 0.825695->0.815389, rate 0.0015\n",
      "2017-11-07T12:57:17.164978: step 20200, loss 0.701452, rating_acc 0.840096->0.819191, rate 0.0015\n",
      "2017-11-07T12:57:18.844046: step 20300, loss 0.325719, rating_acc 0.550381->0.543525, rate 0.0015\n",
      "2017-11-07T12:57:20.508671: step 20400, loss 0.958091, rating_acc 0.980005->0.963314, rate 0.0015\n",
      "2017-11-07T12:57:22.154734: step 20500, loss 0.732248, rating_acc 0.851134->0.838003, rate 0.0015\n",
      "2017-11-07T12:57:23.822312: step 20600, loss 0.64301, rating_acc 0.814759->0.783111, rate 0.0015\n",
      "2017-11-07T12:57:25.508470: step 20700, loss 0.453805, rating_acc 0.65798->0.651324, rate 0.0015\n",
      "2017-11-07T12:57:27.158536: step 20800, loss 0.621348, rating_acc 0.793988->0.769416, rate 0.0015\n",
      "2017-11-07T12:57:28.851008: step 20900, loss 0.498361, rating_acc 0.692381->0.684913, rate 0.0015\n",
      "2017-11-07T12:57:30.348759: step 21000, loss 0.618545, rating_acc 0.786773->0.767814, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:57:30.351410: step 21000, loss 0.849081, rating_acc 0.90558\n",
      "\n",
      "2017-11-07T12:57:31.455485: step 21100, loss 0.525491, rating_acc 0.71369->0.70464, rate 0.0015\n",
      "2017-11-07T12:57:32.711102: step 21200, loss 0.354038, rating_acc 0.580423->0.570327, rate 0.0015\n",
      "2017-11-07T12:57:34.395292: step 21300, loss 1.13558, rating_acc 1.06632->1.05214, rate 0.0015\n",
      "2017-11-07T12:57:36.066294: step 21400, loss 0.624553, rating_acc 0.784598->0.772061, rate 0.0015\n",
      "2017-11-07T12:57:37.729646: step 21500, loss 0.679373, rating_acc 0.829277->0.806836, rate 0.0015\n",
      "2017-11-07T12:57:39.405912: step 21600, loss 0.906049, rating_acc 0.948293->0.936935, rate 0.0015\n",
      "2017-11-07T12:57:41.074711: step 21700, loss 0.94433, rating_acc 0.974943->0.957155, rate 0.0015\n",
      "2017-11-07T12:57:42.737323: step 21800, loss 0.887737, rating_acc 0.944492->0.927135, rate 0.0015\n",
      "2017-11-07T12:57:44.402093: step 21900, loss 0.776407, rating_acc 0.874507->0.865018, rate 0.0015\n",
      "2017-11-07T12:57:46.101216: step 22000, loss 0.652295, rating_acc 0.812576->0.790077, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:57:46.106934: step 22000, loss 0.836242, rating_acc 0.898982\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-22000\n",
      "\n",
      "2017-11-07T12:57:47.905014: step 22100, loss 0.671197, rating_acc 0.817837->0.80199, rate 0.0015\n",
      "2017-11-07T12:57:49.511999: step 22200, loss 0.637849, rating_acc 0.794133->0.780981, rate 0.0015\n",
      "2017-11-07T12:57:50.635930: step 22300, loss 0.83123, rating_acc 0.90627->0.89635, rate 0.0015\n",
      "2017-11-07T12:57:51.817767: step 22400, loss 0.429678, rating_acc 0.645393->0.633965, rate 0.0015\n",
      "2017-11-07T12:57:53.494127: step 22500, loss 0.713421, rating_acc 0.840668->0.828055, rate 0.0015\n",
      "2017-11-07T12:57:55.168781: step 22600, loss 0.505829, rating_acc 0.696885->0.691421, rate 0.0015\n",
      "2017-11-07T12:57:56.867320: step 22700, loss 0.591665, rating_acc 0.766694->0.75096, rate 0.0015\n",
      "2017-11-07T12:57:58.550076: step 22800, loss 0.602256, rating_acc 0.775769->0.758046, rate 0.0015\n",
      "2017-11-07T12:58:00.426330: step 22900, loss 0.179811, rating_acc 0.401657->0.389918, rate 0.0015\n",
      "2017-11-07T12:58:02.094521: step 23000, loss 0.55968, rating_acc 0.74547->0.729353, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:58:02.098837: step 23000, loss 0.832377, rating_acc 0.897025\n",
      "\n",
      "2017-11-07T12:58:03.747179: step 23100, loss 0.508701, rating_acc 0.701195->0.693475, rate 0.0015\n",
      "2017-11-07T12:58:05.440371: step 23200, loss 3.49001, rating_acc 1.89628->1.86073, rate 0.0015\n",
      "2017-11-07T12:58:07.093786: step 23300, loss 0.485102, rating_acc 0.688631->0.676313, rate 0.0015\n",
      "2017-11-07T12:58:08.726814: step 23400, loss 0.567933, rating_acc 0.738417->0.735022, rate 0.0015\n",
      "2017-11-07T12:58:09.860867: step 23500, loss 0.535999, rating_acc 0.72096->0.71303, rate 0.0015\n",
      "2017-11-07T12:58:10.996859: step 23600, loss 0.393159, rating_acc 0.628388->0.604777, rate 0.0015\n",
      "2017-11-07T12:58:12.671452: step 23700, loss 1.32125, rating_acc 1.15133->1.13753, rate 0.0015\n",
      "2017-11-07T12:58:14.351074: step 23800, loss 0.53239, rating_acc 0.722562->0.7104, rate 0.0015\n",
      "2017-11-07T12:58:16.048425: step 23900, loss 0.703752, rating_acc 0.845483->0.822209, rate 0.0015\n",
      "2017-11-07T12:58:17.716032: step 24000, loss 0.363221, rating_acc 0.588889->0.579308, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:58:17.719985: step 24000, loss 0.802268, rating_acc 0.880139\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-24000\n",
      "\n",
      "2017-11-07T12:58:19.509131: step 24100, loss 1.37197, rating_acc 1.17758->1.15953, rate 0.0015\n",
      "2017-11-07T12:58:21.186592: step 24200, loss 0.616173, rating_acc 0.775743->0.767313, rate 0.0015\n",
      "2017-11-07T12:58:22.855751: step 24300, loss 1.46988, rating_acc 1.21676->1.2011, rate 0.0015\n",
      "2017-11-07T12:58:24.516547: step 24400, loss 0.698748, rating_acc 0.833741->0.819561, rate 0.0015\n",
      "2017-11-07T12:58:26.186016: step 24500, loss 0.550336, rating_acc 0.73605->0.723438, rate 0.0015\n",
      "2017-11-07T12:58:27.835723: step 24600, loss 0.465117, rating_acc 0.675731->0.661981, rate 0.0015\n",
      "2017-11-07T12:58:29.001902: step 24700, loss 0.454665, rating_acc 0.664732->0.654081, rate 0.0015\n",
      "2017-11-07T12:58:30.120640: step 24800, loss 0.553617, rating_acc 0.738077->0.725784, rate 0.0015\n",
      "2017-11-07T12:58:31.779974: step 24900, loss 0.682661, rating_acc 0.823376->0.809784, rate 0.0015\n",
      "2017-11-07T12:58:33.461654: step 25000, loss 0.704284, rating_acc 0.833515->0.823131, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:58:33.465826: step 25000, loss 0.800787, rating_acc 0.8798\n",
      "\n",
      "2017-11-07T12:58:35.132637: step 25100, loss 0.684013, rating_acc 0.819606->0.810767, rate 0.0015\n",
      "2017-11-07T12:58:36.790119: step 25200, loss 0.754003, rating_acc 0.864097->0.852893, rate 0.0015\n",
      "2017-11-07T12:58:38.494361: step 25300, loss 0.531089, rating_acc 0.720073->0.710325, rate 0.0015\n",
      "2017-11-07T12:58:40.143160: step 25400, loss 0.754592, rating_acc 0.871487->0.853332, rate 0.0015\n",
      "2017-11-07T12:58:41.817157: step 25500, loss 0.742192, rating_acc 0.860405->0.846041, rate 0.0015\n",
      "2017-11-07T12:58:43.508851: step 25600, loss 0.685095, rating_acc 0.827782->0.811606, rate 0.0015\n",
      "2017-11-07T12:58:44.764913: step 25700, loss 0.538746, rating_acc 0.727984->0.715804, rate 0.0015\n",
      "2017-11-07T12:58:45.877780: step 25800, loss 1.59063, rating_acc 1.26034->1.25073, rate 0.0015\n",
      "2017-11-07T12:58:47.150048: step 25900, loss 0.590357, rating_acc 0.767099->0.751074, rate 0.0015\n",
      "2017-11-07T12:58:48.838802: step 26000, loss 0.640504, rating_acc 0.806157->0.783736, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:58:48.842458: step 26000, loss 0.800593, rating_acc 0.879961\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-26000\n",
      "\n",
      "2017-11-07T12:58:50.469065: step 26100, loss 0.515159, rating_acc 0.707859->0.699165, rate 0.0015\n",
      "2017-11-07T12:58:51.595944: step 26200, loss 0.337951, rating_acc 0.572784->0.558215, rate 0.0015\n",
      "2017-11-07T12:58:52.844021: step 26300, loss 0.420449, rating_acc 0.643602->0.627777, rate 0.0015\n",
      "2017-11-07T12:58:54.517691: step 26400, loss 0.650339, rating_acc 0.805046->0.789882, rate 0.0015\n",
      "2017-11-07T12:58:56.182759: step 26500, loss 0.787834, rating_acc 0.894287->0.872662, rate 0.0015\n",
      "2017-11-07T12:58:57.876413: step 26600, loss 0.596606, rating_acc 0.767054->0.755259, rate 0.0015\n",
      "2017-11-07T12:58:59.584034: step 26700, loss 0.609369, rating_acc 0.778541->0.763534, rate 0.0015\n",
      "2017-11-07T12:59:01.271879: step 26800, loss 0.60921, rating_acc 0.775326->0.763438, rate 0.0015\n",
      "2017-11-07T12:59:02.949350: step 26900, loss 0.608818, rating_acc 0.770846->0.763239, rate 0.0015\n",
      "2017-11-07T12:59:04.625381: step 27000, loss 0.463212, rating_acc 0.671692->0.660945, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:59:04.628720: step 27000, loss 0.804489, rating_acc 0.882114\n",
      "\n",
      "2017-11-07T12:59:06.322108: step 27100, loss 0.394567, rating_acc 0.624054->0.606604, rate 0.0015\n",
      "2017-11-07T12:59:08.019175: step 27200, loss 0.547008, rating_acc 0.733329->0.721368, rate 0.0015\n",
      "2017-11-07T12:59:09.630118: step 27300, loss 0.303157, rating_acc 0.534169->0.525557, rate 0.0015\n",
      "2017-11-07T12:59:10.753038: step 27400, loss 0.855895, rating_acc 0.927999->0.910363, rate 0.0015\n",
      "2017-11-07T12:59:11.947799: step 27500, loss 1.43764, rating_acc 1.20328->1.1876, rate 0.0015\n",
      "2017-11-07T12:59:13.618238: step 27600, loss 0.546011, rating_acc 0.732102->0.720253, rate 0.0015\n",
      "2017-11-07T12:59:15.281384: step 27700, loss 0.767577, rating_acc 0.875353->0.860369, rate 0.0015\n",
      "2017-11-07T12:59:16.965732: step 27800, loss 0.70776, rating_acc 0.835125->0.824747, rate 0.0015\n",
      "2017-11-07T12:59:18.639581: step 27900, loss 1.27199, rating_acc 1.13146->1.1155, rate 0.0015\n",
      "2017-11-07T12:59:20.299420: step 28000, loss 0.873771, rating_acc 0.925693->0.919748, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:59:20.304096: step 28000, loss 0.802538, rating_acc 0.880172\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-28000\n",
      "\n",
      "2017-11-07T12:59:22.068328: step 28100, loss 0.341415, rating_acc 0.570313->0.559836, rate 0.0015\n",
      "2017-11-07T12:59:23.730859: step 28200, loss 0.496733, rating_acc 0.69316->0.684601, rate 0.0015\n",
      "2017-11-07T12:59:25.413408: step 28300, loss 0.815515, rating_acc 0.896078->0.887355, rate 0.0015\n",
      "2017-11-07T12:59:27.069210: step 28400, loss 0.62477, rating_acc 0.789256->0.772412, rate 0.0015\n",
      "2017-11-07T12:59:28.743246: step 28500, loss 0.611313, rating_acc 0.775471->0.763584, rate 0.0015\n",
      "2017-11-07T12:59:29.876425: step 28600, loss 0.578292, rating_acc 0.752507->0.741607, rate 0.0015\n",
      "2017-11-07T12:59:30.977443: step 28700, loss 0.583749, rating_acc 0.753423->0.745124, rate 0.0015\n",
      "2017-11-07T12:59:32.591614: step 28800, loss 0.528697, rating_acc 0.718897->0.707144, rate 0.0015\n",
      "2017-11-07T12:59:34.266918: step 28900, loss 1.02873, rating_acc 1.00866->1.00002, rate 0.0015\n",
      "2017-11-07T12:59:35.953666: step 29000, loss 0.504714, rating_acc 0.698855->0.689867, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:59:35.958536: step 29000, loss 0.801703, rating_acc 0.87915\n",
      "\n",
      "2017-11-07T12:59:37.611244: step 29100, loss 0.988549, rating_acc 0.98977->0.979581, rate 0.0015\n",
      "2017-11-07T12:59:39.296460: step 29200, loss 0.735403, rating_acc 0.853067->0.840452, rate 0.0015\n",
      "2017-11-07T12:59:40.956738: step 29300, loss 0.446381, rating_acc 0.658378->0.645926, rate 0.0015\n",
      "2017-11-07T12:59:42.630237: step 29400, loss 1.09431, rating_acc 1.04955->1.03197, rate 0.0015\n",
      "2017-11-07T12:59:44.314567: step 29500, loss 0.498099, rating_acc 0.695649->0.68454, rate 0.0015\n",
      "2017-11-07T12:59:45.993611: step 29600, loss 0.479294, rating_acc 0.682563->0.670596, rate 0.0015\n",
      "2017-11-07T12:59:47.696050: step 29700, loss 0.629937, rating_acc 0.785476->0.774716, rate 0.0015\n",
      "2017-11-07T12:59:48.969061: step 29800, loss 0.699266, rating_acc 0.835407->0.818139, rate 0.0015\n",
      "2017-11-07T12:59:50.092601: step 29900, loss 0.850318, rating_acc 0.920466->0.905732, rate 0.0015\n",
      "2017-11-07T12:59:51.569980: step 30000, loss 0.394005, rating_acc 0.609939->0.603234, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T12:59:51.576463: step 30000, loss 0.800442, rating_acc 0.877684\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-30000\n",
      "\n",
      "2017-11-07T12:59:53.347837: step 30100, loss 0.690938, rating_acc 0.828695->0.812842, rate 0.0015\n",
      "2017-11-07T12:59:54.983324: step 30200, loss 0.673569, rating_acc 0.812585->0.802024, rate 0.0015\n",
      "2017-11-07T12:59:56.643301: step 30300, loss 0.687561, rating_acc 0.827367->0.810603, rate 0.0015\n",
      "2017-11-07T12:59:58.305487: step 30400, loss 0.613254, rating_acc 0.773275->0.763317, rate 0.0015\n",
      "2017-11-07T12:59:59.964702: step 30500, loss 0.20861, rating_acc 0.428888->0.42168, rate 0.0015\n",
      "2017-11-07T13:00:01.830179: step 30600, loss 0.26475, rating_acc 0.498592->0.483366, rate 0.0015\n",
      "2017-11-07T13:00:03.492247: step 30700, loss 0.599842, rating_acc 0.765511->0.75405, rate 0.0015\n",
      "2017-11-07T13:00:05.195862: step 30800, loss 0.645443, rating_acc 0.794944->0.783613, rate 0.0015\n",
      "2017-11-07T13:00:06.865222: step 30900, loss 0.358576, rating_acc 0.580452->0.571873, rate 0.0015\n",
      "2017-11-07T13:00:08.184814: step 31000, loss 0.571426, rating_acc 0.750488->0.734541, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:00:08.187436: step 31000, loss 0.805002, rating_acc 0.879276\n",
      "\n",
      "2017-11-07T13:00:09.306051: step 31100, loss 1.20929, rating_acc 1.1023->1.08499, rate 0.0015\n",
      "2017-11-07T13:00:10.724185: step 31200, loss 0.313736, rating_acc 0.531136->0.530528, rate 0.0015\n",
      "2017-11-07T13:00:12.399215: step 31300, loss 0.919969, rating_acc 0.956386->0.942127, rate 0.0015\n",
      "2017-11-07T13:00:14.074932: step 31400, loss 0.487971, rating_acc 0.687188->0.674474, rate 0.0015\n",
      "2017-11-07T13:00:15.750795: step 31500, loss 0.560537, rating_acc 0.740442->0.726064, rate 0.0015\n",
      "2017-11-07T13:00:17.438290: step 31600, loss 0.718651, rating_acc 0.844461->0.827687, rate 0.0015\n",
      "2017-11-07T13:00:19.121652: step 31700, loss 0.611093, rating_acc 0.771576->0.759918, rate 0.0015\n",
      "2017-11-07T13:00:20.783548: step 31800, loss 0.598684, rating_acc 0.764108->0.751636, rate 0.0015\n",
      "2017-11-07T13:00:22.465120: step 31900, loss 0.813574, rating_acc 0.89903->0.883068, rate 0.0015\n",
      "2017-11-07T13:00:24.111937: step 32000, loss 0.550882, rating_acc 0.72817->0.719085, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:00:24.117010: step 32000, loss 0.797542, rating_acc 0.873925\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-32000\n",
      "\n",
      "2017-11-07T13:00:25.915543: step 32100, loss 0.440974, rating_acc 0.649966->0.638072, rate 0.0015\n",
      "2017-11-07T13:00:27.346263: step 32200, loss 0.316172, rating_acc 0.548598->0.531264, rate 0.0015\n",
      "2017-11-07T13:00:28.462230: step 32300, loss 0.456653, rating_acc 0.659454->0.650075, rate 0.0015\n",
      "2017-11-07T13:00:29.849487: step 32400, loss 0.626014, rating_acc 0.781958->0.769322, rate 0.0015\n",
      "2017-11-07T13:00:31.513545: step 32500, loss 0.282893, rating_acc 0.509165->0.498516, rate 0.0015\n",
      "2017-11-07T13:00:33.175871: step 32600, loss 0.813988, rating_acc 0.897799->0.882945, rate 0.0015\n",
      "2017-11-07T13:00:34.834471: step 32700, loss 0.588809, rating_acc 0.758691->0.744536, rate 0.0015\n",
      "2017-11-07T13:00:36.478871: step 32800, loss 0.529406, rating_acc 0.707589->0.703421, rate 0.0015\n",
      "2017-11-07T13:00:38.152177: step 32900, loss 0.65661, rating_acc 0.805742->0.788622, rate 0.0015\n",
      "2017-11-07T13:00:39.795719: step 33000, loss 0.332123, rating_acc 0.551275->0.54532, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:00:39.799860: step 33000, loss 0.797602, rating_acc 0.873415\n",
      "\n",
      "2017-11-07T13:00:41.477700: step 33100, loss 0.300878, rating_acc 0.523304->0.51584, rate 0.0015\n",
      "2017-11-07T13:00:43.137292: step 33200, loss 1.02201, rating_acc 1.01087->0.993498, rate 0.0015\n",
      "2017-11-07T13:00:44.804358: step 33300, loss 0.808061, rating_acc 0.89573->0.879182, rate 0.0015\n",
      "2017-11-07T13:00:46.368090: step 33400, loss 0.728988, rating_acc 0.845711->0.832929, rate 0.0015\n",
      "2017-11-07T13:00:47.480352: step 33500, loss 0.236596, rating_acc 0.456322->0.448623, rate 0.0015\n",
      "2017-11-07T13:00:48.664331: step 33600, loss 0.356076, rating_acc 0.582396->0.566291, rate 0.0015\n",
      "2017-11-07T13:00:50.337785: step 33700, loss 0.370751, rating_acc 0.587856->0.579014, rate 0.0015\n",
      "2017-11-07T13:00:52.023138: step 33800, loss 0.448333, rating_acc 0.650941->0.642483, rate 0.0015\n",
      "2017-11-07T13:00:53.678811: step 33900, loss 0.575041, rating_acc 0.757723->0.734428, rate 0.0015\n",
      "2017-11-07T13:00:55.363165: step 34000, loss 0.485976, rating_acc 0.681619->0.670924, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:00:55.368385: step 34000, loss 0.801296, rating_acc 0.874905\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-34000\n",
      "\n",
      "2017-11-07T13:00:57.164531: step 34100, loss 0.972736, rating_acc 0.979122->0.967914, rate 0.0015\n",
      "2017-11-07T13:00:58.834226: step 34200, loss 0.809812, rating_acc 0.893639->0.879702, rate 0.0015\n",
      "2017-11-07T13:01:00.499776: step 34300, loss 0.823979, rating_acc 0.912652->0.887611, rate 0.0015\n",
      "2017-11-07T13:01:02.184030: step 34400, loss 0.527406, rating_acc 0.721252->0.700692, rate 0.0015\n",
      "2017-11-07T13:01:03.876180: step 34500, loss 0.55746, rating_acc 0.731643->0.721833, rate 0.0015\n",
      "2017-11-07T13:01:05.540934: step 34600, loss 0.443755, rating_acc 0.645979->0.638152, rate 0.0015\n",
      "2017-11-07T13:01:06.647904: step 34700, loss 0.539588, rating_acc 0.719582->0.709088, rate 0.0015\n",
      "2017-11-07T13:01:07.762822: step 34800, loss 0.54839, rating_acc 0.725776->0.715125, rate 0.0015\n",
      "2017-11-07T13:01:09.421220: step 34900, loss 0.602945, rating_acc 0.762588->0.752154, rate 0.0015\n",
      "2017-11-07T13:01:11.108987: step 35000, loss 0.435534, rating_acc 0.641038->0.630893, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:01:11.112738: step 35000, loss 0.801836, rating_acc 0.874258\n",
      "\n",
      "2017-11-07T13:01:12.784421: step 35100, loss 1.1067, rating_acc 1.04339->1.03395, rate 0.0015\n",
      "2017-11-07T13:01:14.453724: step 35200, loss 0.296045, rating_acc 0.517884->0.508277, rate 0.0015\n",
      "2017-11-07T13:01:15.783818: step 35300, loss 0.911815, rating_acc 0.951564->0.934945, rate 0.0015\n",
      "2017-11-07T13:01:16.901451: step 35400, loss 0.429942, rating_acc 0.635929->0.626289, rate 0.0015\n",
      "2017-11-07T13:01:18.094495: step 35500, loss 0.548354, rating_acc 0.72552->0.714586, rate 0.0015\n",
      "2017-11-07T13:01:19.761671: step 35600, loss 0.246088, rating_acc 0.465584->0.456318, rate 0.0015\n",
      "2017-11-07T13:01:21.424270: step 35700, loss 0.423552, rating_acc 0.63278->0.620992, rate 0.0015\n",
      "2017-11-07T13:01:23.087827: step 35800, loss 0.413538, rating_acc 0.62036->0.612858, rate 0.0015\n",
      "2017-11-07T13:01:24.750448: step 35900, loss 0.432173, rating_acc 0.634732->0.627912, rate 0.0015\n",
      "2017-11-07T13:01:26.435423: step 36000, loss 0.709251, rating_acc 0.827983->0.819363, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:01:26.439427: step 36000, loss 0.796421, rating_acc 0.870934\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-36000\n",
      "\n",
      "2017-11-07T13:01:27.986536: step 36100, loss 0.638033, rating_acc 0.786796->0.774661, rate 0.0015\n",
      "2017-11-07T13:01:29.106164: step 36200, loss 0.615595, rating_acc 0.768982->0.760005, rate 0.0015\n",
      "2017-11-07T13:01:30.378565: step 36300, loss 1.60154, rating_acc 1.28105->1.25043, rate 0.0015\n",
      "2017-11-07T13:01:32.030924: step 36400, loss 0.496762, rating_acc 0.691929->0.677285, rate 0.0015\n",
      "2017-11-07T13:01:33.674828: step 36500, loss 0.476798, rating_acc 0.677687->0.662396, rate 0.0015\n",
      "2017-11-07T13:01:35.332589: step 36600, loss 0.649497, rating_acc 0.793577->0.781915, rate 0.0015\n",
      "2017-11-07T13:01:36.957557: step 36700, loss 0.431306, rating_acc 0.631252->0.626955, rate 0.0015\n",
      "2017-11-07T13:01:38.658403: step 36800, loss 0.807351, rating_acc 0.8866->0.876972, rate 0.0015\n",
      "2017-11-07T13:01:40.335450: step 36900, loss 0.352837, rating_acc 0.570354->0.560789, rate 0.0015\n",
      "2017-11-07T13:01:41.986166: step 37000, loss 0.748334, rating_acc 0.850713->0.842539, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:01:41.990956: step 37000, loss 0.796572, rating_acc 0.870695\n",
      "\n",
      "2017-11-07T13:01:43.666547: step 37100, loss 0.908706, rating_acc 0.947385->0.932842, rate 0.0015\n",
      "2017-11-07T13:01:45.346552: step 37200, loss 0.572255, rating_acc 0.744657->0.730465, rate 0.0015\n",
      "2017-11-07T13:01:47.040377: step 37300, loss 0.293082, rating_acc 0.522608->0.504255, rate 0.0015\n",
      "2017-11-07T13:01:48.148516: step 37400, loss 1.01249, rating_acc 1.00197->0.986712, rate 0.0015\n",
      "2017-11-07T13:01:49.262302: step 37500, loss 0.660774, rating_acc 0.801414->0.788569, rate 0.0015\n",
      "2017-11-07T13:01:50.930025: step 37600, loss 0.506104, rating_acc 0.697939->0.683421, rate 0.0015\n",
      "2017-11-07T13:01:52.606461: step 37700, loss 0.751674, rating_acc 0.856865->0.844084, rate 0.0015\n",
      "2017-11-07T13:01:54.255951: step 37800, loss 0.585924, rating_acc 0.751015->0.739338, rate 0.0015\n",
      "2017-11-07T13:01:55.922609: step 37900, loss 0.374536, rating_acc 0.590275->0.578838, rate 0.0015\n",
      "2017-11-07T13:01:57.586142: step 38000, loss 0.703838, rating_acc 0.836112->0.815042, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:01:57.590694: step 38000, loss 0.796248, rating_acc 0.869887\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-38000\n",
      "\n",
      "2017-11-07T13:01:59.374530: step 38100, loss 0.407182, rating_acc 0.624319->0.606214, rate 0.0015\n",
      "2017-11-07T13:02:01.247422: step 38200, loss 0.69337, rating_acc 0.819707->0.80825, rate 0.0015\n",
      "2017-11-07T13:02:02.922181: step 38300, loss 0.404643, rating_acc 0.615493->0.603634, rate 0.0015\n",
      "2017-11-07T13:02:04.610068: step 38400, loss 0.65507, rating_acc 0.792755->0.783992, rate 0.0015\n",
      "2017-11-07T13:02:06.233229: step 38500, loss 0.695814, rating_acc 0.822941->0.809478, rate 0.0015\n",
      "2017-11-07T13:02:07.336191: step 38600, loss 0.4051, rating_acc 0.624173->0.603672, rate 0.0015\n",
      "2017-11-07T13:02:08.440233: step 38700, loss 0.989506, rating_acc 0.982979->0.973958, rate 0.0015\n",
      "2017-11-07T13:02:10.081486: step 38800, loss 0.567963, rating_acc 0.740037->0.725929, rate 0.0015\n",
      "2017-11-07T13:02:11.743521: step 38900, loss 0.768107, rating_acc 0.86491->0.852733, rate 0.0015\n",
      "2017-11-07T13:02:13.415032: step 39000, loss 0.94401, rating_acc 0.961686->0.950075, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:02:13.418856: step 39000, loss 0.799306, rating_acc 0.870597\n",
      "\n",
      "2017-11-07T13:02:15.083827: step 39100, loss 0.383798, rating_acc 0.594117->0.584934, rate 0.0015\n",
      "2017-11-07T13:02:16.754874: step 39200, loss 0.81196, rating_acc 0.890669->0.87751, rate 0.0015\n",
      "2017-11-07T13:02:18.429886: step 39300, loss 0.838381, rating_acc 0.907036->0.892408, rate 0.0015\n",
      "2017-11-07T13:02:20.076662: step 39400, loss 0.813632, rating_acc 0.89315->0.878428, rate 0.0015\n",
      "2017-11-07T13:02:21.761331: step 39500, loss 0.298172, rating_acc 0.51716->0.506046, rate 0.0015\n",
      "2017-11-07T13:02:23.450422: step 39600, loss 0.339644, rating_acc 0.554718->0.545499, rate 0.0015\n",
      "2017-11-07T13:02:25.111589: step 39700, loss 0.49546, rating_acc 0.685029->0.673356, rate 0.0015\n",
      "2017-11-07T13:02:26.357085: step 39800, loss 0.347592, rating_acc 0.567494->0.552687, rate 0.0015\n",
      "2017-11-07T13:02:27.466403: step 39900, loss 0.206521, rating_acc 0.415432->0.405285, rate 0.0015\n",
      "2017-11-07T13:02:28.963881: step 40000, loss 0.412792, rating_acc 0.610283->0.608626, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:02:28.968125: step 40000, loss 0.800942, rating_acc 0.870962\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-40000\n",
      "\n",
      "2017-11-07T13:02:30.751214: step 40100, loss 0.268326, rating_acc 0.482811->0.475302, rate 0.00075\n",
      "2017-11-07T13:02:32.404354: step 40200, loss 0.87155, rating_acc 0.916914->0.910552, rate 0.00075\n",
      "2017-11-07T13:02:34.087316: step 40300, loss 0.487349, rating_acc 0.673364->0.667064, rate 0.00075\n",
      "2017-11-07T13:02:35.746967: step 40400, loss 0.516324, rating_acc 0.69509->0.688438, rate 0.00075\n",
      "2017-11-07T13:02:37.418233: step 40500, loss 0.578028, rating_acc 0.738513->0.73191, rate 0.00075\n",
      "2017-11-07T13:02:39.105502: step 40600, loss 0.65211, rating_acc 0.787801->0.780916, rate 0.00075\n",
      "2017-11-07T13:02:40.754565: step 40700, loss 0.675866, rating_acc 0.811247->0.796008, rate 0.00075\n",
      "2017-11-07T13:02:42.411752: step 40800, loss 0.818478, rating_acc 0.890131->0.881033, rate 0.00075\n",
      "2017-11-07T13:02:44.068380: step 40900, loss 0.31505, rating_acc 0.529667->0.522302, rate 0.00075\n",
      "2017-11-07T13:02:45.416303: step 41000, loss 0.302167, rating_acc 0.512337->0.509782, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:02:45.418821: step 41000, loss 0.798101, rating_acc 0.869374\n",
      "\n",
      "2017-11-07T13:02:46.520507: step 41100, loss 0.661155, rating_acc 0.793288->0.786692, rate 0.00075\n",
      "2017-11-07T13:02:47.890968: step 41200, loss 0.451034, rating_acc 0.649918->0.639394, rate 0.00075\n",
      "2017-11-07T13:02:49.573917: step 41300, loss 0.628389, rating_acc 0.773571->0.765603, rate 0.00075\n",
      "2017-11-07T13:02:51.229673: step 41400, loss 0.487513, rating_acc 0.674722->0.667268, rate 0.00075\n",
      "2017-11-07T13:02:52.876595: step 41500, loss 0.363698, rating_acc 0.572618->0.566983, rate 0.00075\n",
      "2017-11-07T13:02:54.568135: step 41600, loss 0.686329, rating_acc 0.807406->0.802528, rate 0.00075\n",
      "2017-11-07T13:02:56.235083: step 41700, loss 0.216642, rating_acc 0.428326->0.417566, rate 0.00075\n",
      "2017-11-07T13:02:57.903340: step 41800, loss 0.498999, rating_acc 0.682535->0.675866, rate 0.00075\n",
      "2017-11-07T13:02:59.555282: step 41900, loss 0.431629, rating_acc 0.629586->0.624025, rate 0.00075\n",
      "2017-11-07T13:03:01.259018: step 42000, loss 0.547916, rating_acc 0.72256->0.711071, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:03:01.266203: step 42000, loss 0.800829, rating_acc 0.870939\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-42000\n",
      "\n",
      "2017-11-07T13:03:03.013175: step 42100, loss 0.342533, rating_acc 0.555248->0.547948, rate 0.00075\n",
      "2017-11-07T13:03:04.497293: step 42200, loss 0.272772, rating_acc 0.487877->0.480106, rate 0.00075\n",
      "2017-11-07T13:03:05.600406: step 42300, loss 0.671189, rating_acc 0.800573->0.79302, rate 0.00075\n",
      "2017-11-07T13:03:06.883065: step 42400, loss 1.88543, rating_acc 1.36608->1.35759, rate 0.00075\n",
      "2017-11-07T13:03:08.558887: step 42500, loss 0.619522, rating_acc 0.767246->0.75965, rate 0.00075\n",
      "2017-11-07T13:03:10.197920: step 42600, loss 0.316998, rating_acc 0.533224->0.523868, rate 0.00075\n",
      "2017-11-07T13:03:11.859058: step 42700, loss 0.608022, rating_acc 0.757892->0.751937, rate 0.00075\n",
      "2017-11-07T13:03:13.508359: step 42800, loss 1.00538, rating_acc 0.988553->0.981192, rate 0.00075\n",
      "2017-11-07T13:03:15.153123: step 42900, loss 0.506184, rating_acc 0.6857->0.680857, rate 0.00075\n",
      "2017-11-07T13:03:16.824273: step 43000, loss 1.02417, rating_acc 0.994615->0.99071, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:03:16.828315: step 43000, loss 0.798319, rating_acc 0.869282\n",
      "\n",
      "2017-11-07T13:03:18.499489: step 43100, loss 0.519781, rating_acc 0.696034->0.690698, rate 0.00075\n",
      "2017-11-07T13:03:20.148803: step 43200, loss 0.173075, rating_acc 0.367679->0.360885, rate 0.00075\n",
      "2017-11-07T13:03:21.814659: step 43300, loss 1.19444, rating_acc 1.07795->1.07312, rate 0.00075\n",
      "2017-11-07T13:03:23.456478: step 43400, loss 1.12152, rating_acc 1.04427->1.03858, rate 0.00075\n",
      "2017-11-07T13:03:24.567707: step 43500, loss 0.338526, rating_acc 0.546607->0.543755, rate 0.00075\n",
      "2017-11-07T13:03:25.684730: step 43600, loss 0.359276, rating_acc 0.567373->0.56253, rate 0.00075\n",
      "2017-11-07T13:03:27.325129: step 43700, loss 0.45308, rating_acc 0.646203->0.640534, rate 0.00075\n",
      "2017-11-07T13:03:28.998373: step 43800, loss 0.729786, rating_acc 0.835847->0.828845, rate 0.00075\n",
      "2017-11-07T13:03:30.659760: step 43900, loss 0.508888, rating_acc 0.686146->0.68272, rate 0.00075\n",
      "2017-11-07T13:03:32.321370: step 44000, loss 0.430565, rating_acc 0.627653->0.622673, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:03:32.325428: step 44000, loss 0.795585, rating_acc 0.867607\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-44000\n",
      "\n",
      "2017-11-07T13:03:34.110673: step 44100, loss 0.412297, rating_acc 0.611474->0.60786, rate 0.00075\n",
      "2017-11-07T13:03:35.798586: step 44200, loss 0.87286, rating_acc 0.918467->0.91109, rate 0.00075\n",
      "2017-11-07T13:03:37.454509: step 44300, loss 0.623573, rating_acc 0.767399->0.762082, rate 0.00075\n",
      "2017-11-07T13:03:39.112332: step 44400, loss 0.568964, rating_acc 0.730294->0.725367, rate 0.00075\n",
      "2017-11-07T13:03:40.796713: step 44500, loss 0.360718, rating_acc 0.572088->0.563877, rate 0.00075\n",
      "2017-11-07T13:03:42.458440: step 44600, loss 0.750336, rating_acc 0.848012->0.841175, rate 0.00075\n",
      "2017-11-07T13:03:43.664349: step 44700, loss 0.704693, rating_acc 0.821396->0.81358, rate 0.00075\n",
      "2017-11-07T13:03:44.786313: step 44800, loss 0.490973, rating_acc 0.675326->0.66946, rate 0.00075\n",
      "2017-11-07T13:03:46.268245: step 44900, loss 0.438827, rating_acc 0.638789->0.629303, rate 0.00075\n",
      "2017-11-07T13:03:47.384141: step 45000, loss 0.360018, rating_acc 0.569162->0.563208, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:03:47.386824: step 45000, loss 0.793897, rating_acc 0.86665\n",
      "\n",
      "2017-11-07T13:03:48.492398: step 45100, loss 0.749777, rating_acc 0.846179->0.840799, rate 0.00075\n",
      "2017-11-07T13:03:49.940414: step 45200, loss 0.436755, rating_acc 0.632791->0.627668, rate 0.00075\n",
      "2017-11-07T13:03:51.623084: step 45300, loss 0.479767, rating_acc 0.668227->0.661032, rate 0.00075\n",
      "2017-11-07T13:03:53.286738: step 45400, loss 0.625068, rating_acc 0.779947->0.763042, rate 0.00075\n",
      "2017-11-07T13:03:54.984870: step 45500, loss 0.27785, rating_acc 0.48984->0.484726, rate 0.00075\n",
      "2017-11-07T13:03:56.646556: step 45600, loss 0.387758, rating_acc 0.592753->0.58728, rate 0.00075\n",
      "2017-11-07T13:03:58.322318: step 45700, loss 1.12117, rating_acc 1.04644->1.03841, rate 0.00075\n",
      "2017-11-07T13:04:00.193089: step 45800, loss 0.617603, rating_acc 0.765304->0.757995, rate 0.00075\n",
      "2017-11-07T13:04:01.857110: step 45900, loss 0.707269, rating_acc 0.83128->0.814967, rate 0.00075\n",
      "2017-11-07T13:04:03.554557: step 46000, loss 0.240081, rating_acc 0.452657->0.443756, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:04:03.559116: step 46000, loss 0.791715, rating_acc 0.865189\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-46000\n",
      "\n",
      "2017-11-07T13:04:05.121176: step 46100, loss 0.826761, rating_acc 0.895975->0.885199, rate 0.00075\n",
      "2017-11-07T13:04:06.230206: step 46200, loss 0.410738, rating_acc 0.615994->0.606179, rate 0.00075\n",
      "2017-11-07T13:04:07.502219: step 46300, loss 0.37714, rating_acc 0.584595->0.57768, rate 0.00075\n",
      "2017-11-07T13:04:09.184716: step 46400, loss 0.579581, rating_acc 0.741615->0.732173, rate 0.00075\n",
      "2017-11-07T13:04:10.837708: step 46500, loss 0.241075, rating_acc 0.459579->0.444435, rate 0.00075\n",
      "2017-11-07T13:04:12.477493: step 46600, loss 0.570988, rating_acc 0.735049->0.726193, rate 0.00075\n",
      "2017-11-07T13:04:14.147362: step 46700, loss 0.554759, rating_acc 0.720979->0.71475, rate 0.00075\n",
      "2017-11-07T13:04:15.830126: step 46800, loss 0.386914, rating_acc 0.593412->0.585571, rate 0.00075\n",
      "2017-11-07T13:04:17.485362: step 46900, loss 0.362573, rating_acc 0.571351->0.564313, rate 0.00075\n",
      "2017-11-07T13:04:19.178870: step 47000, loss 1.2015, rating_acc 1.08251->1.07581, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:04:19.183432: step 47000, loss 0.787088, rating_acc 0.861948\n",
      "\n",
      "2017-11-07T13:04:20.856702: step 47100, loss 0.422395, rating_acc 0.626524->0.614976, rate 0.00075\n",
      "2017-11-07T13:04:22.525068: step 47200, loss 0.479268, rating_acc 0.669963->0.659626, rate 0.00075\n",
      "2017-11-07T13:04:24.136802: step 47300, loss 0.392892, rating_acc 0.595981->0.590541, rate 0.00075\n",
      "2017-11-07T13:04:25.234117: step 47400, loss 0.533224, rating_acc 0.706259->0.699345, rate 0.00075\n",
      "2017-11-07T13:04:26.415013: step 47500, loss 0.317234, rating_acc 0.531513->0.522504, rate 0.00075\n",
      "2017-11-07T13:04:28.057501: step 47600, loss 0.260868, rating_acc 0.470181->0.465437, rate 0.00075\n",
      "2017-11-07T13:04:29.731162: step 47700, loss 0.362016, rating_acc 0.570502->0.563629, rate 0.00075\n",
      "2017-11-07T13:04:31.417643: step 47800, loss 0.32162, rating_acc 0.536629->0.526461, rate 0.00075\n",
      "2017-11-07T13:04:33.112248: step 47900, loss 0.536312, rating_acc 0.711721->0.701321, rate 0.00075\n",
      "2017-11-07T13:04:34.735413: step 48000, loss 0.522119, rating_acc 0.699562->0.691071, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:04:34.740091: step 48000, loss 0.785378, rating_acc 0.860719\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-48000\n",
      "\n",
      "2017-11-07T13:04:36.510382: step 48100, loss 0.772162, rating_acc 0.859689->0.852977, rate 0.00075\n",
      "2017-11-07T13:04:38.187361: step 48200, loss 0.72332, rating_acc 0.831076->0.82381, rate 0.00075\n",
      "2017-11-07T13:04:39.853005: step 48300, loss 0.221582, rating_acc 0.424711->0.420559, rate 0.00075\n",
      "2017-11-07T13:04:41.520232: step 48400, loss 0.607905, rating_acc 0.759132->0.750433, rate 0.00075\n",
      "2017-11-07T13:04:43.190988: step 48500, loss 0.622852, rating_acc 0.766472->0.760262, rate 0.00075\n",
      "2017-11-07T13:04:44.331977: step 48600, loss 0.866284, rating_acc 0.914328->0.906275, rate 0.00075\n",
      "2017-11-07T13:04:45.452338: step 48700, loss 0.548151, rating_acc 0.717357->0.709322, rate 0.00075\n",
      "2017-11-07T13:04:47.127908: step 48800, loss 0.499263, rating_acc 0.680707->0.673957, rate 0.00075\n",
      "2017-11-07T13:04:48.803277: step 48900, loss 0.647213, rating_acc 0.787241->0.775942, rate 0.00075\n",
      "2017-11-07T13:04:50.488212: step 49000, loss 0.589675, rating_acc 0.743527->0.737875, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:04:50.491865: step 49000, loss 0.785806, rating_acc 0.860575\n",
      "\n",
      "2017-11-07T13:04:52.183802: step 49100, loss 0.394717, rating_acc 0.597856->0.591129, rate 0.00075\n",
      "2017-11-07T13:04:53.869223: step 49200, loss 0.348248, rating_acc 0.555167->0.550353, rate 0.00075\n",
      "2017-11-07T13:04:55.553742: step 49300, loss 0.758222, rating_acc 0.849447->0.844261, rate 0.00075\n",
      "2017-11-07T13:04:57.227079: step 49400, loss 0.411769, rating_acc 0.611678->0.605224, rate 0.00075\n",
      "2017-11-07T13:04:58.912046: step 49500, loss 0.500614, rating_acc 0.682075->0.674597, rate 0.00075\n",
      "2017-11-07T13:05:00.590332: step 49600, loss 0.660176, rating_acc 0.788463->0.783894, rate 0.00075\n",
      "2017-11-07T13:05:02.255930: step 49700, loss 0.431862, rating_acc 0.626043->0.62136, rate 0.00075\n",
      "2017-11-07T13:05:03.488072: step 49800, loss 0.35012, rating_acc 0.560223->0.551656, rate 0.00075\n",
      "2017-11-07T13:05:04.600964: step 49900, loss 0.437684, rating_acc 0.635961->0.625934, rate 0.00075\n",
      "2017-11-07T13:05:06.151589: step 50000, loss 0.257312, rating_acc 0.462167->0.459632, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:05:06.156253: step 50000, loss 0.787652, rating_acc 0.861163\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-50000\n",
      "\n",
      "2017-11-07T13:05:08.055212: step 50100, loss 0.656341, rating_acc 0.790746->0.781147, rate 0.00075\n",
      "2017-11-07T13:05:09.744378: step 50200, loss 0.422462, rating_acc 0.621637->0.613306, rate 0.00075\n",
      "2017-11-07T13:05:11.415984: step 50300, loss 0.395524, rating_acc 0.596312->0.590873, rate 0.00075\n",
      "2017-11-07T13:05:13.103205: step 50400, loss 0.426365, rating_acc 0.622274->0.61634, rate 0.00075\n",
      "2017-11-07T13:05:14.788909: step 50500, loss 0.353662, rating_acc 0.557518->0.55418, rate 0.00075\n",
      "2017-11-07T13:05:16.452595: step 50600, loss 0.729361, rating_acc 0.833676->0.826303, rate 0.00075\n",
      "2017-11-07T13:05:18.116410: step 50700, loss 0.53432, rating_acc 0.703371->0.69833, rate 0.00075\n",
      "2017-11-07T13:05:19.775519: step 50800, loss 0.97721, rating_acc 0.979023->0.96464, rate 0.00075\n",
      "2017-11-07T13:05:21.443754: step 50900, loss 0.64316, rating_acc 0.779933->0.772254, rate 0.00075\n",
      "2017-11-07T13:05:22.740575: step 51000, loss 0.331579, rating_acc 0.540384->0.533587, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:05:22.743010: step 51000, loss 0.789155, rating_acc 0.861563\n",
      "\n",
      "2017-11-07T13:05:23.856120: step 51100, loss 0.990621, rating_acc 0.97971->0.971464, rate 0.00075\n",
      "2017-11-07T13:05:25.303718: step 51200, loss 0.476216, rating_acc 0.659957->0.655209, rate 0.00075\n",
      "2017-11-07T13:05:27.001783: step 51300, loss 0.404978, rating_acc 0.608041->0.598351, rate 0.00075\n",
      "2017-11-07T13:05:28.696095: step 51400, loss 0.704318, rating_acc 0.818016->0.81075, rate 0.00075\n",
      "2017-11-07T13:05:30.354775: step 51500, loss 0.733455, rating_acc 0.838449->0.828511, rate 0.00075\n",
      "2017-11-07T13:05:32.017907: step 51600, loss 0.408397, rating_acc 0.605879->0.601076, rate 0.00075\n",
      "2017-11-07T13:05:33.679004: step 51700, loss 0.407026, rating_acc 0.609723->0.599913, rate 0.00075\n",
      "2017-11-07T13:05:35.339194: step 51800, loss 0.806734, rating_acc 0.877857->0.871537, rate 0.00075\n",
      "2017-11-07T13:05:37.022177: step 51900, loss 0.396795, rating_acc 0.595141->0.591236, rate 0.00075\n",
      "2017-11-07T13:05:38.715223: step 52000, loss 0.754174, rating_acc 0.844775->0.840755, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:05:38.720270: step 52000, loss 0.789726, rating_acc 0.861638\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-52000\n",
      "\n",
      "2017-11-07T13:05:40.490596: step 52100, loss 1.0236, rating_acc 0.995793->0.988058, rate 0.00075\n",
      "2017-11-07T13:05:41.902925: step 52200, loss 0.473069, rating_acc 0.657974->0.65243, rate 0.00075\n",
      "2017-11-07T13:05:43.012996: step 52300, loss 0.935554, rating_acc 0.948741->0.942379, rate 0.00075\n",
      "2017-11-07T13:05:44.376666: step 52400, loss 0.361656, rating_acc 0.565436->0.560483, rate 0.00075\n",
      "2017-11-07T13:05:46.066192: step 52500, loss 0.373661, rating_acc 0.579728->0.571056, rate 0.00075\n",
      "2017-11-07T13:05:47.776657: step 52600, loss 0.557611, rating_acc 0.720527->0.71411, rate 0.00075\n",
      "2017-11-07T13:05:49.473748: step 52700, loss 0.316094, rating_acc 0.522739->0.51804, rate 0.00075\n",
      "2017-11-07T13:05:51.146074: step 52800, loss 0.54242, rating_acc 0.716741->0.70334, rate 0.00075\n",
      "2017-11-07T13:05:52.815010: step 52900, loss 0.440745, rating_acc 0.633635->0.626828, rate 0.00075\n",
      "2017-11-07T13:05:54.470368: step 53000, loss 0.501786, rating_acc 0.680106->0.67371, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:05:54.474585: step 53000, loss 0.790466, rating_acc 0.861722\n",
      "\n",
      "2017-11-07T13:05:56.142851: step 53100, loss 0.533134, rating_acc 0.705682->0.696545, rate 0.00075\n",
      "2017-11-07T13:05:57.805965: step 53200, loss 0.391977, rating_acc 0.589829->0.586443, rate 0.00075\n",
      "2017-11-07T13:05:59.468903: step 53300, loss 0.4476, rating_acc 0.6405->0.632053, rate 0.00075\n",
      "2017-11-07T13:06:01.063935: step 53400, loss 0.210964, rating_acc 0.405462->0.403399, rate 0.00075\n",
      "2017-11-07T13:06:02.320100: step 53500, loss 0.555767, rating_acc 0.718019->0.712308, rate 0.00075\n",
      "2017-11-07T13:06:03.550595: step 53600, loss 0.472688, rating_acc 0.657332->0.65131, rate 0.00075\n",
      "2017-11-07T13:06:05.222056: step 53700, loss 0.35949, rating_acc 0.56537->0.557593, rate 0.00075\n",
      "2017-11-07T13:06:06.918199: step 53800, loss 0.31127, rating_acc 0.521055->0.512453, rate 0.00075\n",
      "2017-11-07T13:06:08.578952: step 53900, loss 0.593153, rating_acc 0.744693->0.737801, rate 0.00075\n",
      "2017-11-07T13:06:10.242383: step 54000, loss 0.292408, rating_acc 0.497107->0.49346, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:06:10.246021: step 54000, loss 0.79068, rating_acc 0.861264\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-54000\n",
      "\n",
      "2017-11-07T13:06:12.008154: step 54100, loss 0.288326, rating_acc 0.494287->0.489216, rate 0.00075\n",
      "2017-11-07T13:06:13.685094: step 54200, loss 0.413494, rating_acc 0.610728->0.603697, rate 0.00075\n",
      "2017-11-07T13:06:15.279755: step 54300, loss 0.558706, rating_acc 0.723154->0.713745, rate 0.00075\n",
      "2017-11-07T13:06:16.387697: step 54400, loss 0.699586, rating_acc 0.811598->0.806326, rate 0.00075\n",
      "2017-11-07T13:06:17.503067: step 54500, loss 0.443249, rating_acc 0.63229->0.627466, rate 0.00075\n",
      "2017-11-07T13:06:18.940331: step 54600, loss 0.272618, rating_acc 0.479184->0.47229, rate 0.00075\n",
      "2017-11-07T13:06:20.633067: step 54700, loss 0.687566, rating_acc 0.806081->0.798689, rate 0.00075\n",
      "2017-11-07T13:06:22.314342: step 54800, loss 1.05673, rating_acc 1.011->1.00352, rate 0.00075\n",
      "2017-11-07T13:06:23.715346: step 54900, loss 0.538531, rating_acc 0.709027->0.699146, rate 0.00075\n",
      "2017-11-07T13:06:24.834214: step 55000, loss 0.63263, rating_acc 0.770571->0.763451, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:06:24.837068: step 55000, loss 0.791919, rating_acc 0.861479\n",
      "\n",
      "2017-11-07T13:06:26.268121: step 55100, loss 0.437906, rating_acc 0.628371->0.622948, rate 0.00075\n",
      "2017-11-07T13:06:27.944176: step 55200, loss 0.284835, rating_acc 0.491602->0.484671, rate 0.00075\n",
      "2017-11-07T13:06:29.606415: step 55300, loss 0.27773, rating_acc 0.4853->0.477229, rate 0.00075\n",
      "2017-11-07T13:06:31.284701: step 55400, loss 0.442679, rating_acc 0.631067->0.626571, rate 0.00075\n",
      "2017-11-07T13:06:32.967535: step 55500, loss 0.54723, rating_acc 0.711701->0.705083, rate 0.00075\n",
      "2017-11-07T13:06:34.651289: step 55600, loss 0.280899, rating_acc 0.484782->0.4804, rate 0.00075\n",
      "2017-11-07T13:06:36.322496: step 55700, loss 0.411102, rating_acc 0.603226->0.60078, rate 0.00075\n",
      "2017-11-07T13:06:38.006357: step 55800, loss 0.583275, rating_acc 0.740246->0.730131, rate 0.00075\n",
      "2017-11-07T13:06:39.682210: step 55900, loss 0.41689, rating_acc 0.611572->0.605544, rate 0.00075\n",
      "2017-11-07T13:06:41.370088: step 56000, loss 0.301994, rating_acc 0.503533->0.501772, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:06:41.374537: step 56000, loss 0.790779, rating_acc 0.860558\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-56000\n",
      "\n",
      "2017-11-07T13:06:42.946349: step 56100, loss 0.462574, rating_acc 0.651484->0.642082, rate 0.00075\n",
      "2017-11-07T13:06:44.056327: step 56200, loss 0.513, rating_acc 0.687982->0.68018, rate 0.00075\n",
      "2017-11-07T13:06:45.364887: step 56300, loss 0.452802, rating_acc 0.640474->0.634345, rate 0.00075\n",
      "2017-11-07T13:06:47.028082: step 56400, loss 0.231231, rating_acc 0.428047->0.425171, rate 0.00075\n",
      "2017-11-07T13:06:48.691019: step 56500, loss 0.374039, rating_acc 0.572206->0.568828, rate 0.00075\n",
      "2017-11-07T13:06:50.348657: step 56600, loss 0.258259, rating_acc 0.457848->0.455776, rate 0.00075\n",
      "2017-11-07T13:06:52.041170: step 56700, loss 0.483059, rating_acc 0.663779->0.657625, rate 0.00075\n",
      "2017-11-07T13:06:53.674800: step 56800, loss 0.37365, rating_acc 0.57529->0.568335, rate 0.00075\n",
      "2017-11-07T13:06:55.350812: step 56900, loss 0.360009, rating_acc 0.561684->0.556146, rate 0.00075\n",
      "2017-11-07T13:06:57.016529: step 57000, loss 0.902214, rating_acc 0.932637->0.922751, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:06:57.020466: step 57000, loss 0.793523, rating_acc 0.861846\n",
      "\n",
      "2017-11-07T13:06:58.680036: step 57100, loss 0.431149, rating_acc 0.622448->0.616734, rate 0.00075\n",
      "2017-11-07T13:07:00.353423: step 57200, loss 0.442995, rating_acc 0.640473->0.626198, rate 0.00075\n",
      "2017-11-07T13:07:01.995090: step 57300, loss 0.276865, rating_acc 0.480022->0.475251, rate 0.00075\n",
      "2017-11-07T13:07:03.102871: step 57400, loss 0.893129, rating_acc 0.925767->0.91767, rate 0.00075\n",
      "2017-11-07T13:07:04.213669: step 57500, loss 0.62575, rating_acc 0.767263->0.758077, rate 0.00075\n",
      "2017-11-07T13:07:05.873144: step 57600, loss 0.485947, rating_acc 0.663538->0.659347, rate 0.00075\n",
      "2017-11-07T13:07:07.521193: step 57700, loss 1.06352, rating_acc 1.01453->1.00608, rate 0.00075\n",
      "2017-11-07T13:07:09.203441: step 57800, loss 0.574559, rating_acc 0.731073->0.723269, rate 0.00075\n",
      "2017-11-07T13:07:10.877734: step 57900, loss 0.171521, rating_acc 0.35294->0.346303, rate 0.00075\n",
      "2017-11-07T13:07:12.542447: step 58000, loss 0.566293, rating_acc 0.723044->0.71737, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:07:12.548242: step 58000, loss 0.794341, rating_acc 0.861781\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-58000\n",
      "\n",
      "2017-11-07T13:07:14.314160: step 58100, loss 0.261719, rating_acc 0.464017->0.458245, rate 0.00075\n",
      "2017-11-07T13:07:16.001688: step 58200, loss 0.569818, rating_acc 0.726618->0.719764, rate 0.00075\n",
      "2017-11-07T13:07:17.665250: step 58300, loss 0.315266, rating_acc 0.520412->0.513265, rate 0.00075\n",
      "2017-11-07T13:07:19.336302: step 58400, loss 0.408891, rating_acc 0.602467->0.597495, rate 0.00075\n",
      "2017-11-07T13:07:21.027828: step 58500, loss 0.278826, rating_acc 0.480476->0.476247, rate 0.00075\n",
      "2017-11-07T13:07:22.234939: step 58600, loss 0.315416, rating_acc 0.517538->0.513157, rate 0.00075\n",
      "2017-11-07T13:07:23.352628: step 58700, loss 0.345841, rating_acc 0.545644->0.541937, rate 0.00075\n",
      "2017-11-07T13:07:24.893162: step 58800, loss 0.428245, rating_acc 0.621672->0.613252, rate 0.00075\n",
      "2017-11-07T13:07:26.581163: step 58900, loss 0.404851, rating_acc 0.602658->0.593855, rate 0.00075\n",
      "2017-11-07T13:07:28.243119: step 59000, loss 0.493146, rating_acc 0.671131->0.66401, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:07:28.247177: step 59000, loss 0.794415, rating_acc 0.861498\n",
      "\n",
      "2017-11-07T13:07:29.907683: step 59100, loss 0.569144, rating_acc 0.724896->0.718928, rate 0.00075\n",
      "2017-11-07T13:07:31.582869: step 59200, loss 0.662683, rating_acc 0.795255->0.781274, rate 0.00075\n",
      "2017-11-07T13:07:33.258673: step 59300, loss 0.446357, rating_acc 0.632904->0.627695, rate 0.00075\n",
      "2017-11-07T13:07:34.951438: step 59400, loss 0.323594, rating_acc 0.52402->0.520795, rate 0.00075\n",
      "2017-11-07T13:07:36.615285: step 59500, loss 0.487377, rating_acc 0.66524->0.659504, rate 0.00075\n",
      "2017-11-07T13:07:38.303809: step 59600, loss 0.462313, rating_acc 0.645552->0.640156, rate 0.00075\n",
      "2017-11-07T13:07:39.978566: step 59700, loss 0.637318, rating_acc 0.773694->0.764713, rate 0.00075\n",
      "2017-11-07T13:07:41.297441: step 59800, loss 0.3859, rating_acc 0.585243->0.577346, rate 0.00075\n",
      "2017-11-07T13:07:42.426836: step 59900, loss 0.508169, rating_acc 0.684379->0.674933, rate 0.00075\n",
      "2017-11-07T13:07:43.909485: step 60000, loss 0.600419, rating_acc 0.745827->0.740101, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:07:43.913899: step 60000, loss 0.796328, rating_acc 0.862357\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-60000\n",
      "\n",
      "2017-11-07T13:07:45.737350: step 60100, loss 0.861798, rating_acc 0.902729->0.899501, rate 0.000375\n",
      "2017-11-07T13:07:47.408352: step 60200, loss 0.232958, rating_acc 0.427562->0.42454, rate 0.000375\n",
      "2017-11-07T13:07:49.104621: step 60300, loss 0.664716, rating_acc 0.786521->0.782289, rate 0.000375\n",
      "2017-11-07T13:07:50.736146: step 60400, loss 0.28285, rating_acc 0.484835->0.479693, rate 0.000375\n",
      "2017-11-07T13:07:52.403438: step 60500, loss 0.681046, rating_acc 0.797256->0.792651, rate 0.000375\n",
      "2017-11-07T13:07:54.060538: step 60600, loss 0.68756, rating_acc 0.800369->0.796732, rate 0.000375\n",
      "2017-11-07T13:07:55.734969: step 60700, loss 0.493129, rating_acc 0.666888->0.663579, rate 0.000375\n",
      "2017-11-07T13:07:57.403565: step 60800, loss 0.461466, rating_acc 0.642957->0.639255, rate 0.000375\n",
      "2017-11-07T13:07:59.068310: step 60900, loss 0.583167, rating_acc 0.731857->0.728244, rate 0.000375\n",
      "2017-11-07T13:08:00.448994: step 61000, loss 0.875934, rating_acc 0.912228->0.907238, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:08:00.451594: step 61000, loss 0.796813, rating_acc 0.862532\n",
      "\n",
      "2017-11-07T13:08:01.697717: step 61100, loss 0.505043, rating_acc 0.677021->0.672389, rate 0.000375\n",
      "2017-11-07T13:08:03.117845: step 61200, loss 0.681723, rating_acc 0.795649->0.792941, rate 0.000375\n",
      "2017-11-07T13:08:04.803947: step 61300, loss 0.700041, rating_acc 0.808555->0.80439, rate 0.000375\n",
      "2017-11-07T13:08:06.466126: step 61400, loss 0.457851, rating_acc 0.638995->0.636258, rate 0.000375\n",
      "2017-11-07T13:08:08.139403: step 61500, loss 0.392692, rating_acc 0.584417->0.582772, rate 0.000375\n",
      "2017-11-07T13:08:09.804400: step 61600, loss 0.527835, rating_acc 0.691139->0.689001, rate 0.000375\n",
      "2017-11-07T13:08:11.456257: step 61700, loss 0.515973, rating_acc 0.683721->0.680315, rate 0.000375\n",
      "2017-11-07T13:08:13.124990: step 61800, loss 0.793435, rating_acc 0.865353->0.860398, rate 0.000375\n",
      "2017-11-07T13:08:14.808551: step 61900, loss 0.534586, rating_acc 0.696978->0.693794, rate 0.000375\n",
      "2017-11-07T13:08:16.475214: step 62000, loss 0.297926, rating_acc 0.496381->0.494613, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:08:16.479296: step 62000, loss 0.79405, rating_acc 0.860678\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-62000\n",
      "\n",
      "2017-11-07T13:08:18.230231: step 62100, loss 0.530841, rating_acc 0.694529->0.691006, rate 0.000375\n",
      "2017-11-07T13:08:19.706847: step 62200, loss 0.644496, rating_acc 0.771162->0.768854, rate 0.000375\n",
      "2017-11-07T13:08:20.826901: step 62300, loss 0.704674, rating_acc 0.811436->0.80703, rate 0.000375\n",
      "2017-11-07T13:08:22.155348: step 62400, loss 0.927136, rating_acc 0.93844->0.934737, rate 0.000375\n",
      "2017-11-07T13:08:23.819268: step 62500, loss 0.276192, rating_acc 0.47545->0.472009, rate 0.000375\n",
      "2017-11-07T13:08:25.489813: step 62600, loss 0.547404, rating_acc 0.705579->0.702852, rate 0.000375\n",
      "2017-11-07T13:08:27.159902: step 62700, loss 0.389452, rating_acc 0.58343->0.579684, rate 0.000375\n",
      "2017-11-07T13:08:28.842278: step 62800, loss 0.468924, rating_acc 0.648502->0.644567, rate 0.000375\n",
      "2017-11-07T13:08:30.507060: step 62900, loss 0.542212, rating_acc 0.700954->0.699097, rate 0.000375\n",
      "2017-11-07T13:08:32.205694: step 63000, loss 0.324578, rating_acc 0.52408->0.52064, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:08:32.210288: step 63000, loss 0.793316, rating_acc 0.860118\n",
      "\n",
      "2017-11-07T13:08:33.884452: step 63100, loss 0.667227, rating_acc 0.7868->0.783373, rate 0.000375\n",
      "2017-11-07T13:08:35.562294: step 63200, loss 0.264922, rating_acc 0.462583->0.459761, rate 0.000375\n",
      "2017-11-07T13:08:37.225723: step 63300, loss 0.58596, rating_acc 0.733187->0.729648, rate 0.000375\n",
      "2017-11-07T13:08:38.765706: step 63400, loss 0.491661, rating_acc 0.664597->0.661875, rate 0.000375\n",
      "2017-11-07T13:08:39.880699: step 63500, loss 0.401517, rating_acc 0.593232->0.589852, rate 0.000375\n",
      "2017-11-07T13:08:41.059782: step 63600, loss 0.449754, rating_acc 0.634903->0.629409, rate 0.000375\n",
      "2017-11-07T13:08:42.719162: step 63700, loss 0.385554, rating_acc 0.579708->0.57613, rate 0.000375\n",
      "2017-11-07T13:08:44.383904: step 63800, loss 0.571296, rating_acc 0.723202->0.719481, rate 0.000375\n",
      "2017-11-07T13:08:46.058149: step 63900, loss 0.311323, rating_acc 0.508391->0.507582, rate 0.000375\n",
      "2017-11-07T13:08:47.321005: step 64000, loss 0.68654, rating_acc 0.800693->0.795508, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:08:47.323686: step 64000, loss 0.793095, rating_acc 0.859877\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-64000\n",
      "\n",
      "2017-11-07T13:08:48.517248: step 64100, loss 0.580329, rating_acc 0.729591->0.725691, rate 0.000375\n",
      "2017-11-07T13:08:49.818456: step 64200, loss 0.90463, rating_acc 0.927669->0.922443, rate 0.000375\n",
      "2017-11-07T13:08:51.490900: step 64300, loss 0.401116, rating_acc 0.592473->0.589374, rate 0.000375\n",
      "2017-11-07T13:08:53.171010: step 64400, loss 0.267043, rating_acc 0.464062->0.461836, rate 0.000375\n",
      "2017-11-07T13:08:54.856954: step 64500, loss 0.625712, rating_acc 0.759214->0.756249, rate 0.000375\n",
      "2017-11-07T13:08:56.545025: step 64600, loss 0.224368, rating_acc 0.415512->0.412977, rate 0.000375\n",
      "2017-11-07T13:08:58.231191: step 64700, loss 0.526555, rating_acc 0.690162->0.687561, rate 0.000375\n",
      "2017-11-07T13:08:59.876292: step 64800, loss 0.423017, rating_acc 0.611009->0.607592, rate 0.000375\n",
      "2017-11-07T13:09:01.274083: step 64900, loss 0.39369, rating_acc 0.586442->0.582917, rate 0.000375\n",
      "2017-11-07T13:09:02.400925: step 65000, loss 0.330505, rating_acc 0.530361->0.525914, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:09:02.403393: step 65000, loss 0.794032, rating_acc 0.860298\n",
      "\n",
      "2017-11-07T13:09:03.814156: step 65100, loss 0.269729, rating_acc 0.467055->0.464531, rate 0.000375\n",
      "2017-11-07T13:09:05.468356: step 65200, loss 0.743827, rating_acc 0.835887->0.830573, rate 0.000375\n",
      "2017-11-07T13:09:07.118905: step 65300, loss 0.49235, rating_acc 0.664778->0.66206, rate 0.000375\n",
      "2017-11-07T13:09:08.782436: step 65400, loss 0.443296, rating_acc 0.626371->0.623863, rate 0.000375\n",
      "2017-11-07T13:09:10.434663: step 65500, loss 0.232041, rating_acc 0.424323->0.421764, rate 0.000375\n",
      "2017-11-07T13:09:12.089513: step 65600, loss 0.719205, rating_acc 0.818568->0.815487, rate 0.000375\n",
      "2017-11-07T13:09:13.765894: step 65700, loss 0.595063, rating_acc 0.73937->0.735417, rate 0.000375\n",
      "2017-11-07T13:09:15.402237: step 65800, loss 0.475152, rating_acc 0.651561->0.648785, rate 0.000375\n",
      "2017-11-07T13:09:17.067689: step 65900, loss 0.532017, rating_acc 0.694951->0.691207, rate 0.000375\n",
      "2017-11-07T13:09:18.711884: step 66000, loss 0.313715, rating_acc 0.512448->0.509356, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:09:18.717433: step 66000, loss 0.794273, rating_acc 0.860233\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-66000\n",
      "\n",
      "2017-11-07T13:09:20.312002: step 66100, loss 0.241137, rating_acc 0.434797->0.432211, rate 0.000375\n",
      "2017-11-07T13:09:21.432944: step 66200, loss 0.679795, rating_acc 0.794604->0.790851, rate 0.000375\n",
      "2017-11-07T13:09:22.703766: step 66300, loss 0.850916, rating_acc 0.895454->0.892494, rate 0.000375\n",
      "2017-11-07T13:09:24.360165: step 66400, loss 0.484918, rating_acc 0.659562->0.656156, rate 0.000375\n",
      "2017-11-07T13:09:26.052886: step 66500, loss 0.32836, rating_acc 0.527809->0.523416, rate 0.000375\n",
      "2017-11-07T13:09:27.715693: step 66600, loss 0.472265, rating_acc 0.648448->0.64642, rate 0.000375\n",
      "2017-11-07T13:09:29.379103: step 66700, loss 0.576325, rating_acc 0.725546->0.722431, rate 0.000375\n",
      "2017-11-07T13:09:31.043237: step 66800, loss 0.44228, rating_acc 0.624337->0.622787, rate 0.000375\n",
      "2017-11-07T13:09:32.716950: step 66900, loss 0.393754, rating_acc 0.585442->0.582502, rate 0.000375\n",
      "2017-11-07T13:09:34.384590: step 67000, loss 0.316794, rating_acc 0.515485->0.512192, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:09:34.388261: step 67000, loss 0.794949, rating_acc 0.86052\n",
      "\n",
      "2017-11-07T13:09:36.054341: step 67100, loss 0.778963, rating_acc 0.855613->0.851169, rate 0.000375\n",
      "2017-11-07T13:09:37.708673: step 67200, loss 0.354656, rating_acc 0.55004->0.547852, rate 0.000375\n",
      "2017-11-07T13:09:39.344227: step 67300, loss 0.346907, rating_acc 0.543318->0.540715, rate 0.000375\n",
      "2017-11-07T13:09:40.458435: step 67400, loss 0.254195, rating_acc 0.448681->0.446827, rate 0.000375\n",
      "2017-11-07T13:09:41.576085: step 67500, loss 1.17097, rating_acc 1.06083->1.0566, rate 0.000375\n",
      "2017-11-07T13:09:43.223980: step 67600, loss 0.488235, rating_acc 0.661828->0.658518, rate 0.000375\n",
      "2017-11-07T13:09:44.884298: step 67700, loss 0.294189, rating_acc 0.492362->0.489463, rate 0.000375\n",
      "2017-11-07T13:09:46.547370: step 67800, loss 0.568738, rating_acc 0.72017->0.717011, rate 0.000375\n",
      "2017-11-07T13:09:48.228531: step 67900, loss 0.266805, rating_acc 0.462204->0.460582, rate 0.000375\n",
      "2017-11-07T13:09:49.880711: step 68000, loss 0.645049, rating_acc 0.771974->0.768346, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:09:49.885029: step 68000, loss 0.795797, rating_acc 0.860874\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-68000\n",
      "\n",
      "2017-11-07T13:09:51.633880: step 68100, loss 0.513841, rating_acc 0.681347->0.677608, rate 0.000375\n",
      "2017-11-07T13:09:53.277468: step 68200, loss 0.869434, rating_acc 0.906057->0.902612, rate 0.000375\n",
      "2017-11-07T13:09:54.974285: step 68300, loss 0.591536, rating_acc 0.736932->0.732648, rate 0.000375\n",
      "2017-11-07T13:09:56.633780: step 68400, loss 0.271565, rating_acc 0.467597->0.465579, rate 0.000375\n",
      "2017-11-07T13:09:58.305158: step 68500, loss 0.283817, rating_acc 0.481084->0.478538, rate 0.000375\n",
      "2017-11-07T13:09:59.489686: step 68600, loss 0.506968, rating_acc 0.673952->0.672406, rate 0.000375\n",
      "2017-11-07T13:10:00.739555: step 68700, loss 0.405456, rating_acc 0.594563->0.592062, rate 0.000375\n",
      "2017-11-07T13:10:02.365978: step 68800, loss 0.448026, rating_acc 0.62982->0.626951, rate 0.000375\n",
      "2017-11-07T13:10:04.041787: step 68900, loss 0.254752, rating_acc 0.450369->0.446928, rate 0.000375\n",
      "2017-11-07T13:10:05.719291: step 69000, loss 0.689619, rating_acc 0.800725->0.796608, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:10:05.722944: step 69000, loss 0.798198, rating_acc 0.862069\n",
      "\n",
      "2017-11-07T13:10:07.395178: step 69100, loss 0.60082, rating_acc 0.74205->0.738734, rate 0.000375\n",
      "2017-11-07T13:10:09.084269: step 69200, loss 0.248017, rating_acc 0.440604->0.43917, rate 0.000375\n",
      "2017-11-07T13:10:10.723299: step 69300, loss 0.390092, rating_acc 0.583402->0.578708, rate 0.000375\n",
      "2017-11-07T13:10:12.379511: step 69400, loss 0.280226, rating_acc 0.475886->0.474363, rate 0.000375\n",
      "2017-11-07T13:10:14.058354: step 69500, loss 0.48462, rating_acc 0.657428->0.655264, rate 0.000375\n",
      "2017-11-07T13:10:15.712852: step 69600, loss 0.492794, rating_acc 0.66409->0.661399, rate 0.000375\n",
      "2017-11-07T13:10:17.345836: step 69700, loss 0.332188, rating_acc 0.529063->0.526099, rate 0.000375\n",
      "2017-11-07T13:10:18.659088: step 69800, loss 0.259112, rating_acc 0.452342->0.451285, rate 0.000375\n",
      "2017-11-07T13:10:19.774426: step 69900, loss 0.851951, rating_acc 0.896513->0.892465, rate 0.000375\n",
      "2017-11-07T13:10:21.247806: step 70000, loss 0.529581, rating_acc 0.691009->0.688529, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:10:21.253276: step 70000, loss 0.796864, rating_acc 0.86102\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-70000\n",
      "\n",
      "2017-11-07T13:10:23.037398: step 70100, loss 0.425125, rating_acc 0.610062->0.60796, rate 0.000375\n",
      "2017-11-07T13:10:24.691050: step 70200, loss 0.494417, rating_acc 0.666075->0.662493, rate 0.000375\n",
      "2017-11-07T13:10:26.339200: step 70300, loss 0.580086, rating_acc 0.729179->0.724261, rate 0.000375\n",
      "2017-11-07T13:10:28.002953: step 70400, loss 0.592573, rating_acc 0.736126->0.7328, rate 0.000375\n",
      "2017-11-07T13:10:29.669897: step 70500, loss 0.268815, rating_acc 0.463826->0.46175, rate 0.000375\n",
      "2017-11-07T13:10:31.351247: step 70600, loss 0.336756, rating_acc 0.531869->0.530197, rate 0.000375\n",
      "2017-11-07T13:10:33.017902: step 70700, loss 0.169374, rating_acc 0.341428->0.337153, rate 0.000375\n",
      "2017-11-07T13:10:34.680508: step 70800, loss 0.536965, rating_acc 0.697594->0.693742, rate 0.000375\n",
      "2017-11-07T13:10:36.380496: step 70900, loss 0.400596, rating_acc 0.588885->0.587263, rate 0.000375\n",
      "2017-11-07T13:10:37.735875: step 71000, loss 0.48696, rating_acc 0.660482->0.656678, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:10:37.738442: step 71000, loss 0.795553, rating_acc 0.860127\n",
      "\n",
      "2017-11-07T13:10:38.843495: step 71100, loss 0.682094, rating_acc 0.794891->0.791419, rate 0.000375\n",
      "2017-11-07T13:10:40.185712: step 71200, loss 0.674284, rating_acc 0.789546->0.78646, rate 0.000375\n",
      "2017-11-07T13:10:41.846211: step 71300, loss 0.591805, rating_acc 0.737699->0.732138, rate 0.000375\n",
      "2017-11-07T13:10:43.537494: step 71400, loss 0.483427, rating_acc 0.658727->0.653921, rate 0.000375\n",
      "2017-11-07T13:10:45.185863: step 71500, loss 0.440863, rating_acc 0.623117->0.620494, rate 0.000375\n",
      "2017-11-07T13:10:46.848546: step 71600, loss 0.296522, rating_acc 0.492203->0.490562, rate 0.000375\n",
      "2017-11-07T13:10:48.495838: step 71700, loss 0.504371, rating_acc 0.673958->0.669697, rate 0.000375\n",
      "2017-11-07T13:10:50.139793: step 71800, loss 0.510411, rating_acc 0.678113->0.674168, rate 0.000375\n",
      "2017-11-07T13:10:51.802295: step 71900, loss 0.563159, rating_acc 0.715345->0.712189, rate 0.000375\n",
      "2017-11-07T13:10:53.451140: step 72000, loss 0.339838, rating_acc 0.533978->0.532798, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:10:53.455208: step 72000, loss 0.79645, rating_acc 0.860515\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-72000\n",
      "\n",
      "2017-11-07T13:10:55.208206: step 72100, loss 0.532194, rating_acc 0.692903->0.690071, rate 0.000375\n",
      "2017-11-07T13:10:56.739382: step 72200, loss 0.356263, rating_acc 0.549673->0.547936, rate 0.000375\n",
      "2017-11-07T13:10:57.852509: step 72300, loss 0.264962, rating_acc 0.459302->0.457083, rate 0.000375\n",
      "2017-11-07T13:10:59.101464: step 72400, loss 0.399728, rating_acc 0.589224->0.586229, rate 0.000375\n",
      "2017-11-07T13:11:00.759630: step 72500, loss 0.26038, rating_acc 0.456129->0.451946, rate 0.000375\n",
      "2017-11-07T13:11:02.415329: step 72600, loss 0.353764, rating_acc 0.548498->0.545532, rate 0.000375\n",
      "2017-11-07T13:11:04.071488: step 72700, loss 0.570339, rating_acc 0.721711->0.717059, rate 0.000375\n",
      "2017-11-07T13:11:05.728925: step 72800, loss 0.49006, rating_acc 0.664299->0.658674, rate 0.000375\n",
      "2017-11-07T13:11:07.398074: step 72900, loss 0.504463, rating_acc 0.674649->0.669465, rate 0.000375\n",
      "2017-11-07T13:11:09.046180: step 73000, loss 0.629405, rating_acc 0.76165->0.757021, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:11:09.050322: step 73000, loss 0.797189, rating_acc 0.860735\n",
      "\n",
      "2017-11-07T13:11:10.694346: step 73100, loss 0.628136, rating_acc 0.76077->0.756124, rate 0.000375\n",
      "2017-11-07T13:11:12.364706: step 73200, loss 0.371709, rating_acc 0.566522->0.561485, rate 0.000375\n",
      "2017-11-07T13:11:14.027243: step 73300, loss 0.512317, rating_acc 0.679097->0.675151, rate 0.000375\n",
      "2017-11-07T13:11:15.551336: step 73400, loss 0.252172, rating_acc 0.444038->0.442333, rate 0.000375\n",
      "2017-11-07T13:11:16.666308: step 73500, loss 0.574679, rating_acc 0.724804->0.719821, rate 0.000375\n",
      "2017-11-07T13:11:17.772418: step 73600, loss 0.836286, rating_acc 0.887524->0.883011, rate 0.000375\n",
      "2017-11-07T13:11:19.091182: step 73700, loss 0.518546, rating_acc 0.690124->0.679668, rate 0.000375\n",
      "2017-11-07T13:11:20.192897: step 73800, loss 0.419073, rating_acc 0.606668->0.602013, rate 0.000375\n",
      "2017-11-07T13:11:21.473400: step 73900, loss 0.244012, rating_acc 0.434197->0.432793, rate 0.000375\n",
      "2017-11-07T13:11:23.145436: step 74000, loss 0.500735, rating_acc 0.670668->0.666346, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:11:23.151761: step 74000, loss 0.797696, rating_acc 0.860801\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-74000\n",
      "\n",
      "2017-11-07T13:11:24.937172: step 74100, loss 0.419299, rating_acc 0.604498->0.602131, rate 0.000375\n",
      "2017-11-07T13:11:26.613389: step 74200, loss 0.522935, rating_acc 0.686375->0.68277, rate 0.000375\n",
      "2017-11-07T13:11:28.278754: step 74300, loss 0.395264, rating_acc 0.584699->0.581787, rate 0.000375\n",
      "2017-11-07T13:11:29.975922: step 74400, loss 0.780979, rating_acc 0.854662->0.850988, rate 0.000375\n",
      "2017-11-07T13:11:31.633500: step 74500, loss 0.478249, rating_acc 0.65248->0.649161, rate 0.000375\n",
      "2017-11-07T13:11:33.292347: step 74600, loss 0.462823, rating_acc 0.638998->0.637159, rate 0.000375\n",
      "2017-11-07T13:11:34.995285: step 74700, loss 0.681383, rating_acc 0.793644->0.790268, rate 0.000375\n",
      "2017-11-07T13:11:36.675527: step 74800, loss 0.366088, rating_acc 0.557951->0.556043, rate 0.000375\n",
      "2017-11-07T13:11:38.263472: step 74900, loss 0.68364, rating_acc 0.795466->0.791644, rate 0.000375\n",
      "2017-11-07T13:11:39.393323: step 75000, loss 0.592951, rating_acc 0.736068->0.732114, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:11:39.396793: step 75000, loss 0.798057, rating_acc 0.86087\n",
      "\n",
      "2017-11-07T13:11:40.584181: step 75100, loss 0.401788, rating_acc 0.593219->0.587199, rate 0.000375\n",
      "2017-11-07T13:11:42.275254: step 75200, loss 0.584563, rating_acc 0.729872->0.726319, rate 0.000375\n",
      "2017-11-07T13:11:43.935090: step 75300, loss 0.489498, rating_acc 0.662793->0.657616, rate 0.000375\n",
      "2017-11-07T13:11:45.632122: step 75400, loss 0.929225, rating_acc 0.93817->0.933904, rate 0.000375\n",
      "2017-11-07T13:11:47.299068: step 75500, loss 0.853907, rating_acc 0.897617->0.892647, rate 0.000375\n",
      "2017-11-07T13:11:48.993240: step 75600, loss 0.351766, rating_acc 0.545023->0.542818, rate 0.000375\n",
      "2017-11-07T13:11:50.664584: step 75700, loss 0.43928, rating_acc 0.6227->0.6182, rate 0.000375\n",
      "2017-11-07T13:11:52.311332: step 75800, loss 0.51531, rating_acc 0.67967->0.676876, rate 0.000375\n",
      "2017-11-07T13:11:53.978162: step 75900, loss 0.349367, rating_acc 0.543787->0.540551, rate 0.000375\n",
      "2017-11-07T13:11:55.642679: step 76000, loss 1.16574, rating_acc 1.0575->1.05288, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T13:11:55.647421: step 76000, loss 0.798871, rating_acc 0.861208\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510051919/checkpoints/model-76000\n",
      "\n",
      "2017-11-07T13:11:57.419745: step 76100, loss 0.426177, rating_acc 0.609765->0.607408, rate 0.000375\n",
      "2017-11-07T13:11:58.527647: step 76200, loss 0.480005, rating_acc 0.653974->0.650197, rate 0.000375\n",
      "2017-11-07T13:11:59.648748: step 76300, loss 0.194279, rating_acc 0.372767->0.370106, rate 0.000375\n",
      "0...\n",
      "MRR is 8.625894936599672e-05\n",
      "Precision@10 is 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEACAYAAAByG0uxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npw8+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feec0757cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l2_reg_lambda = 5e-6\n",
    "tf.Graph().as_default()\n",
    "session_conf = tf.ConfigProto(\n",
    "    allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "    log_device_placement=FLAGS.log_device_placement)\n",
    "session_conf.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=session_conf)\n",
    "sess.as_default()\n",
    "model = PredictionModel(\n",
    "    num_users=num_users,\n",
    "    num_movies=num_movies,\n",
    "    embedding_dim=FLAGS.embedding_dim,\n",
    "    l2_reg_lambda=l2_reg_lambda)\n",
    "last_accuracy = train(model, sess, 3e-3, 20000, 0.5)\n",
    "mrr, precision_at_10 = calc_precision(model, sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...\n",
      "11593\n",
      "817\n",
      "11536\n",
      "10822\n",
      "7\n",
      "10996\n",
      "8066\n",
      "526\n",
      "10921\n",
      "604\n",
      "1197\n",
      "2709\n",
      "330\n",
      "11498\n",
      "3493\n",
      "4620\n",
      "10131\n",
      "433\n",
      "8570\n",
      "7468\n",
      "538\n",
      "3016\n",
      "10058\n",
      "10747\n",
      "531\n",
      "10312\n",
      "1827\n",
      "11258\n",
      "10387\n",
      "7329\n",
      "346\n",
      "622\n",
      "8681\n",
      "5682\n",
      "7637\n",
      "11453\n",
      "2626\n",
      "9797\n",
      "9097\n",
      "8467\n",
      "1905\n",
      "305\n",
      "2107\n",
      "381\n",
      "9745\n",
      "392\n",
      "8621\n",
      "336\n",
      "159\n",
      "2058\n",
      "50...\n",
      "11447\n",
      "10051\n",
      "521\n",
      "11205\n",
      "576\n",
      "1190\n",
      "8332\n",
      "499\n",
      "430\n",
      "26\n",
      "10644\n",
      "3\n",
      "294\n",
      "291\n",
      "3194\n",
      "2262\n",
      "27\n",
      "10291\n",
      "11633\n",
      "1767\n",
      "4480\n",
      "272\n",
      "8840\n",
      "1343\n",
      "8410\n",
      "2754\n",
      "1\n",
      "166\n",
      "8247\n",
      "430\n",
      "11\n",
      "936\n",
      "2694\n",
      "694\n",
      "521\n",
      "474\n",
      "989\n",
      "9330\n",
      "11583\n",
      "7735\n",
      "1062\n",
      "1769\n",
      "837\n",
      "9252\n",
      "5802\n",
      "7725\n",
      "10623\n",
      "1663\n",
      "451\n",
      "4646\n",
      "100...\n",
      "10167\n",
      "90\n",
      "180\n",
      "3086\n",
      "11292\n",
      "58\n",
      "1675\n",
      "11263\n",
      "8089\n",
      "8607\n",
      "2\n",
      "7145\n",
      "3207\n",
      "2526\n",
      "338\n",
      "8847\n",
      "8239\n",
      "238\n",
      "1611\n",
      "2612\n",
      "354\n",
      "192\n",
      "2322\n",
      "300\n",
      "11785\n",
      "2011\n",
      "8814\n",
      "11461\n",
      "9818\n",
      "8524\n",
      "8591\n",
      "10394\n",
      "1455\n",
      "4149\n",
      "9798\n",
      "1456\n",
      "10937\n",
      "46\n",
      "10558\n",
      "8586\n",
      "10131\n",
      "8228\n",
      "1504\n",
      "1026\n",
      "11061\n",
      "11277\n",
      "4890\n",
      "7634\n",
      "1674\n",
      "6675\n",
      "150...\n",
      "1665\n",
      "2705\n",
      "475\n",
      "1000\n",
      "808\n",
      "971\n",
      "10912\n",
      "51\n",
      "777\n",
      "9840\n",
      "162\n",
      "11096\n",
      "2815\n",
      "524\n",
      "8328\n",
      "139\n",
      "1333\n",
      "149\n",
      "7646\n",
      "9159\n",
      "8876\n",
      "11551\n",
      "67\n",
      "11100\n",
      "312\n",
      "7847\n",
      "1190\n",
      "323\n",
      "4148\n",
      "509\n",
      "405\n",
      "554\n",
      "10544\n",
      "8285\n",
      "4063\n",
      "1524\n",
      "3378\n",
      "882\n",
      "47\n",
      "3421\n",
      "6\n",
      "10690\n",
      "2494\n",
      "1460\n",
      "607\n",
      "6622\n",
      "6\n",
      "9\n",
      "3022\n",
      "10674\n",
      "200...\n",
      "1193\n",
      "2741\n",
      "7232\n",
      "1\n",
      "11330\n",
      "10276\n",
      "8283\n",
      "265\n",
      "8186\n",
      "10633\n",
      "995\n",
      "4\n",
      "733\n",
      "157\n",
      "411\n",
      "2250\n",
      "2969\n",
      "6388\n",
      "1264\n",
      "425\n",
      "1147\n",
      "5429\n",
      "43\n",
      "9484\n",
      "9903\n",
      "1557\n",
      "7041\n",
      "1897\n",
      "11072\n",
      "1676\n",
      "1748\n",
      "193\n",
      "6645\n",
      "10662\n",
      "7577\n",
      "11718\n",
      "1118\n",
      "8147\n",
      "2289\n",
      "11545\n",
      "8098\n",
      "11537\n",
      "3163\n",
      "6583\n",
      "11302\n",
      "2742\n",
      "7659\n",
      "140\n",
      "2529\n",
      "1144\n",
      "250...\n",
      "10140\n",
      "46\n",
      "628\n",
      "606\n",
      "492\n",
      "6718\n",
      "276\n",
      "9844\n",
      "10344\n",
      "9863\n",
      "8601\n",
      "32\n",
      "10565\n",
      "9923\n",
      "8459\n",
      "509\n",
      "10916\n",
      "8596\n",
      "1479\n",
      "11007\n",
      "10979\n",
      "2613\n",
      "11284\n",
      "10725\n",
      "8731\n",
      "2721\n",
      "11163\n",
      "3120\n",
      "545\n",
      "2938\n",
      "123\n",
      "3020\n",
      "11210\n",
      "847\n",
      "243\n",
      "2220\n",
      "1641\n",
      "3098\n",
      "9886\n",
      "391\n",
      "1674\n",
      "8811\n",
      "9278\n",
      "9535\n",
      "8164\n",
      "3624\n",
      "849\n",
      "9405\n",
      "11777\n",
      "10945\n",
      "300...\n",
      "10927\n",
      "9812\n",
      "1463\n",
      "2275\n",
      "3749\n",
      "609\n",
      "6700\n",
      "5246\n",
      "8502\n",
      "9302\n",
      "2725\n",
      "1616\n",
      "10182\n",
      "9\n",
      "1318\n",
      "1334\n",
      "11183\n",
      "6218\n",
      "567\n",
      "44\n",
      "5392\n",
      "2170\n",
      "10010\n",
      "10859\n",
      "9196\n",
      "2181\n",
      "9479\n",
      "11424\n",
      "10256\n",
      "1129\n",
      "13\n",
      "10813\n",
      "10954\n",
      "753\n",
      "369\n",
      "11365\n",
      "484\n",
      "7798\n",
      "10491\n",
      "10873\n",
      "1312\n",
      "2805\n",
      "8171\n",
      "43\n",
      "9675\n",
      "10311\n",
      "380\n",
      "1834\n",
      "117\n",
      "6102\n",
      "350...\n",
      "7696\n",
      "1637\n",
      "7426\n",
      "9313\n",
      "49\n",
      "1\n",
      "622\n",
      "2511\n",
      "9477\n",
      "8518\n",
      "532\n",
      "1739\n",
      "377\n",
      "9646\n",
      "438\n",
      "7928\n",
      "613\n",
      "192\n",
      "7896\n",
      "5048\n",
      "767\n",
      "41\n",
      "685\n",
      "10355\n",
      "18\n",
      "2\n",
      "11419\n",
      "5796\n",
      "3488\n",
      "9617\n",
      "8242\n",
      "2651\n",
      "7744\n",
      "6639\n",
      "11122\n",
      "248\n",
      "1642\n",
      "2454\n",
      "3908\n",
      "10\n",
      "247\n",
      "82\n",
      "9441\n",
      "3157\n",
      "8944\n",
      "9249\n",
      "6543\n",
      "809\n",
      "723\n",
      "8346\n",
      "400...\n",
      "282\n",
      "7218\n",
      "317\n",
      "9525\n",
      "11434\n",
      "10999\n",
      "191\n",
      "3479\n",
      "519\n",
      "164\n",
      "2984\n",
      "1781\n",
      "1766\n",
      "2451\n",
      "9655\n",
      "2256\n",
      "4457\n",
      "10710\n",
      "10053\n",
      "5959\n",
      "4137\n",
      "8983\n",
      "10637\n",
      "10232\n",
      "84\n",
      "8728\n",
      "10836\n",
      "579\n",
      "1822\n",
      "9226\n",
      "1487\n",
      "8033\n",
      "10530\n",
      "62\n",
      "4598\n",
      "651\n",
      "631\n",
      "10292\n",
      "10636\n",
      "2194\n",
      "4229\n",
      "3827\n",
      "147\n",
      "156\n",
      "8622\n",
      "2941\n",
      "4743\n",
      "312\n",
      "1323\n",
      "11567\n",
      "450...\n",
      "5246\n",
      "2094\n",
      "9209\n",
      "3\n",
      "6747\n",
      "1667\n",
      "11073\n",
      "8692\n",
      "1500\n",
      "81\n",
      "10376\n",
      "3592\n",
      "10068\n",
      "8118\n",
      "4094\n",
      "340\n",
      "8845\n",
      "57\n",
      "1365\n",
      "5110\n",
      "8056\n",
      "36\n",
      "282\n",
      "10261\n",
      "8889\n",
      "167\n",
      "5692\n",
      "8783\n",
      "7048\n",
      "1123\n",
      "193\n",
      "1964\n",
      "10006\n",
      "593\n",
      "9666\n",
      "10103\n",
      "760\n",
      "722\n",
      "16\n",
      "511\n",
      "1117\n",
      "600\n",
      "9945\n",
      "1\n",
      "437\n",
      "8390\n",
      "8864\n",
      "382\n",
      "10026\n",
      "200\n",
      "0.0157223994235\n",
      "0.03\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "mrr = 0\n",
    "precision_at_10 = 0\n",
    "n = 500 # calculate only on first n users in validation set\n",
    "for i in range(n):\n",
    "    user_id, movie_id, rating = x_val[i]\n",
    "    if i % 50 == 0:\n",
    "        print('{}...'.format(i))\n",
    "    num_movies = ratings.id_assigner.get_next_id()\n",
    "    batch_movie_id = np.arange(num_movies)\n",
    "    #batch_user_id = np.ones_like(batch_movie_id) * user_id\n",
    "    #batch_rating = np.zeros_like(batch_movie_id)\n",
    "    feed_dict = {\n",
    "        model.input_user_id: [user_id],\n",
    "        model.input_movie_id: batch_movie_id,\n",
    "    }\n",
    "    scores = sess.run(model.get_predictions(), feed_dict=feed_dict)\n",
    "    #print(scores)\n",
    "#         tmp1, tmp2, tmp3 = sess.run([model.tmp1, model.tmp2, model.tmp3], feed_dict=feed_dict)\n",
    "#         print(tmp1)\n",
    "#         print(tmp2)\n",
    "#         print(tmp3)\n",
    "    s = scores[movie_id] # the score for the correct movie\n",
    "    #print(s)\n",
    "    train_movies = x_train[x_train['user_id'] == user_id]['movie_id']\n",
    "    not_watched = (scores == scores) # all True\n",
    "    not_watched[train_movies] = False\n",
    "    higher_scores = (scores > s)    \n",
    "    rank = np.sum(higher_scores & not_watched) + 1\n",
    "    ranks.append(rank)\n",
    "    #print('for user_id {} the rank is {}'.format(user_id, rank))\n",
    "    mrr += 1. / rank\n",
    "    if rank <= 10:\n",
    "        precision_at_10 += 1\n",
    "    print(rank)\n",
    "mrr /= n\n",
    "precision_at_10 /= n\n",
    "print(mrr)\n",
    "print(precision_at_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEPtJREFUeJzt3X+o3Xd9x/HnaylxbGxG12sXknSJ8yoL4rZwTbPBmG7q\nklZ6ZYytmaO1dl4yGxljoOkc/ierOtgIDc0ihrQgzar44w4jsRScMMyWW9GuscZeQl2TtWuqENjK\nDJnv/XE/1cPtvfd8789zk/t8wCHn+/m8P+d8vh9Ozivn+z3nm1QVkiT91KAnIElaHQwESRJgIEiS\nGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEgDXDXoC83H99dfX1q1bBz0NSbqqPPbYYy9U\n1VC/uqsqELZu3crExMSgpyFJV5Uk3+tS5yEjSRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCB\nIElqrqofpi3G1gNfWtT4p++9ZYlmIkmrk58QJEmAgSBJagwESRJgIEiSmk6BkGR3krNJJpMcmKE/\nSQ62/seT7OjpO5rk+SRPTBvziSTfafWfT7Jh8bsjSVqovoGQZB1wCNgDbAf2Jtk+rWwPMNxuY8D9\nPX3HgN0zPPQjwBur6k3Ad4F75jt5SdLS6fIJYScwWVXnquoycBwYnVYzCjxYU04BG5JsBKiqrwE/\nmP6gVfWVqrrSNk8Bmxe6E5KkxesSCJuAZ3q2z7e2+dbM5b3Al+dRL0laYgM/qZzkw8AV4NOz9I8l\nmUgycfHixZWdnCStIV0C4QKwpWd7c2ubb83LJHkP8E7g3VVVM9VU1ZGqGqmqkaGhvv8lqCRpgboE\nwmlgOMm2JOuB24DxaTXjwO3t20a7gEtV9excD5pkN/BB4NaqenEBc5ckLaG+gdBO/O4HTgJPAg9X\n1Zkk+5Lsa2UngHPAJPBJ4P0vjU/yEPB14A1Jzie5q3XdB/wc8EiSbyY5vFQ7JUmav04Xt6uqE0y9\n6fe2He65X8Dds4zdO0v767pPU5K03AZ+UlmStDoYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMg\nSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQ\nJEmNgSBJAgwESVJjIEiSgI6BkGR3krNJJpMcmKE/SQ62/seT7OjpO5rk+SRPTBvz6iSPJHmq/fmq\nxe+OJGmh+gZCknXAIWAPsB3Ym2T7tLI9wHC7jQH39/QdA3bP8NAHgEerahh4tG1LkgakyyeEncBk\nVZ2rqsvAcWB0Ws0o8GBNOQVsSLIRoKq+BvxghscdBR5o9x8A3rWQHZAkLY0ugbAJeKZn+3xrm2/N\ndDdU1bPt/nPADR3mIklaJqvipHJVFVAz9SUZSzKRZOLixYsrPDNJWju6BMIFYEvP9ubWNt+a6f7r\npcNK7c/nZyqqqiNVNVJVI0NDQx2mK0laiC6BcBoYTrItyXrgNmB8Ws04cHv7ttEu4FLP4aDZjAN3\ntPt3AF+cx7wlSUusbyBU1RVgP3ASeBJ4uKrOJNmXZF8rOwGcAyaBTwLvf2l8koeArwNvSHI+yV2t\n617g7UmeAt7WtiVJA3Jdl6KqOsHUm35v2+Ge+wXcPcvYvbO0fx/43c4zlSQtq1VxUlmSNHgGgiQJ\nMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLU\nGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkoGMgJNmd5GySySQHZuhPkoOt\n//EkO/qNTbIzyekk30wykWTn0uySJGkh+gZCknXAIWAPsB3Ym2T7tLI9wHC7jQH3dxj7ceAjVfVr\nwEfatiRpQLp8QtgJTFbVuaq6DBwHRqfVjAIP1pRTwIYkG/uMfQ74+Xb/lcB/LnJfJEmLcF2Hmk3A\nMz3b54GbOtRs6jP2Q8C/JPlbpoLpN7tPW5K01AZ5UvlTwJ9X1RbgL9r2yyQZa+cYJi5evLiiE5Sk\ntaRLIFwAtvRsb25tXWrmGnsT8Ll2/zNMHV56mao6UlUjVTUyNDTUYbqSpIXoEgingeEk25KsB24D\nxqfVjAO3t28b7QIuVdWzfcZOAr/d7v8O8NQi90WStAh9zyFU1ZUk+4GTwDrgaFWdSbKv9R8GTgA3\nM/Um/yJw51xj20OPAYeSvAL437YtSRqQLieVqaoTTL3p97Yd7rlfwN1dx7b208xymEiStPL8pbIk\nCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS\n1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1nQIhye4kZ5NMJjkw\nQ3+SHGz9jyfZ0WVskg8k+U6SM0k+vvjdkSQt1HX9CpKsAw4BbwfOA6eTjFfVt3vK9gDD7XYTcD9w\n01xjk7wVGAV+tap+mOQ1S7ljkqT56fIJYScwWVXnquoycJypN/Jeo8CDNeUUsCHJxj5j/wy4t6p+\nCFBVzy/B/kiSFqhLIGwCnunZPt/autTMNfb1wG8l+dck/5zkzfOZuCRpafU9ZLTMz/1qYBfwZuDh\nJK+tquotSjIGjAHceOONKz5JSVorunxCuABs6dne3Nq61Mw19jzwuXaY6d+AHwHXT3/yqjpSVSNV\nNTI0NNRhupKkhegSCKeB4STbkqwHbgPGp9WMA7e3bxvtAi5V1bN9xn4BeCtAktcD64EXFr1HkqQF\n6XvIqKquJNkPnATWAUer6kySfa3/MHACuBmYBF4E7pxrbHvoo8DRJE8Al4E7ph8ukiStnE7nEKrq\nBFNv+r1th3vuF3B317Gt/TLwJ/OZrCRp+fhLZUkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmA\ngSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTG\nQJAkAQaCJKkxECRJgIEgSWo6BUKS3UnOJplMcmCG/iQ52PofT7JjHmP/MkkluX5xuyJJWoy+gZBk\nHXAI2ANsB/Ym2T6tbA8w3G5jwP1dxibZArwD+I9F74kkaVG6fELYCUxW1bmqugwcB0an1YwCD9aU\nU8CGJBs7jP074INALXZHJEmL0yUQNgHP9Gyfb21damYdm2QUuFBV35rnnCVJy+C6QTxpkp8B/oqp\nw0X9aseYOgzFjTfeuMwzk6S1q8snhAvAlp7tza2tS81s7b8MbAO+leTp1v6NJL84/cmr6khVjVTV\nyNDQUIfpSpIWoksgnAaGk2xLsh64DRifVjMO3N6+bbQLuFRVz842tqr+vapeU1Vbq2orU4eSdlTV\nc0u1Y5Kk+el7yKiqriTZD5wE1gFHq+pMkn2t/zBwArgZmAReBO6ca+yy7Mky23rgS4sa//S9tyzR\nTCRpeXQ6h1BVJ5h60+9tO9xzv4C7u46doWZrl3lIkpaPv1SWJAEGgiSpMRAkScCAfoewFnlSWtJq\n5ycESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAk\nAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpKZTICTZneRskskkB2boT5KDrf/xJDv6jU3y\niSTfafWfT7JhaXZJkrQQfQMhyTrgELAH2A7sTbJ9WtkeYLjdxoD7O4x9BHhjVb0J+C5wz6L3RpK0\nYNd1qNkJTFbVOYAkx4FR4Ns9NaPAg1VVwKkkG5JsBLbONraqvtIz/hTwB4vdmWvZ1gNfWtT4p++9\nZYlmIula1eWQ0SbgmZ7t862tS02XsQDvBb7cYS6SpGUy8JPKST4MXAE+PUv/WJKJJBMXL15c2clJ\n0hrSJRAuAFt6tje3ti41c45N8h7gncC72+Gml6mqI1U1UlUjQ0NDHaYrSVqILoFwGhhOsi3JeuA2\nYHxazThwe/u20S7gUlU9O9fYJLuBDwK3VtWLS7Q/kqQF6ntSuaquJNkPnATWAUer6kySfa3/MHAC\nuBmYBF4E7pxrbHvo+4BXAI8kAThVVfuWcuckSd1lliM1q9LIyEhNTEwsaOxiv6Wz1vktJenqleSx\nqhrpVzfwk8qSpNXBQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQB3f6DHGng\n/A+CpOXnJwRJEmAgSJIaA0GSBBgIkqTGk8rqxJO60rXPTwiSJMBAkCQ1BoIkCTAQJEmNgSBJAvyW\nkdYIvyUl9WcgSCtg0IE06OfX1cFA0IpY7BvSWuf6aSV0OoeQZHeSs0kmkxyYoT9JDrb+x5Ps6Dc2\nyauTPJLkqfbnq5ZmlyRJC9E3EJKsAw4Be4DtwN4k26eV7QGG220MuL/D2APAo1U1DDzatiVJA9Ll\nkNFOYLKqzgEkOQ6MAt/uqRkFHqyqAk4l2ZBkI7B1jrGjwFva+AeArwIfWuT+SMtirR+yGfT+ew5j\nZXQJhE3AMz3b54GbOtRs6jP2hqp6tt1/Drih45wlrTHXQiBdDSf2V8VJ5aqqJDVTX5Ixpg5DAfx3\nkrM93a8ELnXcvh54YWlm/DLTn3epxsxVM1vfTO392qb3L9daLdc69atbzFr1277a1qpfzXKt1VX9\n9y8f6zx+2f7+zTCH+filTlVVNecN+A3gZM/2PcA902r+Adjbs30W2DjX2Jdq2v2NwNl+c5lhbke6\nbgMT8338hc5jqcbMVTNb30zt/dpmWLdlWavlWqflXKsO21fVWvWrWa618u/f7G0r9ZrqcuvyLaPT\nwHCSbUnWA7cB49NqxoHb27eNdgGXaupw0Fxjx4E72v07gC92mMt0/zTP7eWykOfpMmaumtn6Zmrv\n13a1r1O/usWs1Vp6Tc3V71p161vNf//6SkukuYuSm4G/B9YBR6vqo0n2AVTV4SQB7gN2Ay8Cd1bV\nxGxjW/svAA8DNwLfA/6wqn6wxPvXuw8TVTWyXI9/LXGtunOtunGduhvkWnUKhGtBkrGqOjLoeVwN\nXKvuXKtuXKfuBrlWayYQJElz82qnkiTAQJAkNQaCJAlYw4GQ5GeTPJDkk0nePej5rGZJXpvkU0k+\nO+i5rGZJ3tVeT/+Y5B2Dns9qluRXkhxO8pkkfzro+axm7b1qIsk7l/u5rqlASHI0yfNJnpjWPtMV\nV38f+GxVvQ+4dcUnO2DzWauqOldVdw1mpoM1z3X6Qns97QP+aBDzHaR5rtWTVfXSOv3eIOY7KPN8\nn4Kpa7w9vBJzu6YCATjG1G8hfmyOK65u5ifXWfq/FZzjanGM7mu1lh1j/uv0161/rTnGPNYqya3A\nCeD4yk5z4I7RcZ2SvJ2pi4E+vxITu6YCoaq+Bkz/cduPr9ZaVZeZevGNMnWhvc2t5ppahy7muVZr\n1nzWqf1S/2PAl6vqGys910Gb72uqqsarajc/uWLBmjDPdXoLsAv4Y+B9SZb1vWpVXNxumc12xdWD\nwH1JbmEV/XR8wGZcq/ar8o8Cv57knqr6m4HMbvWY7TX1AeBtwCuTvK6qDg9icqvMbK+ptzB12Pan\nmbr0/Vo34zpV1X6AJO8BXqiqHy3nJNZCIMyoqv4HuHPQ87gaVNX3mTourjlU1UGm/qGhPqrqqxgE\nnVXVsZV4nrVwqOQCsKVne3Nr08u5Vt24Tt25Vt2sinVaC4HQ5WqtmuJadeM6dedadbMq1umaCoQk\nDwFfB96Q5HySu6rqCrAfOAk8CTxcVWcGOc/VwLXqxnXqzrXqZjWvkxe3kyQB19gnBEnSwhkIkiTA\nQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLU/D825bzwAXLd9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fedb10fc828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ranks, bins=np.logspace(0., np.log10(ratings.id_assigner.get_next_id()) , 20), normed=1)\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "6\n",
      "8\n",
      "9\n",
      "9\n",
      "11\n",
      "12\n",
      "12\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 11):\n",
    "    print(np.sum([r <= k for r in ranks]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADTVJREFUeJzt3X+sX3ddx/Hny5ZFBsShvS7Y9nobUiCNujCuYzqjw8Wk\nHcRqYrRDGS6QZslK0Ji4zj/kDxMDQQ2SjDV1lrFI1igs0rCyQuaPmcyZdjLKulG8KbDestnOKVP4\nozR7+8f9zn17bXvPvT23p/v0+Uiafs85n9zzzjfps6fn3vNtqgpJUlt+YOgBJEn9M+6S1CDjLkkN\nMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNWjnUiVetWlVTU1NDnV6SXpEee+yx56pqYqF1g8V9\namqKAwcODHV6SXpFSvKtLuu8LSNJDTLuktQg4y5JDTLuktQg4y5JDVow7kl2JTme5ImzHE+SjyeZ\nSXIwydX9jylJWowuV+73ABvPcXwTsH70aytw1/mPJUk6HwvGvaoeBp4/x5LNwL0151HgiiRv6GtA\nSdLi9XHPfTVwdGx7drRPkjSQC/qEapKtzN26YXJy8kKeWpKY2v7A0CMA8M0Pv3PZz9HHlfsxYO3Y\n9prRvv+nqnZW1XRVTU9MLPjRCJKkJeoj7nuAm0c/NXMt8J2qeqaHrytJWqIFb8skuQ+4HliVZBb4\nEPAqgKraAewFbgRmgO8BtyzXsJKkbhaMe1XdtMDxAm7rbSJJ0nnzCVVJapBxl6QGGXdJapBxl6QG\nGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJ\napBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBx\nl6QGGXdJapBxl6QGdYp7ko1JDieZSbL9DMdXJXkwyVeSHEpyS/+jSpK6WjDuSVYAdwKbgA3ATUk2\nzFu2DfhKVV0FXA/8aZLLep5VktRRlyv3a4CZqjpSVSeB3cDmeWueBV6XJMBrgeeBU71OKknqbGWH\nNauBo2Pbs8Db5635C+Ah4NvA64DfqKoXe5lQkrRoXeLexR3AQeAdwBuBLyX5p6p6YXxRkq3AVoDJ\nycmeTi3pbKa2PzD0CBpIl9syx4C1Y9trRvvGXQf8Tc2ZAb4BvGX+F6qqnVU1XVXTExMTS51ZkrSA\nLnHfD6xPsm70TdItwJ55a74G3ACQ5ErgzcCRPgeVJHW34G2ZqjqVZBuwD1gB7KqqQ0luHR3fAfwx\n8MkkB5n7C+P2qnpuGeeWJJ1Dp3vuVbUX2Dtv346x1yeAd/U7miRpqXxCVZIaZNwlqUHGXZIaZNwl\nqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIa1NenQkqax09k1JC8cpekBhl3SWqQcZekBhl3SWqQcZek\nBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3\nSWqQcZekBhl3SWqQcZekBhl3SWqQcZekBnWKe5KNSQ4nmUmy/Sxrrk/yeJJDSf6x3zElSYuxcqEF\nSVYAdwK/BMwC+5Psqaonx9ZcAXwC2FhVTyf50eUaWJK0sC5X7tcAM1V1pKpOAruBzfPWvBu4v6qe\nBqiq4/2OKUlajC5xXw0cHdueHe0b9ybg9Un+IcljSW7ua0BJ0uIteFtmEV/nbcANwKuBf07yaFV9\nfXxRkq3AVoDJycmeTq2LxdT2B4YeQdJIlyv3Y8Dase01o33jZoF9VfXdqnoOeBi4av4XqqqdVTVd\nVdMTExNLnVmStIAucd8PrE+yLsllwBZgz7w1nwN+LsnKJJcDbwee6ndUSVJXC96WqapTSbYB+4AV\nwK6qOpTk1tHxHVX1VJIHgYPAi8DdVfXEcg4uSTq7Tvfcq2ovsHfevh3ztj8KfLS/0SRJS+UTqpLU\nIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ3q64PDNCA/sEvSfF65S1KDjLskNci4S1KD\njLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLsk\nNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDOsU9ycYkh5PMJNl+jnU/neRUkl/rb0RJ\n0mItGPckK4A7gU3ABuCmJBvOsu4jwBf7HlKStDhdrtyvAWaq6khVnQR2A5vPsO4DwGeB4z3OJ0la\ngi5xXw0cHdueHe37P0lWA78K3NXfaJKkpVrZ09f5GHB7Vb2Y5KyLkmwFtgJMTk72dOrhTG1/YOgR\nJOmMusT9GLB2bHvNaN+4aWD3KOyrgBuTnKqqvx1fVFU7gZ0A09PTtdShJUnn1iXu+4H1SdYxF/Ut\nwLvHF1TVupdeJ7kH+Pz8sEuSLpwF415Vp5JsA/YBK4BdVXUoya2j4zuWeUZJ0iJ1uudeVXuBvfP2\nnTHqVfXb5z+WJOl8+ISqJDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtS\ng4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7\nJDVo5dADLMXU9geGHkGSLmpeuUtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtS\ngzrFPcnGJIeTzCTZfobjv5nkYJKvJnkkyVX9jypJ6mrBuCdZAdwJbAI2ADcl2TBv2TeAX6iqnwT+\nCNjZ96CSpO66XLlfA8xU1ZGqOgnsBjaPL6iqR6rqP0ebjwJr+h1TkrQYXeK+Gjg6tj072nc27wO+\ncKYDSbYmOZDkwIkTJ7pPKUlalF6/oZrkHczF/fYzHa+qnVU1XVXTExMTfZ5akjSmy0f+HgPWjm2v\nGe07TZKfAu4GNlXVf/QzniRpKbpcue8H1idZl+QyYAuwZ3xBkkngfuA9VfX1/seUJC3GglfuVXUq\nyTZgH7AC2FVVh5LcOjq+A/hD4EeATyQBOFVV08s3tiTpXDr9T0xVtRfYO2/fjrHX7wfe3+9okqSl\n8glVSWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3\nSWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQ\ncZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBnWKe5KNSQ4nmUmy/QzHk+Tjo+MHk1zd/6iS\npK4WjHuSFcCdwCZgA3BTkg3zlm0C1o9+bQXu6nlOSdIidLlyvwaYqaojVXUS2A1snrdmM3BvzXkU\nuCLJG3qeVZLUUZe4rwaOjm3PjvYtdo0k6QJZeSFPlmQrc7dtAP4nyeELef5lsAp4bughLiK+H6fz\n/XiZ78WYfOS83o8f77KoS9yPAWvHtteM9i12DVW1E9jZZbBXgiQHqmp66DkuFr4fp/P9eJnvxeku\nxPvR5bbMfmB9knVJLgO2AHvmrdkD3Dz6qZlrge9U1TM9zypJ6mjBK/eqOpVkG7APWAHsqqpDSW4d\nHd8B7AVuBGaA7wG3LN/IkqSFdLrnXlV7mQv4+L4dY68LuK3f0V4RmrnF1BPfj9P5frzM9+J0y/5+\nZK7LkqSW+PEDktQg474ESdYm+fskTyY5lOSDQ880tCQrknw5yeeHnmVoSa5I8pkkX0vyVJKfGXqm\nISW5Y/Rn5Ykk9yX5waFnupCS7EpyPMkTY/t+OMmXkvzb6PfX931e4740p4Dfq6oNwLXAbWf4SIZL\nzQeBp4Ye4iLx58CDVfUW4Cou4fclyRRzz7a8rap+grkfytgy5EwDuAfYOG/fduChqloPPDTa7pVx\nX4Kqeqaq/nX0+r+Z+8N7yT6Rm2QN8E7g7qFnGVqSHwJ+HvhLgKo6WVX/NexUg3oB+D7w6iQrgcuB\nbw870oVVVQ8Dz8/bvRn41Oj1p4Bf6fu8xv08ja5M3gr8y7CTDOpjwO8DLw49yEVgHXAC+OToNtXd\nSV4z9FBDqarngT8BngaeYe4ZmC8OO9VF4cqxZ4GeBa7s+wTG/TwkeS3wWeB3quqFoecZQpJ3Acer\n6rGhZ7lIrASuBu6qqrcC32UZ/sn9SpHkjcDvMveX3o8Br0nyW8NOdXEZ/Sh57z+2aNyXKMmrmAv7\np6vq/qHnGdB1wC8n+SZznxj6i0n+atiRBjULzFbVS/+S+wxzsb9UTQOPVNWJqvo+cD/wswPPdDH4\n95c+OXf0+/G+T2DclyBJmLun+lRV/dnQ8wypqu6oqjVVNcXcN8r+rqou2SuzqnoWOJrkzaNdNwBP\nDjjS0A4D1ya5fPTn5gYu4W8wj9kDvHf0+r3A5/o+wQX9VMiGXAe8B/hqksdH+/5g9CSv9AHg06PP\nYjrCJfxxHFX1eJJ7gQPMfU/my1xiT6smuQ+4HliVZBb4EPBh4K+TvA/4FvDrvZ/XJ1QlqT3elpGk\nBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWrQ/wIJwAdWyehTigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fedab652400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ranks, bins=np.arange(1, 11), normed=1, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranks_backup = ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0134301587302\n"
     ]
    }
   ],
   "source": [
    "# MRR@10\n",
    "mrr10 = np.mean([(1./r if r <= 10 else 0) for r in ranks_backup])\n",
    "print(mrr10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...\n",
      "500...\n",
      "1000...\n",
      "1500...\n",
      "2000...\n",
      "2500...\n",
      "3000...\n",
      "3500...\n",
      "4000...\n",
      "4500...\n",
      "5000...\n",
      "5500...\n",
      "6000...\n",
      "6500...\n",
      "7000...\n",
      "7500...\n",
      "8000...\n",
      "8500...\n",
      "9000...\n",
      "9500...\n",
      "10000...\n",
      "10500...\n",
      "11000...\n",
      "11500...\n"
     ]
    }
   ],
   "source": [
    "# try a naive model - movie bias only\n",
    "\n",
    "total_watches = np.bincount(x_train['movie_id'])\n",
    "mean_rating = np.zeros(total_watches.shape)\n",
    "for i in range(ratings.id_assigner.get_next_id()):\n",
    "    if i % 500 == 0:\n",
    "        print('{}...'.format(i))\n",
    "    t = x_train[x_train['movie_id'] == i]['rating']\n",
    "    if len(t) > 0:\n",
    "        mean_rating[i] = np.mean(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...\n",
      "50...\n",
      "100...\n",
      "150...\n",
      "200...\n",
      "250...\n",
      "300...\n",
      "350...\n",
      "400...\n",
      "450...\n",
      "0.946751315076\n"
     ]
    }
   ],
   "source": [
    "# RMSE for the naive model\n",
    "\n",
    "rmse = 0.\n",
    "n = 500 # calculate only on first n users in validation set\n",
    "for i in range(n):\n",
    "    user_id, movie_id, rating = x_val[i]\n",
    "    if i % 50 == 0:\n",
    "        print('{}...'.format(i))\n",
    "    rmse += (rating - mean_rating[movie_id])**2\n",
    "rmse /= n\n",
    "rmse = np.sqrt(rmse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...\n",
      "4255\n",
      "557\n",
      "1166\n",
      "1723\n",
      "31\n",
      "837\n",
      "677\n",
      "51\n",
      "1021\n",
      "202\n",
      "761\n",
      "34\n",
      "208\n",
      "4292\n",
      "310\n",
      "1040\n",
      "1650\n",
      "78\n",
      "6796\n",
      "506\n",
      "886\n",
      "481\n",
      "3647\n",
      "1535\n",
      "1501\n",
      "1486\n",
      "2258\n",
      "30\n",
      "3140\n",
      "449\n",
      "690\n",
      "314\n",
      "855\n",
      "352\n",
      "3204\n",
      "2113\n",
      "281\n",
      "1112\n",
      "163\n",
      "487\n",
      "4269\n",
      "300\n",
      "736\n",
      "108\n",
      "152\n",
      "1154\n",
      "46\n",
      "270\n",
      "14\n",
      "291\n",
      "50...\n",
      "852\n",
      "2\n",
      "914\n",
      "4087\n",
      "681\n",
      "197\n",
      "874\n",
      "978\n",
      "224\n",
      "1068\n",
      "1715\n",
      "4\n",
      "212\n",
      "29\n",
      "6288\n",
      "376\n",
      "7\n",
      "1332\n",
      "344\n",
      "1712\n",
      "6320\n",
      "223\n",
      "1831\n",
      "679\n",
      "144\n",
      "131\n",
      "14\n",
      "53\n",
      "3363\n",
      "305\n",
      "6\n",
      "2215\n",
      "125\n",
      "196\n",
      "2152\n",
      "291\n",
      "2\n",
      "1093\n",
      "2048\n",
      "312\n",
      "255\n",
      "1662\n",
      "140\n",
      "3592\n",
      "647\n",
      "2236\n",
      "1025\n",
      "358\n",
      "391\n",
      "7\n",
      "100...\n",
      "1415\n",
      "759\n",
      "357\n",
      "75\n",
      "893\n",
      "474\n",
      "2617\n",
      "2388\n",
      "296\n",
      "21\n",
      "9\n",
      "273\n",
      "828\n",
      "3796\n",
      "98\n",
      "3742\n",
      "227\n",
      "269\n",
      "651\n",
      "7\n",
      "597\n",
      "1090\n",
      "785\n",
      "280\n",
      "4365\n",
      "364\n",
      "1713\n",
      "168\n",
      "3797\n",
      "349\n",
      "5730\n",
      "5467\n",
      "147\n",
      "1657\n",
      "2430\n",
      "1675\n",
      "1133\n",
      "265\n",
      "2079\n",
      "825\n",
      "106\n",
      "1107\n",
      "888\n",
      "4483\n",
      "740\n",
      "178\n",
      "188\n",
      "752\n",
      "60\n",
      "271\n",
      "150...\n",
      "277\n",
      "178\n",
      "465\n",
      "2658\n",
      "16\n",
      "49\n",
      "941\n",
      "74\n",
      "5\n",
      "3094\n",
      "1422\n",
      "3620\n",
      "294\n",
      "212\n",
      "4979\n",
      "9\n",
      "174\n",
      "425\n",
      "167\n",
      "1812\n",
      "366\n",
      "222\n",
      "118\n",
      "2866\n",
      "342\n",
      "2383\n",
      "76\n",
      "2247\n",
      "3410\n",
      "51\n",
      "58\n",
      "727\n",
      "1821\n",
      "6658\n",
      "16\n",
      "252\n",
      "1885\n",
      "457\n",
      "14\n",
      "333\n",
      "14\n",
      "3033\n",
      "314\n",
      "64\n",
      "1457\n",
      "502\n",
      "165\n",
      "27\n",
      "4847\n",
      "184\n",
      "200...\n",
      "148\n",
      "3300\n",
      "307\n",
      "13\n",
      "108\n",
      "26\n",
      "106\n",
      "1194\n",
      "3505\n",
      "1603\n",
      "427\n",
      "1\n",
      "1083\n",
      "27\n",
      "2070\n",
      "353\n",
      "2300\n",
      "2792\n",
      "418\n",
      "389\n",
      "3\n",
      "643\n",
      "410\n",
      "4494\n",
      "214\n",
      "92\n",
      "435\n",
      "1358\n",
      "1143\n",
      "644\n",
      "88\n",
      "35\n",
      "511\n",
      "346\n",
      "47\n",
      "2004\n",
      "7\n",
      "457\n",
      "2223\n",
      "328\n",
      "4854\n",
      "1233\n",
      "71\n",
      "4646\n",
      "238\n",
      "2132\n",
      "3226\n",
      "52\n",
      "169\n",
      "1112\n",
      "250...\n",
      "2612\n",
      "977\n",
      "408\n",
      "1689\n",
      "356\n",
      "2055\n",
      "1\n",
      "3749\n",
      "191\n",
      "6\n",
      "5\n",
      "896\n",
      "4206\n",
      "1302\n",
      "302\n",
      "302\n",
      "2920\n",
      "1487\n",
      "1587\n",
      "430\n",
      "87\n",
      "517\n",
      "1124\n",
      "394\n",
      "57\n",
      "585\n",
      "185\n",
      "1350\n",
      "1330\n",
      "1726\n",
      "1091\n",
      "2592\n",
      "189\n",
      "1541\n",
      "253\n",
      "154\n",
      "73\n",
      "44\n",
      "1354\n",
      "4344\n",
      "7122\n",
      "451\n",
      "488\n",
      "1418\n",
      "30\n",
      "23\n",
      "266\n",
      "4172\n",
      "1157\n",
      "287\n",
      "300...\n",
      "758\n",
      "4668\n",
      "814\n",
      "798\n",
      "3722\n",
      "191\n",
      "262\n",
      "618\n",
      "2746\n",
      "56\n",
      "1161\n",
      "1794\n",
      "1557\n",
      "13\n",
      "241\n",
      "400\n",
      "927\n",
      "2282\n",
      "124\n",
      "13\n",
      "1186\n",
      "88\n",
      "6103\n",
      "1316\n",
      "416\n",
      "157\n",
      "2\n",
      "2452\n",
      "204\n",
      "593\n",
      "47\n",
      "294\n",
      "676\n",
      "236\n",
      "163\n",
      "1513\n",
      "1552\n",
      "782\n",
      "387\n",
      "1904\n",
      "298\n",
      "4523\n",
      "4721\n",
      "688\n",
      "132\n",
      "287\n",
      "53\n",
      "2871\n",
      "1232\n",
      "106\n",
      "350...\n",
      "7392\n",
      "132\n",
      "76\n",
      "293\n",
      "13\n",
      "3\n",
      "1359\n",
      "19\n",
      "2458\n",
      "267\n",
      "17\n",
      "6\n",
      "107\n",
      "1514\n",
      "927\n",
      "4659\n",
      "6414\n",
      "140\n",
      "2847\n",
      "8421\n",
      "974\n",
      "90\n",
      "1157\n",
      "318\n",
      "440\n",
      "20\n",
      "2225\n",
      "612\n",
      "938\n",
      "685\n",
      "460\n",
      "8379\n",
      "3325\n",
      "987\n",
      "818\n",
      "3102\n",
      "1454\n",
      "1356\n",
      "1533\n",
      "10\n",
      "32\n",
      "7\n",
      "25\n",
      "2265\n",
      "739\n",
      "852\n",
      "2285\n",
      "56\n",
      "1034\n",
      "680\n",
      "400...\n",
      "844\n",
      "268\n",
      "678\n",
      "694\n",
      "398\n",
      "173\n",
      "1247\n",
      "7123\n",
      "654\n",
      "13\n",
      "219\n",
      "81\n",
      "5467\n",
      "4\n",
      "2411\n",
      "302\n",
      "2297\n",
      "947\n",
      "6754\n",
      "1275\n",
      "727\n",
      "39\n",
      "2497\n",
      "2794\n",
      "720\n",
      "111\n",
      "47\n",
      "495\n",
      "1594\n",
      "721\n",
      "1036\n",
      "61\n",
      "939\n",
      "433\n",
      "563\n",
      "1323\n",
      "60\n",
      "3441\n",
      "2167\n",
      "694\n",
      "2337\n",
      "3278\n",
      "897\n",
      "142\n",
      "1098\n",
      "470\n",
      "1361\n",
      "3217\n",
      "595\n",
      "37\n",
      "450...\n",
      "6002\n",
      "329\n",
      "539\n",
      "169\n",
      "8115\n",
      "434\n",
      "112\n",
      "74\n",
      "2865\n",
      "1210\n",
      "881\n",
      "2379\n",
      "21\n",
      "916\n",
      "858\n",
      "2849\n",
      "36\n",
      "2\n",
      "4168\n",
      "4850\n",
      "6066\n",
      "60\n",
      "371\n",
      "519\n",
      "2281\n",
      "273\n",
      "11872\n",
      "99\n",
      "2831\n",
      "5937\n",
      "36\n",
      "2550\n",
      "1676\n",
      "1063\n",
      "669\n",
      "2050\n",
      "680\n",
      "30\n",
      "2\n",
      "245\n",
      "102\n",
      "893\n",
      "2434\n",
      "56\n",
      "3252\n",
      "623\n",
      "81\n",
      "90\n",
      "195\n",
      "78\n",
      "0.0213013222575\n",
      "0.048\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD7BJREFUeJzt3X+s3Xddx/Hnyy7FuEQqUmbTH7bIBew/6ry01cQIhmG7\nkV3iH7qJ2ZhIU12JMSbQqeE/YkETtaFZLdJ0SwxlEH5cs5JKliCJodqOyKRAw00zXGvnNjFNdJGm\n8vaP8xmcnN17z/f+PPf2Ph/JSc/n17mf7yfnntf9fs/3+22qCkmSfmjUE5AkrQwGgiQJMBAkSY2B\nIEkCDARJUmMgSJKAjoGQZG+Si0mmkhyapj1JjrT2p5Lc3td2IslzSb42MOZVSb6Q5Fvt3x9b+OZI\nkuZraCAkWQccBfYBO4F7k+wc6LYPGGuP/cDDfW0ngb3TvPQh4ImqGgOeaGVJ0oh02UPYBUxV1aWq\nug6cAiYG+kwAj1bPWWBDkk0AVfUl4DvTvO4E8Eh7/gjwjvlsgCRpcXQJhM3AM33ly61urn0G3VZV\nV9vzZ4HbOsxFkrREbhn1BACqqpJMew+NJPvpHYbi1ltv/fk3vvGNyzo3SVrtnnzyyReqauOwfl0C\n4Qqwta+8pdXNtc+g/0iyqaqutsNLz03XqaqOA8cBxsfH6/z58x2mLEl6SZJvd+nX5ZDROWAsyY4k\n64F7gMmBPpPAfe1soz3Atb7DQTOZBO5vz+8HPtdlwpKkpTE0EKrqBnAQOAN8A3isqi4kOZDkQOt2\nGrgETAEfBX7vpfFJPg58GXhDkstJ3t2aDgN3JPkW8NZWliSNSFbT7a89ZCRJc5fkyaoaH9bPK5Ul\nSYCBIElqDARJEmAgSJIaA0GSBKyQK5VXg+2HHl/Q+KcP37VIM5GkpeEegiQJMBAkSY2BIEkCDARJ\nUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIk\nCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJKAjoGQ\nZG+Si0mmkhyapj1JjrT2p5LcPmxskl1JziX5lyTnk+xanE2SJM3H0EBIsg44CuwDdgL3Jtk50G0f\nMNYe+4GHO4z9MPCBqvpZ4AOtLEkakS57CLuAqaq6VFXXgVPAxECfCeDR6jkLbEiyacjYZ4Efbc9f\nCfz7ArdFkrQAt3Tosxl4pq98Gdjdoc/mIWPfD/xjkj+nF0y/2H3akqTFNsovlT8G/H5VbQX+oJVf\nJsn+9h3D+eeff35ZJyhJa0mXQLgCbO0rb2l1XfrMNnY38On2/JP0Di+9TFUdr6rxqhrfuHFjh+lK\nkuajSyCcA8aS7EiyHrgHmBzoMwnc18422gNcq6qrQ8ZOAb/cnv8K8K0FboskaQGGfodQVTeSHATO\nAOuAE1V1IcmB1n4MOA3cSe9D/kXggdnGtpfeDxxN8grgf1tZkjQiXb5UpqpO0/vQ76871ve8gAe7\njm3155jhMJEkafl5pbIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2B\nIEkCDARJUmMgSJIAA0GS1HS6/bVGb/uhxxc0/unDdy3STCTdrNxDkCQBBoIkqTEQJEmAgSBJagwE\nSRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAWvo\nf0xb6P84Jkk3O/cQJElAx0BIsjfJxSRTSQ5N054kR1r7U0lu7zI2yXuTfDPJhSQfXvjmSJLma+gh\noyTrgKPAHcBl4FySyar6el+3fcBYe+wGHgZ2zzY2yVuACeBnquq7SV6zmBsmSZqbLnsIu4CpqrpU\nVdeBU/Q+yPtNAI9Wz1lgQ5JNQ8b+LnC4qr4LUFXPLcL2SJLmqUsgbAae6StfbnVd+sw29vXALyX5\npyT/kORNc5m4JGlxjfIso1uAVwF7gDcBjyV5bVVVf6ck+4H9ANu2bVv2SUrSWtFlD+EKsLWvvKXV\ndekz29jLwKfbYaZ/Br4HvHrwh1fV8aoar6rxjRs3dpiuJGk+ugTCOWAsyY4k64F7gMmBPpPAfe1s\noz3Ataq6OmTsZ4G3ACR5PbAeeGHBWyRJmpehh4yq6kaSg8AZYB1woqouJDnQ2o8Bp4E7gSngReCB\n2ca2lz4BnEjyNeA6cP/g4SJJ0vLp9B1CVZ2m96HfX3es73kBD3Yd2+qvA781l8lKkpaOVypLkgAD\nQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2B\nIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSc8uoJ7BWbD/0+KinIEmz\ncg9BkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBHQMhCR7k1xMMpXk0DTtSXKk\ntT+V5PY5jP3DJJXk1QvbFEnSQgwNhCTrgKPAPmAncG+SnQPd9gFj7bEfeLjL2CRbgbcB/7bgLZEk\nLUiXPYRdwFRVXaqq68ApYGKgzwTwaPWcBTYk2dRh7F8A7wNqoRsiSVqYLoGwGXimr3y51XXpM+PY\nJBPAlar66hznLElaAiO522mSHwH+iN7homF999M7DMW2bduWeGaStHZ12UO4AmztK29pdV36zFT/\nU8AO4KtJnm71X0nyE4M/vKqOV9V4VY1v3Lixw3QlSfPRJRDOAWNJdiRZD9wDTA70mQTua2cb7QGu\nVdXVmcZW1b9W1WuqantVbad3KOn2qnp2sTZMkjQ3Qw8ZVdWNJAeBM8A64ERVXUhyoLUfA04DdwJT\nwIvAA7ONXZItkSQtSKfvEKrqNL0P/f66Y33PC3iw69hp+mzvMg9J0tLxSmVJEmAgSJIaA0GSBBgI\nkqTGQJAkAQaCJKkxECRJwIjuZaTVZ/uhxxc0/unDdy3STCQtFfcQJEmAgSBJagwESRJgIEiSGgNB\nkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEg\nSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRLQMRCS7E1yMclUkkPTtCfJ\nkdb+VJLbh41N8mdJvtn6fybJhsXZJEnSfNwyrEOSdcBR4A7gMnAuyWRVfb2v2z5grD12Aw8Du4eM\n/QLwUFXdSPIh4CHg/Yu3aVpJth96fEHjnz581yLNRNJMuuwh7AKmqupSVV0HTgETA30mgEer5yyw\nIcmm2cZW1d9X1Y02/iywZRG2R5I0T10CYTPwTF/5cqvr0qfLWIDfBj7fYS6SpCUy8i+Vk/wxcAP4\n2xna9yc5n+T8888/v7yTk6Q1pEsgXAG29pW3tLoufWYdm+RdwNuBd1ZVTffDq+p4VY1X1fjGjRs7\nTFeSNB9dAuEcMJZkR5L1wD3A5ECfSeC+drbRHuBaVV2dbWySvcD7gLur6sVF2h5J0jwNPcuonQV0\nEDgDrANOVNWFJAda+zHgNHAnMAW8CDww29j20h8BXgF8IQnA2ao6sJgbJ0nqbmggAFTVaXof+v11\nx/qeF/Bg17Gt/nVzmqkkaUmN/EtlSdLKYCBIkgADQZLUGAiSJKDjl8rSqHkvJGnpuYcgSQIMBElS\nYyBIkgC/Q9Aa4XcQ0nDuIUiSAPcQ1oyF/oUs6ebnHoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQY\nCJIkwECQJDVemCYtA2+dodXAQJA68EpvrQUeMpIkAe4hSKuCh5y0HNxDkCQBBoIkqTEQJEmAgSBJ\nagwESRJgIEiSGgNBkgR4HYK0Jngdg7owECQNZaCsDR4ykiQBBoIkqel0yCjJXuCvgHXA31TV4YH2\ntPY7gReBd1XVV2Ybm+RVwCeA7cDTwK9X1X8tfJMkrTQeclodhu4hJFkHHAX2ATuBe5PsHOi2Dxhr\nj/3Awx3GHgKeqKox4IlWliSNSJc9hF3AVFVdAkhyCpgAvt7XZwJ4tKoKOJtkQ5JN9P76n2nsBPDm\nNv4R4IvA+xe4PZJuQu5hLI8ugbAZeKavfBnY3aHP5iFjb6uqq+35s8BtHecsSXNioHSzIk47rapK\nUtO1JdlP7zAUwH8nudjX/ErgWsfyq4EXFmfGLzP4cxdrzGx9Zmqbrn5Y3WD7Uq3VUq3TsH4LWath\n5dW2VsP6LNVarerfv3yo8/iV+vv3k516VdWsD+AXgDN95YeAhwb6/DVwb1/5IrBptrEv9WnPNwEX\nh81lmrkd71oGzs/19ec7j8UaM1ufmdqmqx9WN826LclaLdU6LeVadSivqrUa1mep1srfv5nrlus9\n1eXR5bTTc8BYkh1J1gP3AJMDfSaB+9KzB7hWvcNBs42dBO5vz+8HPtdhLoP+bo7lpTKfn9NlzGx9\nZmqbrn5Y3Wpfp2H9FrJWa+k9NVu7a9WtbSX//g2Vlkizd0ruBP6S3qmjJ6rqg0kOAFTVsXba6UeA\nvfROO32gqs7PNLbV/zjwGLAN+Da9006/s8jb178N56tqfKle/2biWnXnWnXjOnU3yrXqFAg3gyT7\nq+r4qOexGrhW3blW3bhO3Y1yrdZMIEiSZuetKyRJgIEgSWoMBEkSsIYDIcmtSR5J8tEk7xz1fFay\nJK9N8rEknxr1XFayJO9o76dPJHnbqOezkiX56STHknwyye+Mej4rWfusOp/k7Uv9s26qQEhyIslz\nSb42UL83ycUkU0leuonerwGfqqr3AHcv+2RHbC5rVVWXqurdo5npaM1xnT7b3k8HgN8YxXxHaY5r\n9Y2qemmdfnUU8x2VOX5OQe8eb48tx9xuqkAATtK7FuL7Zrnj6hZ+cJ+l/1vGOa4UJ+m+VmvZSea+\nTn/S2teak8xhrZLcDZwGTi3vNEfuJB3XKckd9G4G+txyTOymCoSq+hIweHHb9+/WWlXX6b35Jujd\naG9L63NTrUMXc1yrNWsu69Su1P8Q8Plq/x/IWjLX91RVTVbVXn5wx4I1YY7r9GZgD/CbwHuSLOln\n1Yq4ud0Sm+mOq0eAjyS5ixV06fiITbtW7aryDwI/l+ShqvrTkcxu5ZjpPfVe4K3AK5O8rqqOjWJy\nK8xM76k30zts+8P0bn2/1k27TlV1ECDJu4AXqup7SzmJtRAI06qq/wEeGPU8VoOq+k96x8U1i6o6\nQu8PDQ1RVV/EIOisqk4ux89ZC4dKrgBb+8pbWp1ezrXqxnXqzrXqZkWs01oIhC53a1WPa9WN69Sd\na9XNilinmyoQknwc+DLwhiSXk7y7qm4AB4EzwDeAx6rqwijnuRK4Vt24Tt25Vt2s5HXy5naSJOAm\n20OQJM2fgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSc3/AwYGVcltx9/zAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fedab2b1470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MRR for the naive model\n",
    "\n",
    "ranks = []\n",
    "mrr = 0\n",
    "precision_at_10 = 0\n",
    "n = 500 # calculate only on first n users in validation set\n",
    "for i in range(n):\n",
    "    user_id, movie_id, rating = x_val[i]\n",
    "    if i % 50 == 0:\n",
    "        print('{}...'.format(i))\n",
    "    scores = total_watches\n",
    "    s = scores[movie_id] # the score for the correct movie\n",
    "    train_movies = x_train[x_train['user_id'] == user_id]['movie_id']\n",
    "    not_watched = (scores == scores) # all True\n",
    "    not_watched[train_movies] = False\n",
    "    higher_scores = (scores > s)    \n",
    "    rank = np.sum(higher_scores & not_watched) + 1\n",
    "    ranks.append(rank)\n",
    "    mrr += 1. / rank\n",
    "    if rank <= 10:\n",
    "        precision_at_10 += 1\n",
    "    print(rank)\n",
    "mrr /= n\n",
    "precision_at_10 /= n\n",
    "print(mrr)\n",
    "print(precision_at_10)\n",
    "plt.hist(ranks, bins=np.logspace(0., np.log10(ratings.id_assigner.get_next_id()) , 20), normed=1)\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "7\n",
      "9\n",
      "11\n",
      "13\n",
      "16\n",
      "21\n",
      "21\n",
      "23\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 11):\n",
    "    print(np.sum([r <= k for r in ranks]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fedab0ea4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ranks, bins=np.arange(1, 11), normed=1, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0152063492063\n"
     ]
    }
   ],
   "source": [
    "# MRR@10\n",
    "mrr10 = np.mean([(1./r if r <= 10 else 0) for r in ranks])\n",
    "print(mrr10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {5e-06: [(0.49899518, 0.88868439)]})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "# centered the rating data around 0\n",
    "# lambda = 5e-6\n",
    "# 10 epochs\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1e-05: [(0.94917321, 0.90415049)],\n",
       "             2e-05: [(1.0507183, 0.94094837)],\n",
       "             5e-05: [(1.1423631, 0.99971867)],\n",
       "             0.0001: [(1.2070471, 1.0341849)],\n",
       "             0.0002: [(1.2103031, 1.032499)],\n",
       "             0.0005: [(1.2117321, 1.031128)],\n",
       "             0.001: [(1.211888, 1.0307046)]})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "# added regularization with various lambda values\n",
    "# centered the rating data around 0\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0.0: [(0.37671992, 1.0658302)],\n",
       "             1e-06: [(0.52806127, 0.92396885)],\n",
       "             2e-06: [(0.61344689, 0.91350502)],\n",
       "             5e-06: [(0.79430443, 0.8886568)]})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "# added regularization with various lambda values\n",
    "# centered the rating data around 0\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1e-06: [(0.66761941, 0.94619572)],\n",
       "             5e-06: [(0.86379659, 0.98251671)]})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "# added regularization with various lambda values\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {2e-06: [(0.74221069, 0.94485837)],\n",
       "             2e-05: [(1.0187515, 1.1469429)]})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "# added regularization with various lambda values\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0.0: [(0.39927015, 1.0191419)],\n",
       "             0.0001: [(1.2529024, 1.5867586)],\n",
       "             0.0002: [(1.3900354, 1.8572755)],\n",
       "             0.0005: [(1.6396413, 2.2380252)],\n",
       "             0.001: [(1.9155307, 2.5586977)],\n",
       "             0.002: [(2.2482929, 2.8847487)],\n",
       "             0.005: [(2.6905251, 3.2657373)],\n",
       "             0.01: [(2.9629481, 3.4821424)],\n",
       "             0.02: [(3.1559427, 3.6294582)],\n",
       "             0.05: [(3.3028438, 3.7391768)],\n",
       "             0.1: [(3.3588696, 3.7807691)]})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "# added regularization with various lambda values\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {0: [(2.9629481, 3.4821424)]})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "# added regularization (0.01)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {0: [(3.3588698, 3.7807691)]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "# added regularization (0.1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {0: [(0.41014692, 1.0094724)]})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# all star ratings are considered watched\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
