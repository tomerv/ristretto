{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use BPR and \"SVD\" to predict user ranking/rating of each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import struct\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = '/home/tvromen/research/subtitles'\n",
    "\n",
    "class Flags(object):\n",
    "    def __init__(self):\n",
    "        # Data loading params\n",
    "        self.val_sample_percentage = .1 # Percentage of the training data to use for validation\n",
    "        self.ratings_file = os.path.join(basedir, 'ml-20m/ratings.csv') # Data source for the ratings\n",
    "        self.text_data_file = os.path.join(basedir, 'movielens-subtitles-1024.txt') # Data source\n",
    "\n",
    "        self.max_lines = 1000000\n",
    "\n",
    "        # Model Hyperparameters\n",
    "        self.embedding_dim = 64 # Dimensionality of user & movie vectors (default: 128)\n",
    "\n",
    "        #self.max_vocab_size = 100000\n",
    "        #self.vocab_embedding_dim = 300 # Dimensionality of character embedding (default: 128)\n",
    "        #self.filter_sizes = \"3,4,5\" # Comma-separated filter sizes (default: '3,4,5')\n",
    "        #self.num_filters = 128 # Number of filters per filter size (default: 128)\n",
    "        #self.dropout_keep_prob = 0.5 # Dropout keep probability (default: 0.5)\n",
    "\n",
    "        #self.words_in_scene = 64\n",
    "        #self.num_scenes = 16\n",
    "\n",
    "        # Training parameters\n",
    "        self.batch_size = 128 # Batch Size (default: 128)\n",
    "        self.num_epochs = 10 # Number of training epochs (default: 8)\n",
    "        self.summary_every = 100\n",
    "        self.evaluate_every = 1000 # Evaluate model on val set after this many steps (default: 100)\n",
    "        self.checkpoint_every = 2000 # Save model after this many steps (default: 100)\n",
    "        self.num_checkpoints = 3 # Number of checkpoints to store (default: 5)\n",
    "        # Misc Parameters\n",
    "        self.allow_soft_placement = True # Allow device soft device placement\n",
    "        self.log_device_placement = True # Log placement of ops on devices\n",
    "\n",
    "FLAGS = Flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data helpers\n",
    "\n",
    "class IdAssigner:\n",
    "    def __init__(self):\n",
    "        self.forward = dict()\n",
    "        self.reverse = dict()\n",
    "        self.next_id = 0\n",
    "    def get_id(self, x):\n",
    "        if x not in self.forward:\n",
    "            self.forward[x] = self.next_id\n",
    "            self.reverse[self.next_id] = x\n",
    "            self.next_id += 1\n",
    "        return self.forward[x]\n",
    "    def get_reverse_id(self, id_):\n",
    "        return self.reverse[id_]\n",
    "    def get_next_id(self):\n",
    "        return self.next_id\n",
    "\n",
    "class Subtitles:\n",
    "    \"\"\"\n",
    "    Class that is in charge of subtitles\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file):\n",
    "        samples = list(open(data_file, 'r').readlines())\n",
    "        samples = [s.strip() for s in samples]\n",
    "        ids = [int(s.split()[0]) for s in samples]\n",
    "        x_text = [' '.join(s.split()[1:]) for s in samples]\n",
    "        self.subs = dict()\n",
    "        for id_,txt in zip(ids, x_text):\n",
    "            self.subs[id_] = txt\n",
    "\n",
    "\n",
    "class RatingsData:\n",
    "    \"\"\"\n",
    "    Loads the ratings from the file. Returns an array x where each row is [user_id, movie_id]\n",
    "    The movie_id is not the original movie_id, but rather a new id which is allocated densely (no skips)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_file, max_lines, subs):\n",
    "        self.id_assigner = IdAssigner()\n",
    "        self.movies_per_user = defaultdict(list) # map user_id -> [movie_id_1, movie_id_2, ...]\n",
    "        self.movie_watch_count = defaultdict(int)   # number of times watched per new_movie_id\n",
    "        # load file\n",
    "        x = self._load_file(data_file, max_lines, subs)\n",
    "        x = self._sort_dataset(x)\n",
    "        x = x[['user_id', 'movie_id', 'rating']]\n",
    "        # split training/validation:\n",
    "        # the training set is all movies for each user except the last (in chronological order)\n",
    "        # validation set is the last movie for each user\n",
    "        user_ids = x['user_id']\n",
    "        is_last = (user_ids != np.append(user_ids[1:], -1))\n",
    "        self.train = x[~is_last]\n",
    "        self.val = x[is_last]\n",
    "\n",
    "    def _load_file(self, data_file, max_lines, subs):\n",
    "        print(\"Loading data...\")\n",
    "        x = np.zeros(\n",
    "            max_lines,\n",
    "            dtype=[('valid',np.bool), ('user_id',np.int32), ('movie_id', np.int32), ('rating', np.float32), ('timestamp', np.int32)]\n",
    "        )\n",
    "        with open(data_file) as f:\n",
    "            _ = f.readline() # skip first line\n",
    "            for i,line in enumerate(f.readlines()):\n",
    "                if i % 1000000 == 0:\n",
    "                    print('{}...'.format(i))\n",
    "                if i == max_lines:\n",
    "                    break\n",
    "                words = line.split(',')\n",
    "                user_id  = int(words[0])\n",
    "                movie_id = int(words[1])\n",
    "                rating   = float(words[2])\n",
    "                timestamp = int(words[3])\n",
    "                if movie_id not in subs.subs:\n",
    "                    # movie doesn't have subtitles\n",
    "                    continue\n",
    "                new_movie_id = self.id_assigner.get_id(movie_id)\n",
    "                x[i] = (True, user_id, new_movie_id, rating, timestamp)\n",
    "                self.movies_per_user[user_id].append(new_movie_id)\n",
    "                self.movie_watch_count[new_movie_id] += 1\n",
    "        valid = (x['valid'] != 0)\n",
    "        x = x[valid]\n",
    "        # center the ratings around 0\n",
    "        x['rating'] = x['rating'] - np.mean(x['rating'])\n",
    "        return x[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "\n",
    "    def _sort_dataset(self, x):\n",
    "        # sort by user and timestamp - use stable sorting algorithm\n",
    "        x = x[x['timestamp'].argsort(kind='mergesort')]  # secondary sort key\n",
    "        x = x[x['user_id'].argsort(kind='mergesort')]  # primary sort key\n",
    "        return x\n",
    "\n",
    "    def get_num_users(self):\n",
    "        return max(self.movies_per_user.keys()) + 1 # starts from 1\n",
    "\n",
    "    def get_num_movies(self):\n",
    "        return self.id_assigner.get_next_id()\n",
    "\n",
    "    def get_train(self, shuffle=True):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(len(self.train)))\n",
    "            return self.train[shuffle_indices]\n",
    "        else:\n",
    "            return self.train\n",
    "\n",
    "    def get_val(self, shuffle=True):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(len(self.val)))\n",
    "            return self.val[shuffle_indices]\n",
    "        else:\n",
    "            return self.val\n",
    "\n",
    "    ## TODO: generate negative with equal probability\n",
    "    ## TODO: this doesn't really use movie_id...\n",
    "    def _batch_iter_generate_neg(self, user_id, movie_id):\n",
    "        \"\"\"\n",
    "        Generates a batch res of same length as the inputs, such that for each tuple in (user_id, movie_id, res),\n",
    "        the item movie_id is ranked same or higher (by the user) than the corresponding item in res.\n",
    "        Also includes movies that haven't been watched. The movies are chosen according to popularity.\n",
    "        inputs:\n",
    "            user_id is array of length n\n",
    "            movie_id is array of length n\n",
    "        output:\n",
    "            res is array of length n\n",
    "        \"\"\"\n",
    "        n = len(user_id)\n",
    "        assert len(movie_id) == n, len(movie_id)\n",
    "        # count number of watches\n",
    "        num_movies = self.id_assigner.get_next_id()\n",
    "        watch_counts = np.array([self.movie_watch_count[i] for i in range(num_movies)], dtype=np.float32)\n",
    "        total_watches = np.sum(watch_counts)\n",
    "        assert(total_watches > 0)\n",
    "        res = np.zeros([n], dtype=np.int32)\n",
    "        # do it per user to speed up things\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            curr_user_id = user_id[i]\n",
    "            user_start_idx = i\n",
    "            while i < n and user_id[i] == curr_user_id:\n",
    "                i += 1\n",
    "                if i % 1000000 == 0:\n",
    "                    print('{}...'.format(i))\n",
    "            if i < n:\n",
    "                assert user_id[i] > curr_user_id, 'input needs to be sorted by user, otherwise this code it really inefficient'\n",
    "            user_end_idx = i # after-last\n",
    "            # update watch counts: subtract this user's watches\n",
    "            to_mask = np.zeros([num_movies], dtype=np.bool)\n",
    "            for j in self.movies_per_user[curr_user_id]:\n",
    "                to_mask[j] = True\n",
    "            total_masked = np.sum(watch_counts[to_mask])\n",
    "            p = (watch_counts * (1-to_mask)) / (total_watches - total_masked)\n",
    "            res[user_start_idx:user_end_idx] = \\\n",
    "                np.random.choice(num_movies, size=[user_end_idx-user_start_idx], p=p)\n",
    "            if False: # too slow\n",
    "                for j in range(user_start_idx, user_end_idx):\n",
    "                    assert res[j] not in self.movies_per_user[curr_user_id]\n",
    "        return res\n",
    "\n",
    "    def batch_iter(self, x, batch_size, num_epochs, shuffle=True):\n",
    "        \"\"\"\n",
    "        Generates the pair for each datapoint, and then\n",
    "        generates a batch iterator.\n",
    "        \"\"\"\n",
    "        assert type(x) == np.ndarray, type(x)\n",
    "        data_size = len(x)\n",
    "        # split to batches\n",
    "        num_batches_per_epoch = ((data_size - 1) // batch_size) + 1\n",
    "        for epoch in range(num_epochs):\n",
    "            # generate the pair\n",
    "            neg = self._batch_iter_generate_neg(x['user_id'], x['movie_id'])\n",
    "            # Shuffle the data\n",
    "            shuffle_indices = np.arange(data_size)\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "            x_shuffled = x[shuffle_indices]\n",
    "            neg = neg[shuffle_indices]\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                x_batch = x[start_index:end_index]\n",
    "                batch_neg = neg[start_index:end_index]\n",
    "                batch_user_id = x_batch['user_id']\n",
    "                batch_pos = x_batch['movie_id']\n",
    "                batch_pos_neg = np.stack([batch_pos,batch_neg], axis=-1)\n",
    "                batch_rating = x_batch['rating']\n",
    "                yield (batch_user_id, batch_pos_neg, batch_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "0...\n",
      "1000000...\n",
      "Train/Val split: 976945/6743\n",
      "Num users: 6744\n",
      "Num movies: 11920\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "subs = Subtitles(FLAGS.text_data_file)\n",
    "ratings = RatingsData(FLAGS.ratings_file, FLAGS.max_lines, subs)\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "x_train = ratings.get_train(shuffle=False)\n",
    "x_val = ratings.get_val(shuffle=False)\n",
    "\n",
    "print(\"Train/Val split: {:d}/{:d}\".format(len(x_train), len(x_val)))\n",
    "\n",
    "num_users = ratings.get_num_users()\n",
    "num_movies = ratings.get_num_movies()\n",
    "\n",
    "print('Num users: {}'.format(num_users))\n",
    "print('Num movies: {}'.format(num_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dynamic_tensor_shape(x):\n",
    "    \"\"\"\n",
    "    Calculate the tensor shape. Use a plain number where possible and a tensor elsewhere.\n",
    "    x is a tensor of some shape.\n",
    "    returns a list with the dimensions of x.\n",
    "    \"\"\"\n",
    "    shape_tensor = tf.shape(x)\n",
    "    shape = list(x.get_shape())\n",
    "    for i in range(len(shape)):\n",
    "        shape[i] = shape[i].value\n",
    "        if shape[i] is None:\n",
    "            # use tensor to represent the dimension\n",
    "            shape[i] = shape_tensor[i]\n",
    "    return shape\n",
    "\n",
    "\n",
    "def embedding_lookup_layer(x, vocab_size, embedding_dim, variable_scope, reuse=False):\n",
    "    \"\"\"\n",
    "    Lookup embedding\n",
    "    x is tensor of shape (d_1, d_2, ..., d_n) and type int32\n",
    "    result is tensor of shape (d_1, d_2, ..., d_n, embedding_dim) of n+1 dimensions and type DT_FLOAT\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(variable_scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            'W',\n",
    "            shape=[vocab_size, embedding_dim],\n",
    "            initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            regularizer=tf.contrib.layers.l2_regularizer(1.)\n",
    "        )\n",
    "    x_embedded = tf.nn.embedding_lookup(W, x)\n",
    "    return x_embedded\n",
    "\n",
    "def bias_lookup_layer(x, vocab_size, variable_scope, reuse=False):\n",
    "    \"\"\"\n",
    "    Lookup embedding\n",
    "    x is tensor of shape (d_1, d_2, ..., d_n) and type int32\n",
    "    result is tensor of same shape in x and type DT_FLOAT\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(variable_scope, reuse=reuse):\n",
    "        b = tf.get_variable(\n",
    "            'b',\n",
    "            shape=[vocab_size, 1],\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            regularizer=tf.contrib.layers.l2_regularizer(1.)\n",
    "        )\n",
    "    x_bias = tf.squeeze(tf.nn.embedding_lookup(b, x), -1)\n",
    "    return x_bias\n",
    "\n",
    "def fc_layer(x, output_size, variable_scope, reuse=False):\n",
    "    \"\"\"\n",
    "    Fully-connected layer\n",
    "    x has shape (batch_size, d_2)\n",
    "    result has shape (batch_size, output_size)\n",
    "    \"\"\"\n",
    "    shape = get_dynamic_tensor_shape(x)\n",
    "    assert len(shape) == 2\n",
    "    ## TODO: regularization\n",
    "    with tf.variable_scope(variable_scope, reuse=reuse):\n",
    "        W = tf.get_variable(\n",
    "            \"W\",\n",
    "            shape=[shape[1], output_size],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable(\n",
    "            \"b\",\n",
    "            shape=[output_size],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "    result = tf.nn.xw_plus_b(x, W, b, name=\"fc\")\n",
    "    return result\n",
    "\n",
    "\n",
    "class PredictionModel(object):\n",
    "    \"\"\"\n",
    "    A neural network for predicting per-user movie ratings.\n",
    "    The input to the network is the user_id and movie_id.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_movies, embedding_dim, l2_reg_lambda, loss_mult_ranking, loss_mult_rating):\n",
    "\n",
    "        assert num_users >= 1\n",
    "        self.num_users = num_users\n",
    "        assert num_movies >= 1\n",
    "        self.num_movies = num_movies\n",
    "        assert embedding_dim >= 1\n",
    "        self.embedding_dim = embedding_dim\n",
    "        assert l2_reg_lambda >= 0\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_user_id = tf.placeholder(tf.int32, [None], name=\"input_user_id\")\n",
    "        self.input_pos_neg = tf.placeholder(tf.int32, [None, 2], name=\"input_pos_neg\")\n",
    "        self.input_movie_id = tf.placeholder(tf.int32, [None], name=\"input_movie_id\")\n",
    "        self.input_rating = tf.placeholder(tf.float32, [None], name=\"input_rating\")\n",
    "        #self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\") # TODO: do we need dropout?\n",
    "\n",
    "        asrt1 = tf.assert_equal(tf.shape(self.input_user_id)[0], tf.shape(self.input_pos_neg)[0])\n",
    "        asrt2 = tf.assert_equal(tf.shape(self.input_user_id)[0], tf.shape(self.input_rating)[0])\n",
    "\n",
    "        # embedding lookup layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding_lookup'), tf.control_dependencies([asrt1]):\n",
    "            user_embedding = embedding_lookup_layer(self.input_user_id, self.num_users, self.embedding_dim, 'user_embedding')\n",
    "            user_bias = bias_lookup_layer(self.input_user_id, self.num_users, 'user_embedding')\n",
    "            movie_embedding = embedding_lookup_layer(self.input_pos_neg, self.num_movies, self.embedding_dim, 'movie_embedding')\n",
    "            movie_bias = bias_lookup_layer(self.input_pos_neg, self.num_movies, 'movie_embedding')\n",
    "            pos_embedding = movie_embedding[:,0,:]\n",
    "            neg_embedding = movie_embedding[:,1,:]\n",
    "            pos_bias = movie_bias[:,0]\n",
    "            neg_bias = movie_bias[:,1]\n",
    "\n",
    "        # ranking prediction (BPR)\n",
    "        with tf.name_scope('ranking_prediction'):\n",
    "            delta_embedding = pos_embedding - neg_embedding\n",
    "            delta_bias = pos_bias - neg_bias\n",
    "            self.ranking_prediction = tf.reduce_sum(user_embedding * delta_embedding, axis=1) + delta_bias\n",
    "\n",
    "        # rating prediction (\"SVD for recommender systems\")\n",
    "        with tf.name_scope('rating_prediction'):\n",
    "            self.rating_prediction = tf.reduce_sum(user_embedding * pos_embedding, axis=1) + user_bias + pos_bias\n",
    "\n",
    "        # Calculate loss\n",
    "        with tf.name_scope('ranking_loss'):\n",
    "            #losses = tf.log(tf.sigmoid(-self.ranking_prediction) + 0.01)\n",
    "            self.ranking_loss = tf.reduce_mean(tf.sigmoid(-self.ranking_prediction)) ## TODO use log of sigmoid\n",
    "\n",
    "        with tf.name_scope('rating_loss'):\n",
    "            self.rating_loss = tf.reduce_mean(tf.square(self.input_rating - self.rating_prediction))\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "            self.loss = \\\n",
    "                (loss_mult_ranking * self.ranking_loss) \\\n",
    "                + (loss_mult_rating * self.rating_loss) \\\n",
    "                + l2_reg_lambda * sum(reg_losses)\n",
    "\n",
    "        # Ranking accuracy\n",
    "        with tf.name_scope('ranking_accuracy'):\n",
    "            self.ranking_accuracy = tf.reduce_mean(tf.cast(self.ranking_prediction > 0, tf.float32))\n",
    "\n",
    "        # Rating accuracy = RMSE (for now...)\n",
    "        # (this is actually error rate, and not accuracy, i.e. lower is better)\n",
    "        with tf.name_scope('rating_accuracy'):\n",
    "            self.rating_accuracy = tf.sqrt(self.rating_loss)\n",
    "\n",
    "\n",
    "    def get_ranking_predictions(self):\n",
    "        asrt1 = tf.assert_equal(tf.shape(self.input_user_id)[0], 1)\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding_lookup'), tf.control_dependencies([asrt1]):\n",
    "            user_embedding = embedding_lookup_layer(self.input_user_id, self.num_users, self.embedding_dim, 'user_embedding', True)\n",
    "            movie_embedding = embedding_lookup_layer(self.input_movie_id, self.num_movies, self.embedding_dim, 'movie_embedding', True)\n",
    "            movie_bias = bias_lookup_layer(self.input_movie_id, self.num_movies, 'movie_embedding', True)\n",
    "        prediction = tf.reduce_sum(user_embedding * movie_embedding, axis=1) + movie_bias\n",
    "        return prediction\n",
    "\n",
    "    def get_rating_predictions(self):\n",
    "        asrt1 = tf.assert_equal(tf.shape(self.input_user_id)[0], 1)\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding_lookup'), tf.control_dependencies([asrt1]):\n",
    "            user_embedding = embedding_lookup_layer(self.input_user_id, self.num_users, self.embedding_dim, 'user_embedding', True)\n",
    "            user_bias = bias_lookup_layer(self.input_user_id, self.num_users, 'user_embedding', True)\n",
    "            movie_embedding = embedding_lookup_layer(self.input_movie_id, self.num_movies, self.embedding_dim, 'movie_embedding', True)\n",
    "            movie_bias = bias_lookup_layer(self.input_movie_id, self.num_movies, 'movie_embedding', True)\n",
    "        prediction = tf.reduce_sum(user_embedding * movie_embedding, axis=1) + user_bias + movie_bias\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "def train(\n",
    "    cnn, sess, starter_learning_rate, learning_rate_decay_every, learning_rate_decay_by\n",
    "):\n",
    "    last_accuracy = 0\n",
    "\n",
    "    # Define Training procedure\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    #optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        starter_learning_rate, global_step, learning_rate_decay_every,\n",
    "        learning_rate_decay_by, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "    # Keep track of gradient values and sparsity (optional)\n",
    "    grad_summaries = []\n",
    "    #for g, v in grads_and_vars:\n",
    "    for g,v in []:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    #grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    # Summaries for loss and accuracy\n",
    "    rating_loss_summary = tf.summary.scalar(\"rating_loss\", cnn.rating_loss)\n",
    "    loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", cnn.rating_accuracy)\n",
    "    learning_rate_summary = tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([rating_loss_summary, loss_summary, acc_summary, learning_rate_summary])#, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Val summaries\n",
    "    val_summary_op = tf.summary.merge([rating_loss_summary, loss_summary, acc_summary, learning_rate_summary])\n",
    "    val_summary_dir = os.path.join(out_dir, \"summaries\", \"val\")\n",
    "    val_summary_writer = tf.summary.FileWriter(val_summary_dir, sess.graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train_step(batch_user_id, batch_pos_neg, batch_rating):\n",
    "        \"\"\"\n",
    "        A single training step \n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            cnn.input_user_id: batch_user_id,\n",
    "            cnn.input_pos_neg: batch_pos_neg,\n",
    "            cnn.input_rating: batch_rating\n",
    "        }\n",
    "        pretrain_ranking_accuracy, pretrain_rating_accuracy = sess.run(\n",
    "            [cnn.ranking_accuracy, cnn.rating_accuracy],\n",
    "            feed_dict)\n",
    "        sess.run(train_op, feed_dict)\n",
    "        step, loss, ranking_accuracy, rating_accuracy, rate = sess.run(\n",
    "            [global_step, cnn.loss, cnn.ranking_accuracy, cnn.rating_accuracy, learning_rate],\n",
    "            feed_dict)\n",
    "        if step % FLAGS.summary_every == 0:\n",
    "            summaries = sess.run(train_summary_op, feed_dict)\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        if step % FLAGS.summary_every == 0:\n",
    "            print(\"{}: step {}, loss {:g}, ranking_acc {:g}->{:g}, rating_acc {:g}->{:g}, rate {:g}\".format(\n",
    "                time_str, step, loss, \n",
    "                pretrain_ranking_accuracy, ranking_accuracy,\n",
    "                pretrain_rating_accuracy, rating_accuracy,\n",
    "                rate)\n",
    "            )\n",
    "        return ranking_accuracy, rating_accuracy\n",
    "\n",
    "    def val_step(batch_user_id, batch_pos_neg, batch_rating, writer=None):\n",
    "        \"\"\"\n",
    "        Evaluates model on a val set\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            cnn.input_user_id: batch_user_id,\n",
    "            cnn.input_pos_neg: batch_pos_neg,\n",
    "            cnn.input_rating: batch_rating\n",
    "        }\n",
    "        step, summaries, loss, ranking_accuracy, rating_accuracy = sess.run(\n",
    "            [global_step, val_summary_op, cnn.loss, cnn.ranking_accuracy, cnn.rating_accuracy],\n",
    "            feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, ranking_acc {:g}, rating_acc {:g}\".format(\n",
    "            time_str, step, loss, ranking_accuracy, rating_accuracy))\n",
    "        if writer:\n",
    "            writer.add_summary(summaries, step)\n",
    "        return ranking_accuracy, rating_accuracy\n",
    "\n",
    "    # Generate batches\n",
    "    batches = ratings.batch_iter(x_train, FLAGS.batch_size, FLAGS.num_epochs)\n",
    "    # Training loop. For each batch...\n",
    "    last_test_rating_accuracy = None\n",
    "    for batch_user_id, batch_pos_neg, batch_rating in batches:\n",
    "        last_ranking_accuracy, last_rating_accuracy = train_step(batch_user_id, batch_pos_neg, batch_rating)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % FLAGS.evaluate_every == 0:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            ((val_user_id, val_pos_neg, val_rating),) = ratings.batch_iter(x_val, len(x_val), 1)\n",
    "            if len(x_val) > 1024:\n",
    "                val_user_id, val_pos_neg, val_rating = val_user_id[:1024], val_pos_neg[:1024], val_rating[:1024]\n",
    "            last_test_ranking_accuracy, last_test_rating_accuracy = \\\n",
    "                val_step(val_user_id, val_pos_neg, val_rating, writer=val_summary_writer)\n",
    "            print(\"\")\n",
    "        if current_step % FLAGS.checkpoint_every == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "            pass\n",
    "    return (last_ranking_accuracy, last_test_ranking_accuracy, last_rating_accuracy, last_test_rating_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_precision(prediction_getter):\n",
    "    ranks = []\n",
    "    mrr = 0\n",
    "    precision_at_10 = 0\n",
    "    mrr_at_10 = 0.\n",
    "    n = 50 # calculate only on first n users in validation set\n",
    "    for i in range(n):\n",
    "        user_id, movie_id, rating = x_val[i]\n",
    "        if i % 50 == 0:\n",
    "            print('{}...'.format(i))\n",
    "        scores = prediction_getter(user_id)\n",
    "        s = scores[movie_id] # the score for the correct movie\n",
    "        #print(s)\n",
    "        train_movies = x_train[x_train['user_id'] == user_id]['movie_id']\n",
    "        not_watched = (scores == scores) # all True\n",
    "        not_watched[train_movies] = False\n",
    "        higher_scores = (scores > s)    \n",
    "        rank = np.sum(higher_scores & not_watched) + 1\n",
    "        ranks.append(rank)\n",
    "        #print('for user_id {} the rank is {}'.format(user_id, rank))\n",
    "        mrr += 1. / rank\n",
    "        if rank <= 10:\n",
    "            precision_at_10 += 1\n",
    "            mrr_at_10 += 1. / rank\n",
    "    mrr /= n\n",
    "    precision_at_10 /= n\n",
    "    mrr_at_10 /= n\n",
    "    print('MRR is {}'.format(mrr))\n",
    "    print('Precision@10 is {}'.format(precision_at_10))\n",
    "    print('MRR@10 is {}'.format(mrr_at_10))\n",
    "    plt.hist(ranks, bins=np.logspace(0., np.log10(ratings.id_assigner.get_next_id()) , 20), normed=1)\n",
    "    plt.gca().set_xscale(\"log\")\n",
    "    plt.show()\n",
    "    return mrr, precision_at_10, mrr_at_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runall():\n",
    "    res = defaultdict(lambda : defaultdict(list))\n",
    "    with open('results.txt', 'a') as f:\n",
    "        for l2_reg_lambda in [1e-6]:\n",
    "            with tf.Graph().as_default():\n",
    "                session_conf = tf.ConfigProto(\n",
    "                    allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "                    log_device_placement=FLAGS.log_device_placement)\n",
    "                session_conf.gpu_options.allow_growth=True\n",
    "                sess = tf.Session(config=session_conf)\n",
    "                with sess.as_default():\n",
    "                    model = PredictionModel(\n",
    "                        num_users=num_users,\n",
    "                        num_movies=num_movies,\n",
    "                        embedding_dim=FLAGS.embedding_dim,\n",
    "                        l2_reg_lambda=l2_reg_lambda,\n",
    "                        loss_mult_ranking=0.,\n",
    "                        loss_mult_rating=1.)\n",
    "                    for i in range(1):\n",
    "                        f.write('lambda: {}\\n'.format(l2_reg_lambda))\n",
    "                        last_accuracy = train(model, sess, 3e-3, 20000, 0.5)\n",
    "                        f.write('accuracy: {}\\n'.format(last_accuracy))\n",
    "                        f.flush()\n",
    "                        res[l2_reg_lambda]['accuracy'].append(last_accuracy)\n",
    "                        def ranking_prediction_getter(user_id):\n",
    "                            num_movies = ratings.id_assigner.get_next_id()\n",
    "                            batch_movie_id = np.arange(num_movies)\n",
    "                            feed_dict = {\n",
    "                                model.input_user_id: [user_id],\n",
    "                                model.input_movie_id: batch_movie_id,\n",
    "                            }\n",
    "                            scores = sess.run(model.get_ranking_predictions(), feed_dict=feed_dict)\n",
    "                            return scores\n",
    "                        mrr, precision_at_10, mrr_at_10 = calc_precision(ranking_prediction_getter)\n",
    "                        f.write(repr((mrr, precision_at_10, mrr_at_10)) + '\\n')\n",
    "                        f.write('\\n')\n",
    "                        f.flush()\n",
    "                        res[l2_reg_lambda]['mrr'].append(mrr)\n",
    "                        res[l2_reg_lambda]['precision_at_10'].append(precision_at_10)\n",
    "                        res[l2_reg_lambda]['mrr_at_10'].append(mrr_at_10)\n",
    "                        def rating_prediction_getter(user_id):\n",
    "                            num_movies = ratings.id_assigner.get_next_id()\n",
    "                            batch_movie_id = np.arange(num_movies)\n",
    "                            feed_dict = {\n",
    "                                model.input_user_id: [user_id],\n",
    "                                model.input_movie_id: batch_movie_id,\n",
    "                            }\n",
    "                            scores = sess.run(model.get_rating_predictions(), feed_dict=feed_dict)\n",
    "                            return scores\n",
    "                        mrr, precision_at_10, mrr_at_10 = calc_precision(rating_prediction_getter)\n",
    "                        f.write(repr((mrr, precision_at_10, mrr_at_10)) + '\\n')\n",
    "                        f.write('\\n')\n",
    "                        f.flush()\n",
    "                        res[l2_reg_lambda]['mrr'].append(mrr)\n",
    "                        res[l2_reg_lambda]['precision_at_10'].append(precision_at_10)\n",
    "                        res[l2_reg_lambda]['mrr_at_10'].append(mrr_at_10)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/tvromen/research/subtitles2/runs/1510492085\n",
      "\n",
      "2017-11-12T15:08:09.637226: step 100, loss 0.907285, ranking_acc 0.46875->0.5, rating_acc 0.955856->0.952484, rate 0.003\n",
      "2017-11-12T15:08:11.104678: step 200, loss 0.367804, ranking_acc 0.515625->0.523438, rating_acc 0.610722->0.606319, rate 0.003\n",
      "2017-11-12T15:08:12.577103: step 300, loss 2.62112, ranking_acc 0.3125->0.304688, rating_acc 1.63397->1.61889, rate 0.003\n",
      "2017-11-12T15:08:14.019162: step 400, loss 0.973991, ranking_acc 0.46875->0.46875, rating_acc 0.995346->0.986642, rate 0.003\n",
      "2017-11-12T15:08:15.476635: step 500, loss 1.25383, ranking_acc 0.328125->0.328125, rating_acc 1.12917->1.11938, rate 0.003\n",
      "2017-11-12T15:08:16.918560: step 600, loss 0.889815, ranking_acc 0.554688->0.546875, rating_acc 0.949394->0.942796, rate 0.003\n",
      "2017-11-12T15:08:18.303005: step 700, loss 0.874668, ranking_acc 0.421875->0.460938, rating_acc 0.949376->0.934564, rate 0.003\n",
      "2017-11-12T15:08:19.777040: step 800, loss 0.886087, ranking_acc 0.476562->0.492188, rating_acc 0.948533->0.940479, rate 0.003\n",
      "2017-11-12T15:08:21.154279: step 900, loss 0.810291, ranking_acc 0.703125->0.6875, rating_acc 0.918992->0.898973, rate 0.003\n",
      "2017-11-12T15:08:22.541279: step 1000, loss 0.859118, ranking_acc 0.539062->0.539062, rating_acc 0.939423->0.925609, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:08:24.469651: step 1000, loss 0.888306, ranking_acc 0.517578, rating_acc 0.941243\n",
      "\n",
      "2017-11-12T15:08:25.953540: step 1100, loss 0.536669, ranking_acc 0.492188->0.484375, rating_acc 0.736305->0.730664, rate 0.003\n",
      "2017-11-12T15:08:27.416021: step 1200, loss 0.867561, ranking_acc 0.445312->0.4375, rating_acc 0.960078->0.929836, rate 0.003\n",
      "2017-11-12T15:08:28.876928: step 1300, loss 1.00953, ranking_acc 0.703125->0.742188, rating_acc 1.06001->1.0032, rate 0.003\n",
      "2017-11-12T15:08:30.373787: step 1400, loss 2.32022, ranking_acc 0.625->0.640625, rating_acc 1.55115->1.52216, rate 0.003\n",
      "2017-11-12T15:08:31.843676: step 1500, loss 0.627951, ranking_acc 0.625->0.632812, rating_acc 0.805722->0.790309, rate 0.003\n",
      "2017-11-12T15:08:33.299212: step 1600, loss 0.832869, ranking_acc 0.539062->0.570312, rating_acc 0.919207->0.910679, rate 0.003\n",
      "2017-11-12T15:08:34.779891: step 1700, loss 0.800126, ranking_acc 0.375->0.398438, rating_acc 0.937635->0.892326, rate 0.003\n",
      "2017-11-12T15:08:36.259962: step 1800, loss 0.602304, ranking_acc 0.632812->0.648438, rating_acc 0.791512->0.773417, rate 0.003\n",
      "2017-11-12T15:08:37.735755: step 1900, loss 1.78189, ranking_acc 0.265625->0.265625, rating_acc 1.41991->1.33324, rate 0.003\n",
      "2017-11-12T15:08:39.224049: step 2000, loss 0.453696, ranking_acc 0.59375->0.609375, rating_acc 0.683245->0.670105, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:08:41.168407: step 2000, loss 0.958722, ranking_acc 0.547852, rating_acc 0.976764\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-2000\n",
      "\n",
      "2017-11-12T15:08:42.689733: step 2100, loss 0.981076, ranking_acc 0.453125->0.429688, rating_acc 1.05967->0.988102, rate 0.003\n",
      "2017-11-12T15:08:44.118384: step 2200, loss 0.539368, ranking_acc 0.796875->0.789062, rating_acc 0.777684->0.73111, rate 0.003\n",
      "2017-11-12T15:08:45.521398: step 2300, loss 0.415877, ranking_acc 0.585938->0.59375, rating_acc 0.651614->0.641045, rate 0.003\n",
      "2017-11-12T15:08:46.988634: step 2400, loss 0.496918, ranking_acc 0.398438->0.382812, rating_acc 0.760119->0.701244, rate 0.003\n",
      "2017-11-12T15:08:48.391463: step 2500, loss 1.4948, ranking_acc 0.554688->0.539062, rating_acc 1.27058->1.22046, rate 0.003\n",
      "2017-11-12T15:08:49.796036: step 2600, loss 0.911516, ranking_acc 0.710938->0.742188, rating_acc 0.960999->0.95185, rate 0.003\n",
      "2017-11-12T15:08:51.248985: step 2700, loss 0.519456, ranking_acc 0.53125->0.523438, rating_acc 0.722001->0.716763, rate 0.003\n",
      "2017-11-12T15:08:52.700670: step 2800, loss 0.691327, ranking_acc 0.5->0.492188, rating_acc 0.825601->0.827931, rate 0.003\n",
      "2017-11-12T15:08:54.176038: step 2900, loss 2.05186, ranking_acc 0.570312->0.5625, rating_acc 1.4385->1.43032, rate 0.003\n",
      "2017-11-12T15:08:55.619627: step 3000, loss 0.337105, ranking_acc 0.625->0.632812, rating_acc 0.643725->0.575212, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:08:57.537897: step 3000, loss 1.02841, ranking_acc 0.520508, rating_acc 1.01102\n",
      "\n",
      "2017-11-12T15:08:58.957640: step 3100, loss 0.692386, ranking_acc 0.5->0.5, rating_acc 0.881738->0.828247, rate 0.003\n",
      "2017-11-12T15:09:00.199950: step 3200, loss 1.38138, ranking_acc 0.320312->0.3125, rating_acc 1.20746->1.17254, rate 0.003\n",
      "2017-11-12T15:09:01.613317: step 3300, loss 0.623515, ranking_acc 0.492188->0.492188, rating_acc 0.823708->0.785368, rate 0.003\n",
      "2017-11-12T15:09:03.102345: step 3400, loss 0.654853, ranking_acc 0.4375->0.398438, rating_acc 0.888894->0.805025, rate 0.003\n",
      "2017-11-12T15:09:04.564741: step 3500, loss 0.778742, ranking_acc 0.578125->0.5625, rating_acc 0.919009->0.878492, rate 0.003\n",
      "2017-11-12T15:09:05.934907: step 3600, loss 0.584931, ranking_acc 0.429688->0.4375, rating_acc 0.769157->0.760208, rate 0.003\n",
      "2017-11-12T15:09:07.365970: step 3700, loss 1.138, ranking_acc 0.375->0.351562, rating_acc 1.07384->1.06345, rate 0.003\n",
      "2017-11-12T15:09:08.782000: step 3800, loss 0.589311, ranking_acc 0.445312->0.460938, rating_acc 0.761748->0.763003, rate 0.003\n",
      "2017-11-12T15:09:10.201195: step 3900, loss 1.31353, ranking_acc 0.578125->0.601562, rating_acc 1.19288->1.14288, rate 0.003\n",
      "2017-11-12T15:09:11.638933: step 4000, loss 0.553205, ranking_acc 0.242188->0.25, rating_acc 0.779181->0.738765, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:09:13.559538: step 4000, loss 1.05891, ranking_acc 0.525391, rating_acc 1.02542\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-4000\n",
      "\n",
      "2017-11-12T15:09:15.077674: step 4100, loss 0.510576, ranking_acc 0.5625->0.5625, rating_acc 0.729763->0.709287, rate 0.003\n",
      "2017-11-12T15:09:16.517648: step 4200, loss 0.84851, ranking_acc 0.367188->0.34375, rating_acc 0.981528->0.91699, rate 0.003\n",
      "2017-11-12T15:09:17.918084: step 4300, loss 1.29292, ranking_acc 0.554688->0.554688, rating_acc 1.15832->1.13367, rate 0.003\n",
      "2017-11-12T15:09:19.303452: step 4400, loss 0.713623, ranking_acc 0.773438->0.78125, rating_acc 0.864565->0.839984, rate 0.003\n",
      "2017-11-12T15:09:20.751239: step 4500, loss 0.483993, ranking_acc 0.65625->0.640625, rating_acc 0.678258->0.689758, rate 0.003\n",
      "2017-11-12T15:09:22.180184: step 4600, loss 0.739373, ranking_acc 0.4375->0.4375, rating_acc 0.858068->0.855002, rate 0.003\n",
      "2017-11-12T15:09:23.644999: step 4700, loss 0.385793, ranking_acc 0.375->0.382812, rating_acc 0.646819->0.614325, rate 0.003\n",
      "2017-11-12T15:09:25.057584: step 4800, loss 2.52839, ranking_acc 0.5625->0.585938, rating_acc 1.63444->1.58741, rate 0.003\n",
      "2017-11-12T15:09:26.512340: step 4900, loss 1.05686, ranking_acc 0.398438->0.40625, rating_acc 1.05968->1.02373, rate 0.003\n",
      "2017-11-12T15:09:27.943471: step 5000, loss 1.86004, ranking_acc 0.34375->0.335938, rating_acc 1.44225->1.36056, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:09:30.031919: step 5000, loss 1.06959, ranking_acc 0.530273, rating_acc 1.0299\n",
      "\n",
      "2017-11-12T15:09:31.469075: step 5100, loss 0.710017, ranking_acc 0.421875->0.421875, rating_acc 0.856272->0.83728, rate 0.003\n",
      "2017-11-12T15:09:32.897228: step 5200, loss 0.526388, ranking_acc 0.46875->0.515625, rating_acc 0.743736->0.719245, rate 0.003\n",
      "2017-11-12T15:09:34.330508: step 5300, loss 0.5244, ranking_acc 0.5->0.492188, rating_acc 0.728255->0.717877, rate 0.003\n",
      "2017-11-12T15:09:35.761687: step 5400, loss 1.04535, ranking_acc 0.640625->0.648438, rating_acc 1.04441->1.01798, rate 0.003\n",
      "2017-11-12T15:09:37.209538: step 5500, loss 0.689755, ranking_acc 0.484375->0.492188, rating_acc 0.878313->0.825064, rate 0.003\n",
      "2017-11-12T15:09:38.650073: step 5600, loss 1.34918, ranking_acc 0.546875->0.539062, rating_acc 1.27166->1.15761, rate 0.003\n",
      "2017-11-12T15:09:40.054116: step 5700, loss 0.582931, ranking_acc 0.515625->0.539062, rating_acc 0.798861->0.757513, rate 0.003\n",
      "2017-11-12T15:09:41.467257: step 5800, loss 1.28413, ranking_acc 0.609375->0.625, rating_acc 1.14763->1.12908, rate 0.003\n",
      "2017-11-12T15:09:42.906553: step 5900, loss 1.00234, ranking_acc 0.757812->0.75, rating_acc 1.01056->0.996441, rate 0.003\n",
      "2017-11-12T15:09:44.390801: step 6000, loss 1.50307, ranking_acc 0.375->0.367188, rating_acc 1.28739->1.22214, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:09:46.306133: step 6000, loss 1.07491, ranking_acc 0.52832, rating_acc 1.03222\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-6000\n",
      "\n",
      "2017-11-12T15:09:47.791847: step 6100, loss 0.575368, ranking_acc 0.617188->0.609375, rating_acc 0.805025->0.752293, rate 0.003\n",
      "2017-11-12T15:09:49.255891: step 6200, loss 0.387512, ranking_acc 0.585938->0.585938, rating_acc 0.614093->0.614757, rate 0.003\n",
      "2017-11-12T15:09:50.614226: step 6300, loss 0.488793, ranking_acc 0.515625->0.523438, rating_acc 0.697869->0.692221, rate 0.003\n",
      "2017-11-12T15:09:52.111988: step 6400, loss 0.72889, ranking_acc 0.3125->0.382812, rating_acc 0.922268->0.847994, rate 0.003\n",
      "2017-11-12T15:09:53.606444: step 6500, loss 0.720615, ranking_acc 0.59375->0.578125, rating_acc 0.882952->0.842996, rate 0.003\n",
      "2017-11-12T15:09:55.089092: step 6600, loss 0.6752, ranking_acc 0.578125->0.59375, rating_acc 0.865259->0.815494, rate 0.003\n",
      "2017-11-12T15:09:56.563233: step 6700, loss 0.636621, ranking_acc 0.523438->0.507812, rating_acc 0.927459->0.791488, rate 0.003\n",
      "2017-11-12T15:09:58.052000: step 6800, loss 1.34435, ranking_acc 0.390625->0.429688, rating_acc 1.26584->1.15504, rate 0.003\n",
      "2017-11-12T15:09:59.526709: step 6900, loss 0.593312, ranking_acc 0.539062->0.53125, rating_acc 0.802996->0.763538, rate 0.003\n",
      "2017-11-12T15:10:00.999352: step 7000, loss 1.14032, ranking_acc 0.617188->0.625, rating_acc 1.08676->1.06301, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:10:02.914125: step 7000, loss 1.07906, ranking_acc 0.521484, rating_acc 1.0338\n",
      "\n",
      "2017-11-12T15:10:04.425129: step 7100, loss 1.29463, ranking_acc 0.53125->0.546875, rating_acc 1.19953->1.13325, rate 0.003\n",
      "2017-11-12T15:10:05.871987: step 7200, loss 1.08989, ranking_acc 0.554688->0.585938, rating_acc 1.0569->1.03902, rate 0.003\n",
      "2017-11-12T15:10:07.349368: step 7300, loss 0.630775, ranking_acc 0.773438->0.757812, rating_acc 0.801217->0.787635, rate 0.003\n",
      "2017-11-12T15:10:08.837372: step 7400, loss 0.450252, ranking_acc 0.476562->0.4375, rating_acc 0.711713->0.663159, rate 0.003\n",
      "2017-11-12T15:10:10.315906: step 7500, loss 0.55164, ranking_acc 0.429688->0.40625, rating_acc 0.746123->0.735565, rate 0.003\n",
      "2017-11-12T15:10:11.767386: step 7600, loss 1.36418, ranking_acc 0.59375->0.601562, rating_acc 1.1861->1.16348, rate 0.003\n",
      "2017-11-12T15:10:15.597970: step 7700, loss 0.882259, ranking_acc 0.578125->0.59375, rating_acc 0.993256->0.933581, rate 0.003\n",
      "2017-11-12T15:10:17.052286: step 7800, loss 0.525765, ranking_acc 0.398438->0.40625, rating_acc 0.722481->0.717597, rate 0.003\n",
      "2017-11-12T15:10:18.505679: step 7900, loss 0.778305, ranking_acc 0.65625->0.664062, rating_acc 0.880196->0.876094, rate 0.003\n",
      "2017-11-12T15:10:20.005172: step 8000, loss 0.393375, ranking_acc 0.640625->0.664062, rating_acc 0.618357->0.618919, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:10:21.915664: step 8000, loss 1.26706, ranking_acc 0.533203, rating_acc 1.12105\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-8000\n",
      "\n",
      "2017-11-12T15:10:23.458604: step 8100, loss 1.63607, ranking_acc 0.4375->0.46875, rating_acc 1.3199->1.27524, rate 0.003\n",
      "2017-11-12T15:10:24.907100: step 8200, loss 1.41547, ranking_acc 0.460938->0.46875, rating_acc 1.22262->1.18573, rate 0.003\n",
      "2017-11-12T15:10:26.390880: step 8300, loss 0.581337, ranking_acc 0.460938->0.46875, rating_acc 0.762613->0.756408, rate 0.003\n",
      "2017-11-12T15:10:27.836308: step 8400, loss 0.8251, ranking_acc 0.21875->0.210938, rating_acc 0.941103->0.903469, rate 0.003\n",
      "2017-11-12T15:10:29.290687: step 8500, loss 0.66708, ranking_acc 0.703125->0.703125, rating_acc 0.861953->0.811244, rate 0.003\n",
      "2017-11-12T15:10:30.771218: step 8600, loss 1.32395, ranking_acc 0.390625->0.390625, rating_acc 1.15924->1.1468, rate 0.003\n",
      "2017-11-12T15:10:32.247875: step 8700, loss 0.73048, ranking_acc 0.617188->0.617188, rating_acc 0.868807->0.849569, rate 0.003\n",
      "2017-11-12T15:10:33.713792: step 8800, loss 0.723346, ranking_acc 0.414062->0.429688, rating_acc 0.877474->0.845501, rate 0.003\n",
      "2017-11-12T15:10:35.164268: step 8900, loss 0.95858, ranking_acc 0.539062->0.523438, rating_acc 0.996734->0.974829, rate 0.003\n",
      "2017-11-12T15:10:36.638183: step 9000, loss 0.936438, ranking_acc 0.585938->0.617188, rating_acc 0.984585->0.963507, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:10:38.556431: step 9000, loss 0.982501, ranking_acc 0.522461, rating_acc 0.987122\n",
      "\n",
      "2017-11-12T15:10:39.984686: step 9100, loss 0.477509, ranking_acc 0.523438->0.515625, rating_acc 0.701863->0.685214, rate 0.003\n",
      "2017-11-12T15:10:41.440445: step 9200, loss 0.372729, ranking_acc 0.523438->0.53125, rating_acc 0.629909->0.603982, rate 0.003\n",
      "2017-11-12T15:10:42.884233: step 9300, loss 0.425708, ranking_acc 0.5625->0.5625, rating_acc 0.67296->0.646415, rate 0.003\n",
      "2017-11-12T15:10:44.336816: step 9400, loss 0.607659, ranking_acc 0.601562->0.617188, rating_acc 0.804214->0.774533, rate 0.003\n",
      "2017-11-12T15:10:45.806718: step 9500, loss 0.867987, ranking_acc 0.585938->0.578125, rating_acc 0.949639->0.927533, rate 0.003\n",
      "2017-11-12T15:10:47.304577: step 9600, loss 0.882769, ranking_acc 0.5625->0.5625, rating_acc 0.948783->0.935468, rate 0.003\n",
      "2017-11-12T15:10:48.762542: step 9700, loss 0.662537, ranking_acc 0.648438->0.65625, rating_acc 0.831058->0.809282, rate 0.003\n",
      "2017-11-12T15:10:50.205990: step 9800, loss 0.671679, ranking_acc 0.609375->0.601562, rating_acc 0.836581->0.814952, rate 0.003\n",
      "2017-11-12T15:10:51.654459: step 9900, loss 0.659518, ranking_acc 0.507812->0.507812, rating_acc 0.832513->0.807483, rate 0.003\n",
      "2017-11-12T15:10:53.104659: step 10000, loss 1.07744, ranking_acc 0.351562->0.351562, rating_acc 1.07259->1.03438, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:10:55.017496: step 10000, loss 0.942622, ranking_acc 0.524414, rating_acc 0.967014\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-10000\n",
      "\n",
      "2017-11-12T15:10:56.549389: step 10100, loss 0.684081, ranking_acc 0.640625->0.648438, rating_acc 0.841905->0.822589, rate 0.003\n",
      "2017-11-12T15:10:58.003879: step 10200, loss 0.494138, ranking_acc 0.695312->0.671875, rating_acc 0.747423->0.697657, rate 0.003\n",
      "2017-11-12T15:10:59.450164: step 10300, loss 1.32731, ranking_acc 0.34375->0.34375, rating_acc 1.17226->1.14886, rate 0.003\n",
      "2017-11-12T15:11:00.903262: step 10400, loss 0.738864, ranking_acc 0.484375->0.476562, rating_acc 0.891013->0.855242, rate 0.003\n",
      "2017-11-12T15:11:02.360679: step 10500, loss 0.938638, ranking_acc 0.648438->0.664062, rating_acc 0.987555->0.965019, rate 0.003\n",
      "2017-11-12T15:11:03.850966: step 10600, loss 0.276262, ranking_acc 0.460938->0.460938, rating_acc 0.52008->0.518552, rate 0.003\n",
      "2017-11-12T15:11:05.283571: step 10700, loss 0.566048, ranking_acc 0.570312->0.578125, rating_acc 0.800349->0.747442, rate 0.003\n",
      "2017-11-12T15:11:06.776000: step 10800, loss 0.531123, ranking_acc 0.445312->0.460938, rating_acc 0.731878->0.723717, rate 0.003\n",
      "2017-11-12T15:11:08.225167: step 10900, loss 0.409593, ranking_acc 0.570312->0.585938, rating_acc 0.649454->0.634213, rate 0.003\n",
      "2017-11-12T15:11:09.695160: step 11000, loss 0.49163, ranking_acc 0.585938->0.601562, rating_acc 0.716969->0.695859, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:11:11.607540: step 11000, loss 0.923499, ranking_acc 0.530273, rating_acc 0.957126\n",
      "\n",
      "2017-11-12T15:11:13.125533: step 11100, loss 0.646112, ranking_acc 0.71875->0.726562, rating_acc 0.820423->0.799151, rate 0.003\n",
      "2017-11-12T15:11:14.590224: step 11200, loss 0.877053, ranking_acc 0.484375->0.515625, rating_acc 0.952298->0.932521, rate 0.003\n",
      "2017-11-12T15:11:16.017369: step 11300, loss 1.15395, ranking_acc 0.375->0.375, rating_acc 1.10579->1.07078, rate 0.003\n",
      "2017-11-12T15:11:17.504442: step 11400, loss 0.791402, ranking_acc 0.273438->0.28125, rating_acc 0.90207->0.885428, rate 0.003\n",
      "2017-11-12T15:11:18.967219: step 11500, loss 1.22899, ranking_acc 0.515625->0.523438, rating_acc 1.13003->1.1052, rate 0.003\n",
      "2017-11-12T15:11:20.430314: step 11600, loss 0.395271, ranking_acc 0.453125->0.4375, rating_acc 0.629954->0.622719, rate 0.003\n",
      "2017-11-12T15:11:21.924916: step 11700, loss 0.837021, ranking_acc 0.210938->0.226562, rating_acc 0.92779->0.910775, rate 0.003\n",
      "2017-11-12T15:11:23.384182: step 11800, loss 1.19713, ranking_acc 0.65625->0.65625, rating_acc 1.11632->1.09066, rate 0.003\n",
      "2017-11-12T15:11:24.833605: step 11900, loss 0.984724, ranking_acc 0.546875->0.554688, rating_acc 1.02057->0.988466, rate 0.003\n",
      "2017-11-12T15:11:26.315059: step 12000, loss 0.779514, ranking_acc 0.398438->0.414062, rating_acc 0.893862->0.878517, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:11:28.239961: step 12000, loss 0.923047, ranking_acc 0.539062, rating_acc 0.956726\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-12000\n",
      "\n",
      "2017-11-12T15:11:29.777761: step 12100, loss 0.347854, ranking_acc 0.632812->0.65625, rating_acc 0.611544->0.583054, rate 0.003\n",
      "2017-11-12T15:11:31.214144: step 12200, loss 0.63877, ranking_acc 0.5625->0.570312, rating_acc 0.814425->0.794248, rate 0.003\n",
      "2017-11-12T15:11:32.681274: step 12300, loss 0.300596, ranking_acc 0.4375->0.4375, rating_acc 0.55941->0.540969, rate 0.003\n",
      "2017-11-12T15:11:34.133123: step 12400, loss 0.803289, ranking_acc 0.5->0.5, rating_acc 0.925927->0.891816, rate 0.003\n",
      "2017-11-12T15:11:35.601561: step 12500, loss 0.593359, ranking_acc 0.640625->0.640625, rating_acc 0.801647->0.765071, rate 0.003\n",
      "2017-11-12T15:11:37.041570: step 12600, loss 0.576059, ranking_acc 0.375->0.375, rating_acc 0.775734->0.753669, rate 0.003\n",
      "2017-11-12T15:11:38.499472: step 12700, loss 0.482477, ranking_acc 0.453125->0.4375, rating_acc 0.709486->0.688773, rate 0.003\n",
      "2017-11-12T15:11:39.985024: step 12800, loss 0.619128, ranking_acc 0.585938->0.601562, rating_acc 0.793521->0.781686, rate 0.003\n",
      "2017-11-12T15:11:41.436082: step 12900, loss 0.659536, ranking_acc 0.445312->0.453125, rating_acc 0.824374->0.80711, rate 0.003\n",
      "2017-11-12T15:11:42.909695: step 13000, loss 0.767387, ranking_acc 0.460938->0.46875, rating_acc 0.89532->0.871373, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:11:44.821194: step 13000, loss 0.914917, ranking_acc 0.532227, rating_acc 0.952271\n",
      "\n",
      "2017-11-12T15:11:46.259939: step 13100, loss 0.701777, ranking_acc 0.476562->0.492188, rating_acc 0.862588->0.832894, rate 0.003\n",
      "2017-11-12T15:11:47.730944: step 13200, loss 0.552808, ranking_acc 0.546875->0.5625, rating_acc 0.763194->0.738077, rate 0.003\n",
      "2017-11-12T15:11:49.185685: step 13300, loss 0.513573, ranking_acc 0.453125->0.453125, rating_acc 0.733468->0.710998, rate 0.003\n",
      "2017-11-12T15:11:50.637515: step 13400, loss 0.797406, ranking_acc 0.617188->0.609375, rating_acc 0.95095->0.888473, rate 0.003\n",
      "2017-11-12T15:11:52.119307: step 13500, loss 0.549351, ranking_acc 0.59375->0.648438, rating_acc 0.748154->0.735752, rate 0.003\n",
      "2017-11-12T15:11:53.607970: step 13600, loss 1.08853, ranking_acc 0.617188->0.609375, rating_acc 1.07103->1.03949, rate 0.003\n",
      "2017-11-12T15:11:55.070472: step 13700, loss 0.713268, ranking_acc 0.40625->0.398438, rating_acc 0.863295->0.839807, rate 0.003\n",
      "2017-11-12T15:11:56.550712: step 13800, loss 0.653573, ranking_acc 0.515625->0.523438, rating_acc 0.814029->0.803451, rate 0.003\n",
      "2017-11-12T15:11:58.046093: step 13900, loss 0.649894, ranking_acc 0.53125->0.539062, rating_acc 0.81621->0.801148, rate 0.003\n",
      "2017-11-12T15:11:59.500776: step 14000, loss 0.508498, ranking_acc 0.625->0.632812, rating_acc 0.731526->0.707428, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:12:01.419937: step 14000, loss 0.923072, ranking_acc 0.546875, rating_acc 0.956571\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-14000\n",
      "\n",
      "2017-11-12T15:12:02.945416: step 14100, loss 0.5465, ranking_acc 0.523438->0.539062, rating_acc 0.768824->0.733777, rate 0.003\n",
      "2017-11-12T15:12:04.387463: step 14200, loss 0.740373, ranking_acc 0.6875->0.671875, rating_acc 0.88546->0.855736, rate 0.003\n",
      "2017-11-12T15:12:05.853483: step 14300, loss 0.890947, ranking_acc 0.625->0.632812, rating_acc 0.950601->0.939582, rate 0.003\n",
      "2017-11-12T15:12:07.317481: step 14400, loss 0.978075, ranking_acc 0.53125->0.554688, rating_acc 1.01112->0.984842, rate 0.003\n",
      "2017-11-12T15:12:08.809177: step 14500, loss 1.07026, ranking_acc 0.734375->0.71875, rating_acc 1.05101->1.03059, rate 0.003\n",
      "2017-11-12T15:12:10.294132: step 14600, loss 1.01516, ranking_acc 0.421875->0.421875, rating_acc 1.02947->1.00351, rate 0.003\n",
      "2017-11-12T15:12:11.706287: step 14700, loss 0.662688, ranking_acc 0.34375->0.34375, rating_acc 0.825942->0.809041, rate 0.003\n",
      "2017-11-12T15:12:13.199579: step 14800, loss 0.392773, ranking_acc 0.734375->0.742188, rating_acc 0.649234->0.620174, rate 0.003\n",
      "2017-11-12T15:12:14.656644: step 14900, loss 0.632628, ranking_acc 0.742188->0.734375, rating_acc 0.81155->0.790221, rate 0.003\n",
      "2017-11-12T15:12:16.119279: step 15000, loss 0.544052, ranking_acc 0.59375->0.609375, rating_acc 0.751084->0.731971, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:12:18.034860: step 15000, loss 0.910515, ranking_acc 0.524414, rating_acc 0.949865\n",
      "\n",
      "2017-11-12T15:12:19.494346: step 15100, loss 1.06023, ranking_acc 0.578125->0.585938, rating_acc 1.07636->1.02565, rate 0.003\n",
      "2017-11-12T15:12:20.965136: step 15200, loss 0.67538, ranking_acc 0.625->0.609375, rating_acc 0.833853->0.816788, rate 0.003\n",
      "2017-11-12T15:12:24.737432: step 15300, loss 0.857535, ranking_acc 0.710938->0.703125, rating_acc 0.938832->0.921518, rate 0.003\n",
      "2017-11-12T15:12:26.204877: step 15400, loss 0.902434, ranking_acc 0.539062->0.539062, rating_acc 0.967864->0.945528, rate 0.003\n",
      "2017-11-12T15:12:27.672934: step 15500, loss 0.898316, ranking_acc 0.671875->0.703125, rating_acc 0.986113->0.943326, rate 0.003\n",
      "2017-11-12T15:12:29.131343: step 15600, loss 0.274852, ranking_acc 0.539062->0.554688, rating_acc 0.555925->0.516177, rate 0.003\n",
      "2017-11-12T15:12:30.620046: step 15700, loss 0.323933, ranking_acc 0.757812->0.765625, rating_acc 0.593624->0.561684, rate 0.003\n",
      "2017-11-12T15:12:32.083824: step 15800, loss 1.13816, ranking_acc 0.546875->0.570312, rating_acc 1.08524->1.06284, rate 0.003\n",
      "2017-11-12T15:12:33.535395: step 15900, loss 0.945139, ranking_acc 0.507812->0.5, rating_acc 0.982267->0.967761, rate 0.003\n",
      "2017-11-12T15:12:35.007434: step 16000, loss 1.44875, ranking_acc 0.460938->0.46875, rating_acc 1.23071->1.20008, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:12:36.918170: step 16000, loss 0.918043, ranking_acc 0.543945, rating_acc 0.953671\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-16000\n",
      "\n",
      "2017-11-12T15:12:38.477151: step 16100, loss 0.810702, ranking_acc 0.695312->0.703125, rating_acc 0.913248->0.895468, rate 0.003\n",
      "2017-11-12T15:12:39.904854: step 16200, loss 0.481761, ranking_acc 0.578125->0.546875, rating_acc 0.711696->0.687521, rate 0.003\n",
      "2017-11-12T15:12:41.348448: step 16300, loss 0.561455, ranking_acc 0.492188->0.492188, rating_acc 0.765439->0.743082, rate 0.003\n",
      "2017-11-12T15:12:42.800428: step 16400, loss 0.570341, ranking_acc 0.742188->0.757812, rating_acc 0.77071->0.748976, rate 0.003\n",
      "2017-11-12T15:12:44.266283: step 16500, loss 0.688444, ranking_acc 0.59375->0.578125, rating_acc 0.835678->0.824022, rate 0.003\n",
      "2017-11-12T15:12:45.698895: step 16600, loss 0.162403, ranking_acc 0.398438->0.398438, rating_acc 0.395575->0.391067, rate 0.003\n",
      "2017-11-12T15:12:47.204219: step 16700, loss 0.449277, ranking_acc 0.367188->0.375, rating_acc 0.690405->0.66314, rate 0.003\n",
      "2017-11-12T15:12:48.662457: step 16800, loss 0.718415, ranking_acc 0.523438->0.523438, rating_acc 0.86258->0.841944, rate 0.003\n",
      "2017-11-12T15:12:50.121009: step 16900, loss 0.767808, ranking_acc 0.65625->0.65625, rating_acc 0.892238->0.870745, rate 0.003\n",
      "2017-11-12T15:12:51.581705: step 17000, loss 0.363728, ranking_acc 0.492188->0.5, rating_acc 0.641422->0.594984, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:12:53.499404: step 17000, loss 0.900362, ranking_acc 0.542969, rating_acc 0.943737\n",
      "\n",
      "2017-11-12T15:12:54.962481: step 17100, loss 0.570972, ranking_acc 0.679688->0.664062, rating_acc 0.778367->0.749113, rate 0.003\n",
      "2017-11-12T15:12:56.436106: step 17200, loss 0.298726, ranking_acc 0.648438->0.640625, rating_acc 0.572849->0.537409, rate 0.003\n",
      "2017-11-12T15:12:57.889585: step 17300, loss 0.825606, ranking_acc 0.382812->0.390625, rating_acc 0.938534->0.903066, rate 0.003\n",
      "2017-11-12T15:12:59.332782: step 17400, loss 0.565555, ranking_acc 0.664062->0.640625, rating_acc 0.773972->0.745285, rate 0.003\n",
      "2017-11-12T15:13:00.789934: step 17500, loss 1.12372, ranking_acc 0.757812->0.742188, rating_acc 1.08528->1.05523, rate 0.003\n",
      "2017-11-12T15:13:02.267592: step 17600, loss 0.482559, ranking_acc 0.703125->0.71875, rating_acc 0.707758->0.687221, rate 0.003\n",
      "2017-11-12T15:13:03.749054: step 17700, loss 0.687815, ranking_acc 0.75->0.742188, rating_acc 0.856655->0.823056, rate 0.003\n",
      "2017-11-12T15:13:05.201746: step 17800, loss 0.660054, ranking_acc 0.429688->0.4375, rating_acc 0.865929->0.805989, rate 0.003\n",
      "2017-11-12T15:13:06.676698: step 17900, loss 1.21839, ranking_acc 0.554688->0.546875, rating_acc 1.13575->1.09901, rate 0.003\n",
      "2017-11-12T15:13:08.143541: step 18000, loss 0.573291, ranking_acc 0.523438->0.507812, rating_acc 0.773925->0.750078, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:13:10.061793: step 18000, loss 0.891695, ranking_acc 0.538086, rating_acc 0.938628\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-18000\n",
      "\n",
      "2017-11-12T15:13:11.609529: step 18100, loss 0.351872, ranking_acc 0.46875->0.5, rating_acc 0.604256->0.584007, rate 0.003\n",
      "2017-11-12T15:13:13.055720: step 18200, loss 0.742961, ranking_acc 0.390625->0.375, rating_acc 0.885987->0.855608, rate 0.003\n",
      "2017-11-12T15:13:14.526199: step 18300, loss 0.609134, ranking_acc 0.539062->0.546875, rating_acc 0.82144->0.773429, rate 0.003\n",
      "2017-11-12T15:13:15.961936: step 18400, loss 0.299897, ranking_acc 0.40625->0.398438, rating_acc 0.545566->0.537419, rate 0.003\n",
      "2017-11-12T15:13:17.469740: step 18500, loss 0.735955, ranking_acc 0.609375->0.601562, rating_acc 0.870977->0.851338, rate 0.003\n",
      "2017-11-12T15:13:18.932791: step 18600, loss 0.57318, ranking_acc 0.648438->0.65625, rating_acc 0.776293->0.749618, rate 0.003\n",
      "2017-11-12T15:13:20.398570: step 18700, loss 0.394777, ranking_acc 0.585938->0.59375, rating_acc 0.648147->0.619144, rate 0.003\n",
      "2017-11-12T15:13:21.873832: step 18800, loss 0.357892, ranking_acc 0.53125->0.507812, rating_acc 0.636681->0.588529, rate 0.003\n",
      "2017-11-12T15:13:23.341274: step 18900, loss 0.886988, ranking_acc 0.4375->0.429688, rating_acc 0.966642->0.935659, rate 0.003\n",
      "2017-11-12T15:13:24.830821: step 19000, loss 0.387758, ranking_acc 0.710938->0.71875, rating_acc 0.68336->0.613275, rate 0.003\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:13:26.745972: step 19000, loss 0.893432, ranking_acc 0.555664, rating_acc 0.939032\n",
      "\n",
      "2017-11-12T15:13:28.253848: step 19100, loss 0.550133, ranking_acc 0.554688->0.554688, rating_acc 0.799544->0.733711, rate 0.003\n",
      "2017-11-12T15:13:29.725623: step 19200, loss 0.543688, ranking_acc 0.679688->0.703125, rating_acc 0.757649->0.729238, rate 0.003\n",
      "2017-11-12T15:13:31.193061: step 19300, loss 0.407182, ranking_acc 0.53125->0.539062, rating_acc 0.670164->0.628654, rate 0.003\n",
      "2017-11-12T15:13:32.663270: step 19400, loss 0.982463, ranking_acc 0.625->0.640625, rating_acc 1.00728->0.985034, rate 0.003\n",
      "2017-11-12T15:13:34.155087: step 19500, loss 1.51992, ranking_acc 0.484375->0.515625, rating_acc 1.25441->1.22782, rate 0.003\n",
      "2017-11-12T15:13:35.613778: step 19600, loss 0.623108, ranking_acc 0.679688->0.6875, rating_acc 0.826105->0.7814, rate 0.003\n",
      "2017-11-12T15:13:37.088616: step 19700, loss 0.280051, ranking_acc 0.3125->0.320312, rating_acc 0.553937->0.516995, rate 0.003\n",
      "2017-11-12T15:13:38.550749: step 19800, loss 1.40309, ranking_acc 0.539062->0.539062, rating_acc 1.233->1.17906, rate 0.003\n",
      "2017-11-12T15:13:39.971504: step 19900, loss 0.572117, ranking_acc 0.734375->0.742188, rating_acc 0.775733->0.747737, rate 0.003\n",
      "2017-11-12T15:13:41.469369: step 20000, loss 0.816445, ranking_acc 0.578125->0.578125, rating_acc 0.919623->0.896326, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:13:43.383184: step 20000, loss 0.880062, ranking_acc 0.55957, rating_acc 0.931138\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-20000\n",
      "\n",
      "2017-11-12T15:13:44.911533: step 20100, loss 0.629236, ranking_acc 0.664062->0.671875, rating_acc 0.796112->0.784976, rate 0.0015\n",
      "2017-11-12T15:13:46.366267: step 20200, loss 0.536321, ranking_acc 0.515625->0.523438, rating_acc 0.735683->0.723381, rate 0.0015\n",
      "2017-11-12T15:13:47.812885: step 20300, loss 0.33192, ranking_acc 0.609375->0.625, rating_acc 0.581981->0.564683, rate 0.0015\n",
      "2017-11-12T15:13:49.300247: step 20400, loss 0.858167, ranking_acc 0.625->0.625, rating_acc 0.935313->0.919313, rate 0.0015\n",
      "2017-11-12T15:13:50.767049: step 20500, loss 0.713337, ranking_acc 0.585938->0.578125, rating_acc 0.855746->0.836845, rate 0.0015\n",
      "2017-11-12T15:13:52.245089: step 20600, loss 0.391269, ranking_acc 0.460938->0.484375, rating_acc 0.634053->0.615027, rate 0.0015\n",
      "2017-11-12T15:13:53.719177: step 20700, loss 0.451783, ranking_acc 0.6875->0.6875, rating_acc 0.67773->0.662408, rate 0.0015\n",
      "2017-11-12T15:13:55.192924: step 20800, loss 0.489199, ranking_acc 0.328125->0.3125, rating_acc 0.709381->0.690089, rate 0.0015\n",
      "2017-11-12T15:13:56.632144: step 20900, loss 0.454048, ranking_acc 0.617188->0.617188, rating_acc 0.676066->0.664135, rate 0.0015\n",
      "2017-11-12T15:13:58.113410: step 21000, loss 0.468972, ranking_acc 0.648438->0.648438, rating_acc 0.693614->0.675296, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:14:00.028055: step 21000, loss 0.8709, ranking_acc 0.548828, rating_acc 0.926258\n",
      "\n",
      "2017-11-12T15:14:01.476423: step 21100, loss 0.442363, ranking_acc 0.34375->0.335938, rating_acc 0.665579->0.655299, rate 0.0015\n",
      "2017-11-12T15:14:02.917659: step 21200, loss 0.293774, ranking_acc 0.773438->0.765625, rating_acc 0.540297->0.529947, rate 0.0015\n",
      "2017-11-12T15:14:04.411945: step 21300, loss 1.02186, ranking_acc 0.414062->0.40625, rating_acc 1.01756->1.00447, rate 0.0015\n",
      "2017-11-12T15:14:05.874403: step 21400, loss 0.498365, ranking_acc 0.703125->0.695312, rating_acc 0.710184->0.69675, rate 0.0015\n",
      "2017-11-12T15:14:07.351954: step 21500, loss 0.555635, ranking_acc 0.601562->0.609375, rating_acc 0.75285->0.736701, rate 0.0015\n",
      "2017-11-12T15:14:08.839381: step 21600, loss 0.752076, ranking_acc 0.46875->0.46875, rating_acc 0.873956->0.859764, rate 0.0015\n",
      "2017-11-12T15:14:10.286925: step 21700, loss 0.730745, ranking_acc 0.742188->0.742188, rating_acc 0.857966->0.847267, rate 0.0015\n",
      "2017-11-12T15:14:11.751150: step 21800, loss 0.849663, ranking_acc 0.5->0.5, rating_acc 0.935863->0.914758, rate 0.0015\n",
      "2017-11-12T15:14:13.262592: step 21900, loss 0.656747, ranking_acc 0.53125->0.53125, rating_acc 0.817057->0.802413, rate 0.0015\n",
      "2017-11-12T15:14:14.719185: step 22000, loss 0.545491, ranking_acc 0.609375->0.601562, rating_acc 0.756469->0.729816, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:14:16.648402: step 22000, loss 0.865825, ranking_acc 0.55957, rating_acc 0.923561\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-22000\n",
      "\n",
      "2017-11-12T15:14:18.101464: step 22100, loss 0.581814, ranking_acc 0.445312->0.460938, rating_acc 0.768956->0.754296, rate 0.0015\n",
      "2017-11-12T15:14:19.587380: step 22200, loss 0.580476, ranking_acc 0.40625->0.421875, rating_acc 0.772893->0.753415, rate 0.0015\n",
      "2017-11-12T15:14:21.039552: step 22300, loss 0.806435, ranking_acc 0.578125->0.585938, rating_acc 0.906919->0.890854, rate 0.0015\n",
      "2017-11-12T15:14:22.504566: step 22400, loss 0.371524, ranking_acc 0.539062->0.546875, rating_acc 0.615594->0.59891, rate 0.0015\n",
      "2017-11-12T15:14:23.970329: step 22500, loss 0.582808, ranking_acc 0.289062->0.28125, rating_acc 0.774902->0.754962, rate 0.0015\n",
      "2017-11-12T15:14:25.433404: step 22600, loss 0.483593, ranking_acc 0.546875->0.546875, rating_acc 0.69754->0.68611, rate 0.0015\n",
      "2017-11-12T15:14:26.911993: step 22700, loss 0.45857, ranking_acc 0.632812->0.632812, rating_acc 0.684958->0.667621, rate 0.0015\n",
      "2017-11-12T15:14:28.377811: step 22800, loss 0.472928, ranking_acc 0.398438->0.421875, rating_acc 0.696185->0.678302, rate 0.0015\n",
      "2017-11-12T15:14:32.120080: step 22900, loss 0.307041, ranking_acc 0.71875->0.710938, rating_acc 0.570618->0.54237, rate 0.0015\n",
      "2017-11-12T15:14:33.581362: step 23000, loss 0.588258, ranking_acc 0.640625->0.632812, rating_acc 0.79112->0.758535, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:14:35.494253: step 23000, loss 0.858285, ranking_acc 0.553711, rating_acc 0.919458\n",
      "\n",
      "2017-11-12T15:14:36.974707: step 23100, loss 0.415627, ranking_acc 0.648438->0.648438, rating_acc 0.659087->0.63462, rate 0.0015\n",
      "2017-11-12T15:14:38.443838: step 23200, loss 1.73655, ranking_acc 0.28125->0.273438, rating_acc 1.35751->1.3129, rate 0.0015\n",
      "2017-11-12T15:14:39.873644: step 23300, loss 0.402338, ranking_acc 0.476562->0.476562, rating_acc 0.632674->0.62411, rate 0.0015\n",
      "2017-11-12T15:14:41.334444: step 23400, loss 0.574177, ranking_acc 0.578125->0.585938, rating_acc 0.760547->0.749231, rate 0.0015\n",
      "2017-11-12T15:14:42.764976: step 23500, loss 0.426577, ranking_acc 0.539062->0.554688, rating_acc 0.656177->0.643237, rate 0.0015\n",
      "2017-11-12T15:14:44.216530: step 23600, loss 0.360984, ranking_acc 0.3125->0.3125, rating_acc 0.608332->0.590076, rate 0.0015\n",
      "2017-11-12T15:14:45.650538: step 23700, loss 1.02846, ranking_acc 0.484375->0.484375, rating_acc 1.02322->1.00781, rate 0.0015\n",
      "2017-11-12T15:14:47.132331: step 23800, loss 0.456456, ranking_acc 0.65625->0.664062, rating_acc 0.67806->0.666088, rate 0.0015\n",
      "2017-11-12T15:14:48.584066: step 23900, loss 0.54287, ranking_acc 0.648438->0.648438, rating_acc 0.749292->0.728044, rate 0.0015\n",
      "2017-11-12T15:14:50.033408: step 24000, loss 0.262246, ranking_acc 0.539062->0.539062, rating_acc 0.517837->0.499421, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:14:51.948066: step 24000, loss 0.803424, ranking_acc 0.542969, rating_acc 0.889156\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-24000\n",
      "\n",
      "2017-11-12T15:14:53.552727: step 24100, loss 1.08066, ranking_acc 0.546875->0.546875, rating_acc 1.05553->1.03338, rate 0.0015\n",
      "2017-11-12T15:14:55.008857: step 24200, loss 0.529236, ranking_acc 0.375->0.382812, rating_acc 0.736867->0.718627, rate 0.0015\n",
      "2017-11-12T15:14:56.477356: step 24300, loss 1.11154, ranking_acc 0.625->0.617188, rating_acc 1.06967->1.04823, rate 0.0015\n",
      "2017-11-12T15:14:57.944659: step 24400, loss 0.547434, ranking_acc 0.492188->0.492188, rating_acc 0.745432->0.731236, rate 0.0015\n",
      "2017-11-12T15:14:59.404482: step 24500, loss 0.423253, ranking_acc 0.640625->0.648438, rating_acc 0.654784->0.640735, rate 0.0015\n",
      "2017-11-12T15:15:00.833722: step 24600, loss 0.494903, ranking_acc 0.578125->0.601562, rating_acc 0.713917->0.694417, rate 0.0015\n",
      "2017-11-12T15:15:02.299160: step 24700, loss 0.402555, ranking_acc 0.5625->0.554688, rating_acc 0.636336->0.62441, rate 0.0015\n",
      "2017-11-12T15:15:03.767281: step 24800, loss 0.420019, ranking_acc 0.695312->0.679688, rating_acc 0.654272->0.638231, rate 0.0015\n",
      "2017-11-12T15:15:05.210150: step 24900, loss 0.629297, ranking_acc 0.71875->0.734375, rating_acc 0.801172->0.785224, rate 0.0015\n",
      "2017-11-12T15:15:06.674304: step 25000, loss 0.645989, ranking_acc 0.375->0.382812, rating_acc 0.812623->0.795795, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:15:08.594307: step 25000, loss 0.792203, ranking_acc 0.571289, rating_acc 0.882896\n",
      "\n",
      "2017-11-12T15:15:10.034258: step 25100, loss 0.543989, ranking_acc 0.65625->0.640625, rating_acc 0.74222->0.728894, rate 0.0015\n",
      "2017-11-12T15:15:11.466616: step 25200, loss 0.54404, ranking_acc 0.601562->0.601562, rating_acc 0.74623->0.728926, rate 0.0015\n",
      "2017-11-12T15:15:12.893377: step 25300, loss 0.467562, ranking_acc 0.476562->0.476562, rating_acc 0.690004->0.674424, rate 0.0015\n",
      "2017-11-12T15:15:14.293218: step 25400, loss 0.520382, ranking_acc 0.507812->0.515625, rating_acc 0.732925->0.712509, rate 0.0015\n",
      "2017-11-12T15:15:15.722073: step 25500, loss 0.641026, ranking_acc 0.578125->0.578125, rating_acc 0.817918->0.792655, rate 0.0015\n",
      "2017-11-12T15:15:17.090924: step 25600, loss 0.593943, ranking_acc 0.65625->0.664062, rating_acc 0.783607->0.762375, rate 0.0015\n",
      "2017-11-12T15:15:18.480044: step 25700, loss 0.319241, ranking_acc 0.40625->0.40625, rating_acc 0.569485->0.55363, rate 0.0015\n",
      "2017-11-12T15:15:19.941599: step 25800, loss 1.21593, ranking_acc 0.476562->0.484375, rating_acc 1.11425->1.09691, rate 0.0015\n",
      "2017-11-12T15:15:21.408906: step 25900, loss 0.47456, ranking_acc 0.46875->0.445312, rating_acc 0.696956->0.679585, rate 0.0015\n",
      "2017-11-12T15:15:22.823930: step 26000, loss 0.404053, ranking_acc 0.570312->0.585938, rating_acc 0.647145->0.625535, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:15:24.746777: step 26000, loss 0.800762, ranking_acc 0.574219, rating_acc 0.887695\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-26000\n",
      "\n",
      "2017-11-12T15:15:26.240307: step 26100, loss 0.485088, ranking_acc 0.5625->0.5625, rating_acc 0.697737->0.687241, rate 0.0015\n",
      "2017-11-12T15:15:27.663126: step 26200, loss 0.306699, ranking_acc 0.796875->0.804688, rating_acc 0.558496->0.542131, rate 0.0015\n",
      "2017-11-12T15:15:29.097441: step 26300, loss 0.341186, ranking_acc 0.429688->0.453125, rating_acc 0.591753->0.573036, rate 0.0015\n",
      "2017-11-12T15:15:30.482845: step 26400, loss 0.397933, ranking_acc 0.585938->0.585938, rating_acc 0.637541->0.62056, rate 0.0015\n",
      "2017-11-12T15:15:31.902158: step 26500, loss 0.735801, ranking_acc 0.242188->0.242188, rating_acc 0.871489->0.850269, rate 0.0015\n",
      "2017-11-12T15:15:33.310134: step 26600, loss 0.394783, ranking_acc 0.640625->0.640625, rating_acc 0.633939->0.618015, rate 0.0015\n",
      "2017-11-12T15:15:34.625794: step 26700, loss 0.535534, ranking_acc 0.523438->0.515625, rating_acc 0.743434->0.722924, rate 0.0015\n",
      "2017-11-12T15:15:35.997145: step 26800, loss 0.436392, ranking_acc 0.679688->0.671875, rating_acc 0.663081->0.65072, rate 0.0015\n",
      "2017-11-12T15:15:37.454302: step 26900, loss 0.503534, ranking_acc 0.5->0.484375, rating_acc 0.710131->0.700412, rate 0.0015\n",
      "2017-11-12T15:15:38.913459: step 27000, loss 0.252384, ranking_acc 0.71875->0.71875, rating_acc 0.509692->0.489258, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:15:40.840667: step 27000, loss 0.798688, ranking_acc 0.560547, rating_acc 0.886384\n",
      "\n",
      "2017-11-12T15:15:42.303249: step 27100, loss 0.364674, ranking_acc 0.703125->0.703125, rating_acc 0.611942->0.592919, rate 0.0015\n",
      "2017-11-12T15:15:43.652542: step 27200, loss 0.449897, ranking_acc 0.554688->0.5625, rating_acc 0.676692->0.66085, rate 0.0015\n",
      "2017-11-12T15:15:44.959487: step 27300, loss 0.296718, ranking_acc 0.609375->0.609375, rating_acc 0.5463->0.532364, rate 0.0015\n",
      "2017-11-12T15:15:46.362268: step 27400, loss 0.591575, ranking_acc 0.492188->0.484375, rating_acc 0.776501->0.760396, rate 0.0015\n",
      "2017-11-12T15:15:47.818553: step 27500, loss 1.12578, ranking_acc 0.4375->0.4375, rating_acc 1.07841->1.05468, rate 0.0015\n",
      "2017-11-12T15:15:49.249895: step 27600, loss 0.482552, ranking_acc 0.460938->0.453125, rating_acc 0.696682->0.684902, rate 0.0015\n",
      "2017-11-12T15:15:50.696893: step 27700, loss 0.620157, ranking_acc 0.53125->0.539062, rating_acc 0.794338->0.778871, rate 0.0015\n",
      "2017-11-12T15:15:52.197268: step 27800, loss 0.54453, ranking_acc 0.609375->0.609375, rating_acc 0.741419->0.728641, rate 0.0015\n",
      "2017-11-12T15:15:53.701990: step 27900, loss 1.03896, ranking_acc 0.460938->0.4375, rating_acc 1.03294->1.01257, rate 0.0015\n",
      "2017-11-12T15:15:55.161140: step 28000, loss 0.754877, ranking_acc 0.5->0.507812, rating_acc 0.870076->0.860888, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:15:57.089891: step 28000, loss 0.80013, ranking_acc 0.541016, rating_acc 0.886781\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-28000\n",
      "\n",
      "2017-11-12T15:15:58.676106: step 28100, loss 0.286554, ranking_acc 0.554688->0.539062, rating_acc 0.530754->0.522232, rate 0.0015\n",
      "2017-11-12T15:16:00.178641: step 28200, loss 0.388871, ranking_acc 0.445312->0.445312, rating_acc 0.623135->0.612365, rate 0.0015\n",
      "2017-11-12T15:16:01.657665: step 28300, loss 0.689945, ranking_acc 0.617188->0.632812, rating_acc 0.834982->0.822199, rate 0.0015\n",
      "2017-11-12T15:16:03.103160: step 28400, loss 0.507781, ranking_acc 0.507812->0.492188, rating_acc 0.720521->0.702707, rate 0.0015\n",
      "2017-11-12T15:16:04.600273: step 28500, loss 0.420604, ranking_acc 0.648438->0.632812, rating_acc 0.652451->0.637621, rate 0.0015\n",
      "2017-11-12T15:16:06.123279: step 28600, loss 0.440313, ranking_acc 0.5->0.492188, rating_acc 0.664447->0.652854, rate 0.0015\n",
      "2017-11-12T15:16:07.580910: step 28700, loss 0.452939, ranking_acc 0.53125->0.523438, rating_acc 0.674196->0.662398, rate 0.0015\n",
      "2017-11-12T15:16:09.074417: step 28800, loss 0.363695, ranking_acc 0.648438->0.648438, rating_acc 0.604224->0.591167, rate 0.0015\n",
      "2017-11-12T15:16:10.554214: step 28900, loss 0.787941, ranking_acc 0.625->0.617188, rating_acc 0.893317->0.879592, rate 0.0015\n",
      "2017-11-12T15:16:12.014397: step 29000, loss 0.495084, ranking_acc 0.484375->0.492188, rating_acc 0.703902->0.693361, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:16:13.945535: step 29000, loss 0.804966, ranking_acc 0.549805, rating_acc 0.889175\n",
      "\n",
      "2017-11-12T15:16:15.407534: step 29100, loss 0.905744, ranking_acc 0.421875->0.429688, rating_acc 0.957075->0.944107, rate 0.0015\n",
      "2017-11-12T15:16:16.876618: step 29200, loss 0.636637, ranking_acc 0.507812->0.523438, rating_acc 0.802875->0.788782, rate 0.0015\n",
      "2017-11-12T15:16:18.379819: step 29300, loss 0.328813, ranking_acc 0.679688->0.679688, rating_acc 0.572734->0.56062, rate 0.0015\n",
      "2017-11-12T15:16:19.878724: step 29400, loss 0.884436, ranking_acc 0.554688->0.570312, rating_acc 0.951471->0.932663, rate 0.0015\n",
      "2017-11-12T15:16:21.361069: step 29500, loss 0.411329, ranking_acc 0.429688->0.4375, rating_acc 0.643383->0.629842, rate 0.0015\n",
      "2017-11-12T15:16:22.859913: step 29600, loss 0.428168, ranking_acc 0.492188->0.484375, rating_acc 0.657717->0.643033, rate 0.0015\n",
      "2017-11-12T15:16:24.303473: step 29700, loss 0.572878, ranking_acc 0.6875->0.6875, rating_acc 0.762312->0.747084, rate 0.0015\n",
      "2017-11-12T15:16:25.777106: step 29800, loss 0.501591, ranking_acc 0.617188->0.648438, rating_acc 0.712677->0.697697, rate 0.0015\n",
      "2017-11-12T15:16:27.283511: step 29900, loss 0.550526, ranking_acc 0.609375->0.609375, rating_acc 0.750461->0.7319, rate 0.0015\n",
      "2017-11-12T15:16:28.769185: step 30000, loss 0.334696, ranking_acc 0.773438->0.789062, rating_acc 0.577483->0.565491, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:16:30.745986: step 30000, loss 0.803881, ranking_acc 0.572266, rating_acc 0.888237\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-30000\n",
      "\n",
      "2017-11-12T15:16:32.357680: step 30100, loss 0.536955, ranking_acc 0.367188->0.367188, rating_acc 0.738921->0.722479, rate 0.0015\n",
      "2017-11-12T15:16:33.856846: step 30200, loss 0.492781, ranking_acc 0.585938->0.570312, rating_acc 0.702689->0.691187, rate 0.0015\n",
      "2017-11-12T15:16:35.327334: step 30300, loss 0.521495, ranking_acc 0.648438->0.625, rating_acc 0.727754->0.711602, rate 0.0015\n",
      "2017-11-12T15:16:36.794008: step 30400, loss 0.421931, ranking_acc 0.648438->0.640625, rating_acc 0.650329->0.637783, rate 0.0015\n",
      "2017-11-12T15:16:38.125967: step 30500, loss 0.173591, ranking_acc 0.65625->0.664062, rating_acc 0.405103->0.397925, rate 0.0015\n",
      "2017-11-12T15:16:41.947350: step 30600, loss 0.189096, ranking_acc 0.820312->0.828125, rating_acc 0.432297->0.416828, rate 0.0015\n",
      "2017-11-12T15:16:43.369893: step 30700, loss 0.515027, ranking_acc 0.5625->0.570312, rating_acc 0.718036->0.706818, rate 0.0015\n",
      "2017-11-12T15:16:44.855329: step 30800, loss 0.446481, ranking_acc 0.585938->0.585938, rating_acc 0.670155->0.656484, rate 0.0015\n",
      "2017-11-12T15:16:46.282194: step 30900, loss 0.27623, ranking_acc 0.789062->0.78125, rating_acc 0.519293->0.510536, rate 0.0015\n",
      "2017-11-12T15:16:47.686990: step 31000, loss 0.428211, ranking_acc 0.640625->0.625, rating_acc 0.660536->0.642289, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:16:49.627015: step 31000, loss 0.81683, ranking_acc 0.550781, rating_acc 0.895072\n",
      "\n",
      "2017-11-12T15:16:50.998374: step 31100, loss 0.720838, ranking_acc 0.671875->0.679688, rating_acc 0.86269->0.839693, rate 0.0015\n",
      "2017-11-12T15:16:52.415746: step 31200, loss 0.256007, ranking_acc 0.601562->0.601562, rating_acc 0.494109->0.490076, rate 0.0015\n",
      "2017-11-12T15:16:53.765934: step 31300, loss 0.712239, ranking_acc 0.492188->0.484375, rating_acc 0.850183->0.834476, rate 0.0015\n",
      "2017-11-12T15:16:55.217775: step 31400, loss 0.376635, ranking_acc 0.648438->0.648438, rating_acc 0.616789->0.60046, rate 0.0015\n",
      "2017-11-12T15:16:56.616221: step 31500, loss 0.329197, ranking_acc 0.6875->0.6875, rating_acc 0.574909->0.55946, rate 0.0015\n",
      "2017-11-12T15:16:58.073474: step 31600, loss 0.497923, ranking_acc 0.539062->0.53125, rating_acc 0.712916->0.693997, rate 0.0015\n",
      "2017-11-12T15:16:59.507919: step 31700, loss 0.427844, ranking_acc 0.5625->0.5625, rating_acc 0.655501->0.641491, rate 0.0015\n",
      "2017-11-12T15:17:00.914211: step 31800, loss 0.412881, ranking_acc 0.554688->0.554688, rating_acc 0.646019->0.629661, rate 0.0015\n",
      "2017-11-12T15:17:02.211269: step 31900, loss 0.522991, ranking_acc 0.648438->0.648438, rating_acc 0.728434->0.71172, rate 0.0015\n",
      "2017-11-12T15:17:03.559875: step 32000, loss 0.473517, ranking_acc 0.617188->0.617188, rating_acc 0.685308->0.676042, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:17:05.490488: step 32000, loss 0.816653, ranking_acc 0.581055, rating_acc 0.894522\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-32000\n",
      "\n",
      "2017-11-12T15:17:07.024409: step 32100, loss 0.296302, ranking_acc 0.695312->0.6875, rating_acc 0.543401->0.528942, rate 0.0015\n",
      "2017-11-12T15:17:08.513224: step 32200, loss 0.260009, ranking_acc 0.742188->0.742188, rating_acc 0.50421->0.493383, rate 0.0015\n",
      "2017-11-12T15:17:09.978462: step 32300, loss 0.409342, ranking_acc 0.492188->0.492188, rating_acc 0.64056->0.62666, rate 0.0015\n",
      "2017-11-12T15:17:11.426175: step 32400, loss 0.403701, ranking_acc 0.539062->0.539062, rating_acc 0.637401->0.622105, rate 0.0015\n",
      "2017-11-12T15:17:12.907091: step 32500, loss 0.209451, ranking_acc 0.617188->0.625, rating_acc 0.44999->0.438938, rate 0.0015\n",
      "2017-11-12T15:17:14.367926: step 32600, loss 0.603039, ranking_acc 0.5625->0.570312, rating_acc 0.781499->0.765648, rate 0.0015\n",
      "2017-11-12T15:17:15.821498: step 32700, loss 0.374065, ranking_acc 0.632812->0.625, rating_acc 0.613307->0.597655, rate 0.0015\n",
      "2017-11-12T15:17:17.290174: step 32800, loss 0.464236, ranking_acc 0.4375->0.4375, rating_acc 0.676535->0.668804, rate 0.0015\n",
      "2017-11-12T15:17:18.747249: step 32900, loss 0.392668, ranking_acc 0.492188->0.492188, rating_acc 0.62489->0.612937, rate 0.0015\n",
      "2017-11-12T15:17:20.205226: step 33000, loss 0.27473, ranking_acc 0.6875->0.6875, rating_acc 0.516481->0.507648, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:17:22.243615: step 33000, loss 0.813314, ranking_acc 0.566406, rating_acc 0.892351\n",
      "\n",
      "2017-11-12T15:17:23.701206: step 33100, loss 0.272613, ranking_acc 0.585938->0.585938, rating_acc 0.516402->0.505515, rate 0.0015\n",
      "2017-11-12T15:17:25.153692: step 33200, loss 0.76162, ranking_acc 0.484375->0.484375, rating_acc 0.881145->0.862833, rate 0.0015\n",
      "2017-11-12T15:17:26.613099: step 33300, loss 0.598867, ranking_acc 0.648438->0.640625, rating_acc 0.779336->0.762682, rate 0.0015\n",
      "2017-11-12T15:17:28.072880: step 33400, loss 0.487953, ranking_acc 0.640625->0.640625, rating_acc 0.701489->0.686089, rate 0.0015\n",
      "2017-11-12T15:17:29.544321: step 33500, loss 0.209769, ranking_acc 0.5625->0.5625, rating_acc 0.447329->0.438704, rate 0.0015\n",
      "2017-11-12T15:17:30.965468: step 33600, loss 0.291902, ranking_acc 0.664062->0.671875, rating_acc 0.542975->0.523998, rate 0.0015\n",
      "2017-11-12T15:17:32.396956: step 33700, loss 0.3088, ranking_acc 0.515625->0.515625, rating_acc 0.554697->0.539852, rate 0.0015\n",
      "2017-11-12T15:17:33.877829: step 33800, loss 0.293209, ranking_acc 0.625->0.625, rating_acc 0.534412->0.525178, rate 0.0015\n",
      "2017-11-12T15:17:35.277771: step 33900, loss 0.302914, ranking_acc 0.765625->0.765625, rating_acc 0.551727->0.534297, rate 0.0015\n",
      "2017-11-12T15:17:36.693784: step 34000, loss 0.381797, ranking_acc 0.515625->0.515625, rating_acc 0.616067->0.603561, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:17:38.614280: step 34000, loss 0.817631, ranking_acc 0.580078, rating_acc 0.894494\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-34000\n",
      "\n",
      "2017-11-12T15:17:40.128153: step 34100, loss 0.650509, ranking_acc 0.484375->0.484375, rating_acc 0.812314->0.795582, rate 0.0015\n",
      "2017-11-12T15:17:41.535276: step 34200, loss 0.481622, ranking_acc 0.460938->0.484375, rating_acc 0.698434->0.681192, rate 0.0015\n",
      "2017-11-12T15:17:42.910537: step 34300, loss 0.312896, ranking_acc 0.546875->0.554688, rating_acc 0.565126->0.543352, rate 0.0015\n",
      "2017-11-12T15:17:44.371800: step 34400, loss 0.38272, ranking_acc 0.578125->0.585938, rating_acc 0.620469->0.604143, rate 0.0015\n",
      "2017-11-12T15:17:45.771573: step 34500, loss 0.360775, ranking_acc 0.710938->0.710938, rating_acc 0.604675->0.585678, rate 0.0015\n",
      "2017-11-12T15:17:47.210045: step 34600, loss 0.384076, ranking_acc 0.492188->0.484375, rating_acc 0.618528->0.605214, rate 0.0015\n",
      "2017-11-12T15:17:48.646121: step 34700, loss 0.452491, ranking_acc 0.664062->0.664062, rating_acc 0.672582->0.659241, rate 0.0015\n",
      "2017-11-12T15:17:50.011141: step 34800, loss 0.438517, ranking_acc 0.695312->0.679688, rating_acc 0.663166->0.648491, rate 0.0015\n",
      "2017-11-12T15:17:51.487624: step 34900, loss 0.468408, ranking_acc 0.703125->0.6875, rating_acc 0.681546->0.671074, rate 0.0015\n",
      "2017-11-12T15:17:52.957786: step 35000, loss 0.263274, ranking_acc 0.554688->0.546875, rating_acc 0.508868->0.495081, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:17:54.900051: step 35000, loss 0.81376, ranking_acc 0.583984, rating_acc 0.891959\n",
      "\n",
      "2017-11-12T15:17:56.321117: step 35100, loss 0.749093, ranking_acc 0.570312->0.578125, rating_acc 0.870206->0.854914, rate 0.0015\n",
      "2017-11-12T15:17:57.679264: step 35200, loss 0.282938, ranking_acc 0.445312->0.46875, rating_acc 0.526258->0.514459, rate 0.0015\n",
      "2017-11-12T15:17:59.045239: step 35300, loss 0.717035, ranking_acc 0.515625->0.515625, rating_acc 0.851973->0.835901, rate 0.0015\n",
      "2017-11-12T15:18:00.403180: step 35400, loss 0.310954, ranking_acc 0.609375->0.585938, rating_acc 0.552645->0.540918, rate 0.0015\n",
      "2017-11-12T15:18:01.862578: step 35500, loss 0.447661, ranking_acc 0.640625->0.640625, rating_acc 0.665145->0.655182, rate 0.0015\n",
      "2017-11-12T15:18:03.214345: step 35600, loss 0.204845, ranking_acc 0.578125->0.578125, rating_acc 0.439498->0.431726, rate 0.0015\n",
      "2017-11-12T15:18:04.631919: step 35700, loss 0.378734, ranking_acc 0.71875->0.71875, rating_acc 0.614284->0.600183, rate 0.0015\n",
      "2017-11-12T15:18:05.995216: step 35800, loss 0.346668, ranking_acc 0.554688->0.554688, rating_acc 0.58073->0.57281, rate 0.0015\n",
      "2017-11-12T15:18:07.334564: step 35900, loss 0.330986, ranking_acc 0.65625->0.65625, rating_acc 0.570017->0.558928, rate 0.0015\n",
      "2017-11-12T15:18:08.673534: step 36000, loss 0.525101, ranking_acc 0.570312->0.578125, rating_acc 0.722987->0.711676, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:18:10.611928: step 36000, loss 0.819867, ranking_acc 0.563477, rating_acc 0.895125\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-36000\n",
      "\n",
      "2017-11-12T15:18:12.047512: step 36100, loss 0.453409, ranking_acc 0.570312->0.585938, rating_acc 0.672454->0.659356, rate 0.0015\n",
      "2017-11-12T15:18:13.380996: step 36200, loss 0.507376, ranking_acc 0.59375->0.585938, rating_acc 0.715662->0.699052, rate 0.0015\n",
      "2017-11-12T15:18:14.853088: step 36300, loss 0.807669, ranking_acc 0.484375->0.476562, rating_acc 0.920488->0.888228, rate 0.0015\n",
      "2017-11-12T15:18:16.285545: step 36400, loss 0.295197, ranking_acc 0.703125->0.703125, rating_acc 0.538986->0.525769, rate 0.0015\n",
      "2017-11-12T15:18:17.771150: step 36500, loss 0.367778, ranking_acc 0.515625->0.507812, rating_acc 0.602064->0.590746, rate 0.0015\n",
      "2017-11-12T15:18:19.277121: step 36600, loss 0.441065, ranking_acc 0.664062->0.671875, rating_acc 0.665093->0.649779, rate 0.0015\n",
      "2017-11-12T15:18:20.719146: step 36700, loss 0.34453, ranking_acc 0.5625->0.5625, rating_acc 0.578426->0.570627, rate 0.0015\n",
      "2017-11-12T15:18:22.184410: step 36800, loss 0.66869, ranking_acc 0.664062->0.65625, rating_acc 0.820281->0.80606, rate 0.0015\n",
      "2017-11-12T15:18:23.643515: step 36900, loss 0.251737, ranking_acc 0.773438->0.78125, rating_acc 0.494629->0.482427, rate 0.0015\n",
      "2017-11-12T15:18:25.073783: step 37000, loss 0.604065, ranking_acc 0.515625->0.515625, rating_acc 0.776351->0.764863, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:18:27.018175: step 37000, loss 0.822378, ranking_acc 0.560547, rating_acc 0.896286\n",
      "\n",
      "2017-11-12T15:18:28.487381: step 37100, loss 0.707616, ranking_acc 0.578125->0.59375, rating_acc 0.848612->0.829774, rate 0.0015\n",
      "2017-11-12T15:18:29.855050: step 37200, loss 0.33833, ranking_acc 0.75->0.75, rating_acc 0.58265->0.564962, rate 0.0015\n",
      "2017-11-12T15:18:31.287147: step 37300, loss 0.230221, ranking_acc 0.804688->0.804688, rating_acc 0.474757->0.459367, rate 0.0015\n",
      "2017-11-12T15:18:32.706116: step 37400, loss 0.721137, ranking_acc 0.296875->0.296875, rating_acc 0.861038->0.837783, rate 0.0015\n",
      "2017-11-12T15:18:34.139931: step 37500, loss 0.426453, ranking_acc 0.617188->0.617188, rating_acc 0.652867->0.638079, rate 0.0015\n",
      "2017-11-12T15:18:35.588622: step 37600, loss 0.503675, ranking_acc 0.328125->0.328125, rating_acc 0.719508->0.695931, rate 0.0015\n",
      "2017-11-12T15:18:37.049374: step 37700, loss 0.476972, ranking_acc 0.515625->0.523438, rating_acc 0.697713->0.676426, rate 0.0015\n",
      "2017-11-12T15:18:38.506943: step 37800, loss 0.439864, ranking_acc 0.40625->0.414062, rating_acc 0.661653->0.648374, rate 0.0015\n",
      "2017-11-12T15:18:39.939283: step 37900, loss 0.305948, ranking_acc 0.640625->0.632812, rating_acc 0.548859->0.535147, rate 0.0015\n",
      "2017-11-12T15:18:41.330978: step 38000, loss 0.415774, ranking_acc 0.71875->0.726562, rating_acc 0.656747->0.629415, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:18:43.286281: step 38000, loss 0.818315, ranking_acc 0.551758, rating_acc 0.893702\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-38000\n",
      "\n",
      "2017-11-12T15:18:44.746885: step 38100, loss 0.236326, ranking_acc 0.539062->0.539062, rating_acc 0.484179->0.465467, rate 0.0015\n",
      "2017-11-12T15:18:48.509936: step 38200, loss 0.408585, ranking_acc 0.757812->0.742188, rating_acc 0.638148->0.623552, rate 0.0015\n",
      "2017-11-12T15:18:49.890905: step 38300, loss 0.33373, ranking_acc 0.8125->0.804688, rating_acc 0.574062->0.56028, rate 0.0015\n",
      "2017-11-12T15:18:51.373351: step 38400, loss 0.425114, ranking_acc 0.53125->0.53125, rating_acc 0.649191->0.636585, rate 0.0015\n",
      "2017-11-12T15:18:52.824386: step 38500, loss 0.446253, ranking_acc 0.710938->0.710938, rating_acc 0.667838->0.652931, rate 0.0015\n",
      "2017-11-12T15:18:54.208340: step 38600, loss 0.350739, ranking_acc 0.773438->0.773438, rating_acc 0.595639->0.575091, rate 0.0015\n",
      "2017-11-12T15:18:55.666289: step 38700, loss 0.739893, ranking_acc 0.390625->0.398438, rating_acc 0.8633->0.848412, rate 0.0015\n",
      "2017-11-12T15:18:57.145234: step 38800, loss 0.381987, ranking_acc 0.414062->0.40625, rating_acc 0.620074->0.601534, rate 0.0015\n",
      "2017-11-12T15:18:58.603447: step 38900, loss 0.512889, ranking_acc 0.578125->0.5625, rating_acc 0.716607->0.701935, rate 0.0015\n",
      "2017-11-12T15:19:00.058942: step 39000, loss 0.521839, ranking_acc 0.640625->0.640625, rating_acc 0.726167->0.708221, rate 0.0015\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:19:02.010917: step 39000, loss 0.8286, ranking_acc 0.567383, rating_acc 0.899076\n",
      "\n",
      "2017-11-12T15:19:03.431742: step 39100, loss 0.301099, ranking_acc 0.671875->0.671875, rating_acc 0.542348->0.529856, rate 0.0015\n",
      "2017-11-12T15:19:04.884674: step 39200, loss 0.379794, ranking_acc 0.46875->0.46875, rating_acc 0.619833->0.599452, rate 0.0015\n",
      "2017-11-12T15:19:06.367891: step 39300, loss 0.692083, ranking_acc 0.476562->0.476562, rating_acc 0.836258->0.819496, rate 0.0015\n",
      "2017-11-12T15:19:07.826446: step 39400, loss 0.54557, ranking_acc 0.46875->0.46875, rating_acc 0.737166->0.72458, rate 0.0015\n",
      "2017-11-12T15:19:09.305498: step 39500, loss 0.20829, ranking_acc 0.414062->0.445312, rating_acc 0.448565->0.433219, rate 0.0015\n",
      "2017-11-12T15:19:10.754470: step 39600, loss 0.275769, ranking_acc 0.65625->0.648438, rating_acc 0.514826->0.505097, rate 0.0015\n",
      "2017-11-12T15:19:12.216925: step 39700, loss 0.430663, ranking_acc 0.578125->0.570312, rating_acc 0.650784->0.640303, rate 0.0015\n",
      "2017-11-12T15:19:13.687460: step 39800, loss 0.219293, ranking_acc 0.484375->0.492188, rating_acc 0.466324->0.445599, rate 0.0015\n",
      "2017-11-12T15:19:15.168805: step 39900, loss 0.135654, ranking_acc 0.59375->0.609375, rating_acc 0.353878->0.338916, rate 0.0015\n",
      "2017-11-12T15:19:16.654071: step 40000, loss 0.343398, ranking_acc 0.75->0.75, rating_acc 0.574843->0.567943, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:19:18.587018: step 40000, loss 0.836937, ranking_acc 0.566406, rating_acc 0.903382\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-40000\n",
      "\n",
      "2017-11-12T15:19:20.117049: step 40100, loss 0.168086, ranking_acc 0.734375->0.734375, rating_acc 0.391557->0.383695, rate 0.00075\n",
      "2017-11-12T15:19:21.599254: step 40200, loss 0.567992, ranking_acc 0.53125->0.539062, rating_acc 0.746903->0.739659, rate 0.00075\n",
      "2017-11-12T15:19:23.081529: step 40300, loss 0.315697, ranking_acc 0.484375->0.484375, rating_acc 0.549874->0.542949, rate 0.00075\n",
      "2017-11-12T15:19:24.551939: step 40400, loss 0.323266, ranking_acc 0.460938->0.460938, rating_acc 0.558017->0.549856, rate 0.00075\n",
      "2017-11-12T15:19:26.039869: step 40500, loss 0.331427, ranking_acc 0.507812->0.507812, rating_acc 0.565552->0.557211, rate 0.00075\n",
      "2017-11-12T15:19:27.510837: step 40600, loss 0.33249, ranking_acc 0.648438->0.632812, rating_acc 0.566503->0.558152, rate 0.00075\n",
      "2017-11-12T15:19:28.975347: step 40700, loss 0.530646, ranking_acc 0.648438->0.648438, rating_acc 0.726694->0.713917, rate 0.00075\n",
      "2017-11-12T15:19:30.482793: step 40800, loss 0.513864, ranking_acc 0.492188->0.492188, rating_acc 0.713864->0.702047, rate 0.00075\n",
      "2017-11-12T15:19:31.961554: step 40900, loss 0.232276, ranking_acc 0.523438->0.523438, rating_acc 0.467769->0.459646, rate 0.00075\n",
      "2017-11-12T15:19:33.439627: step 41000, loss 0.238437, ranking_acc 0.625->0.625, rating_acc 0.470294->0.46627, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:19:35.369277: step 41000, loss 0.837472, ranking_acc 0.573242, rating_acc 0.903572\n",
      "\n",
      "2017-11-12T15:19:36.864157: step 41100, loss 0.420378, ranking_acc 0.546875->0.546875, rating_acc 0.641222->0.631924, rate 0.00075\n",
      "2017-11-12T15:19:38.339558: step 41200, loss 0.294169, ranking_acc 0.742188->0.75, rating_acc 0.529947->0.522595, rate 0.00075\n",
      "2017-11-12T15:19:39.791124: step 41300, loss 0.342175, ranking_acc 0.710938->0.710938, rating_acc 0.574375->0.566647, rate 0.00075\n",
      "2017-11-12T15:19:41.253958: step 41400, loss 0.317513, ranking_acc 0.554688->0.554688, rating_acc 0.551675->0.544431, rate 0.00075\n",
      "2017-11-12T15:19:42.760237: step 41500, loss 0.26012, ranking_acc 0.695312->0.6875, rating_acc 0.496466->0.488883, rate 0.00075\n",
      "2017-11-12T15:19:44.234404: step 41600, loss 0.436733, ranking_acc 0.570312->0.5625, rating_acc 0.651884->0.644661, rate 0.00075\n",
      "2017-11-12T15:19:45.703271: step 41700, loss 0.160256, ranking_acc 0.46875->0.46875, rating_acc 0.378244->0.372953, rate 0.00075\n",
      "2017-11-12T15:19:47.193644: step 41800, loss 0.28751, ranking_acc 0.476562->0.476562, rating_acc 0.523518->0.516086, rate 0.00075\n",
      "2017-11-12T15:19:48.658050: step 41900, loss 0.289789, ranking_acc 0.460938->0.453125, rating_acc 0.523474->0.518261, rate 0.00075\n",
      "2017-11-12T15:19:50.104304: step 42000, loss 0.382576, ranking_acc 0.554688->0.5625, rating_acc 0.612826->0.601131, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:19:52.042503: step 42000, loss 0.840433, ranking_acc 0.572266, rating_acc 0.905106\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-42000\n",
      "\n",
      "2017-11-12T15:19:53.624474: step 42100, loss 0.303259, ranking_acc 0.546875->0.546875, rating_acc 0.538695->0.531056, rate 0.00075\n",
      "2017-11-12T15:19:55.071885: step 42200, loss 0.242421, ranking_acc 0.671875->0.671875, rating_acc 0.476469->0.470282, rate 0.00075\n",
      "2017-11-12T15:19:56.549695: step 42300, loss 0.500613, ranking_acc 0.539062->0.53125, rating_acc 0.701159->0.692337, rate 0.00075\n",
      "2017-11-12T15:19:58.032574: step 42400, loss 1.31442, ranking_acc 0.515625->0.539062, rating_acc 1.1495->1.13715, rate 0.00075\n",
      "2017-11-12T15:19:59.527682: step 42500, loss 0.447088, ranking_acc 0.578125->0.578125, rating_acc 0.660942->0.652493, rate 0.00075\n",
      "2017-11-12T15:20:00.987637: step 42600, loss 0.276011, ranking_acc 0.671875->0.671875, rating_acc 0.51392->0.504605, rate 0.00075\n",
      "2017-11-12T15:20:02.472712: step 42700, loss 0.446561, ranking_acc 0.4375->0.4375, rating_acc 0.65905->0.652031, rate 0.00075\n",
      "2017-11-12T15:20:03.909972: step 42800, loss 0.436454, ranking_acc 0.65625->0.648438, rating_acc 0.655277->0.644211, rate 0.00075\n",
      "2017-11-12T15:20:05.371808: step 42900, loss 0.41469, ranking_acc 0.664062->0.664062, rating_acc 0.633601->0.627084, rate 0.00075\n",
      "2017-11-12T15:20:06.841891: step 43000, loss 0.719299, ranking_acc 0.570312->0.570312, rating_acc 0.842248->0.835356, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:20:08.763795: step 43000, loss 0.839912, ranking_acc 0.567383, rating_acc 0.904673\n",
      "\n",
      "2017-11-12T15:20:10.220568: step 43100, loss 0.44109, ranking_acc 0.539062->0.539062, rating_acc 0.654706->0.647752, rate 0.00075\n",
      "2017-11-12T15:20:11.669588: step 43200, loss 0.179669, ranking_acc 0.570312->0.570312, rating_acc 0.40676->0.397655, rate 0.00075\n",
      "2017-11-12T15:20:13.118814: step 43300, loss 0.834745, ranking_acc 0.75->0.742188, rating_acc 0.910522->0.90177, rate 0.00075\n",
      "2017-11-12T15:20:14.543915: step 43400, loss 0.90481, ranking_acc 0.476562->0.484375, rating_acc 0.94694->0.939806, rate 0.00075\n",
      "2017-11-12T15:20:16.018379: step 43500, loss 0.261634, ranking_acc 0.492188->0.484375, rating_acc 0.495011->0.489947, rate 0.00075\n",
      "2017-11-12T15:20:17.488351: step 43600, loss 0.262747, ranking_acc 0.703125->0.710938, rating_acc 0.499837->0.491069, rate 0.00075\n",
      "2017-11-12T15:20:18.942274: step 43700, loss 0.387431, ranking_acc 0.398438->0.390625, rating_acc 0.610594->0.604834, rate 0.00075\n",
      "2017-11-12T15:20:20.395175: step 43800, loss 0.434663, ranking_acc 0.710938->0.710938, rating_acc 0.651474->0.642683, rate 0.00075\n",
      "2017-11-12T15:20:21.869812: step 43900, loss 0.329763, ranking_acc 0.578125->0.578125, rating_acc 0.561193->0.555098, rate 0.00075\n",
      "2017-11-12T15:20:23.344676: step 44000, loss 0.320456, ranking_acc 0.5->0.5, rating_acc 0.55265->0.54664, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:20:25.272777: step 44000, loss 0.841394, ranking_acc 0.572266, rating_acc 0.905402\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-44000\n",
      "\n",
      "2017-11-12T15:20:26.801310: step 44100, loss 0.304467, ranking_acc 0.507812->0.507812, rating_acc 0.535367->0.531818, rate 0.00075\n",
      "2017-11-12T15:20:28.282863: step 44200, loss 0.675698, ranking_acc 0.523438->0.53125, rating_acc 0.818313->0.808733, rate 0.00075\n",
      "2017-11-12T15:20:29.736680: step 44300, loss 0.444769, ranking_acc 0.507812->0.507812, rating_acc 0.656519->0.650456, rate 0.00075\n",
      "2017-11-12T15:20:31.204191: step 44400, loss 0.440315, ranking_acc 0.53125->0.53125, rating_acc 0.651079->0.647014, rate 0.00075\n",
      "2017-11-12T15:20:32.675749: step 44500, loss 0.232714, ranking_acc 0.664062->0.6875, rating_acc 0.470206->0.459367, rate 0.00075\n",
      "2017-11-12T15:20:34.147844: step 44600, loss 0.552633, ranking_acc 0.492188->0.507812, rating_acc 0.736041->0.728644, rate 0.00075\n",
      "2017-11-12T15:20:35.610799: step 44700, loss 0.509654, ranking_acc 0.460938->0.460938, rating_acc 0.708132->0.698524, rate 0.00075\n",
      "2017-11-12T15:20:37.078314: step 44800, loss 0.274774, ranking_acc 0.539062->0.53125, rating_acc 0.511238->0.503035, rate 0.00075\n",
      "2017-11-12T15:20:38.545245: step 44900, loss 0.28657, ranking_acc 0.578125->0.570312, rating_acc 0.526591->0.514614, rate 0.00075\n",
      "2017-11-12T15:20:40.016675: step 45000, loss 0.338638, ranking_acc 0.609375->0.617188, rating_acc 0.571182->0.562923, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:20:41.943126: step 45000, loss 0.842227, ranking_acc 0.564453, rating_acc 0.905799\n",
      "\n",
      "2017-11-12T15:20:43.411254: step 45100, loss 0.618819, ranking_acc 0.492188->0.492188, rating_acc 0.781035->0.77269, rate 0.00075\n",
      "2017-11-12T15:20:44.921966: step 45200, loss 0.359432, ranking_acc 0.664062->0.664062, rating_acc 0.587884->0.581082, rate 0.00075\n",
      "2017-11-12T15:20:46.374493: step 45300, loss 0.290503, ranking_acc 0.6875->0.6875, rating_acc 0.526263->0.518374, rate 0.00075\n",
      "2017-11-12T15:20:47.861346: step 45400, loss 0.500108, ranking_acc 0.226562->0.226562, rating_acc 0.707951->0.691588, rate 0.00075\n",
      "2017-11-12T15:20:49.356657: step 45500, loss 0.255496, ranking_acc 0.84375->0.84375, rating_acc 0.490866->0.483378, rate 0.00075\n",
      "2017-11-12T15:20:50.822016: step 45600, loss 0.274353, ranking_acc 0.65625->0.65625, rating_acc 0.508433->0.502494, rate 0.00075\n",
      "2017-11-12T15:20:52.266054: step 45700, loss 0.619133, ranking_acc 0.460938->0.460938, rating_acc 0.784765->0.772831, rate 0.00075\n",
      "2017-11-12T15:20:56.092029: step 45800, loss 0.334232, ranking_acc 0.453125->0.460938, rating_acc 0.568935->0.558872, rate 0.00075\n",
      "2017-11-12T15:20:57.548715: step 45900, loss 0.58924, ranking_acc 0.476562->0.46875, rating_acc 0.765626->0.753215, rate 0.00075\n",
      "2017-11-12T15:20:59.024705: step 46000, loss 0.200326, ranking_acc 0.703125->0.710938, rating_acc 0.4366->0.422367, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:21:00.954994: step 46000, loss 0.844066, ranking_acc 0.579102, rating_acc 0.906716\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-46000\n",
      "\n",
      "2017-11-12T15:21:02.484569: step 46100, loss 0.377181, ranking_acc 0.3125->0.320312, rating_acc 0.607753->0.596018, rate 0.00075\n",
      "2017-11-12T15:21:03.990030: step 46200, loss 0.24258, ranking_acc 0.515625->0.515625, rating_acc 0.478045->0.469686, rate 0.00075\n",
      "2017-11-12T15:21:05.443826: step 46300, loss 0.293018, ranking_acc 0.648438->0.640625, rating_acc 0.528809->0.520595, rate 0.00075\n",
      "2017-11-12T15:21:06.915545: step 46400, loss 0.294435, ranking_acc 0.507812->0.5, rating_acc 0.534232->0.521925, rate 0.00075\n",
      "2017-11-12T15:21:08.409141: step 46500, loss 0.218073, ranking_acc 0.304688->0.296875, rating_acc 0.456455->0.442749, rate 0.00075\n",
      "2017-11-12T15:21:09.896865: step 46600, loss 0.330764, ranking_acc 0.695312->0.703125, rating_acc 0.565296->0.5556, rate 0.00075\n",
      "2017-11-12T15:21:11.381206: step 46700, loss 0.351623, ranking_acc 0.46875->0.46875, rating_acc 0.578648->0.574009, rate 0.00075\n",
      "2017-11-12T15:21:12.862849: step 46800, loss 0.221852, ranking_acc 0.546875->0.554688, rating_acc 0.456335->0.446865, rate 0.00075\n",
      "2017-11-12T15:21:14.348105: step 46900, loss 0.227518, ranking_acc 0.5625->0.5625, rating_acc 0.460824->0.45314, rate 0.00075\n",
      "2017-11-12T15:21:15.789538: step 47000, loss 0.778455, ranking_acc 0.367188->0.367188, rating_acc 0.880723->0.869632, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:21:17.712672: step 47000, loss 0.839515, ranking_acc 0.583984, rating_acc 0.904057\n",
      "\n",
      "2017-11-12T15:21:19.192887: step 47100, loss 0.298146, ranking_acc 0.726562->0.726562, rating_acc 0.534431->0.525284, rate 0.00075\n",
      "2017-11-12T15:21:20.644848: step 47200, loss 0.298881, ranking_acc 0.75->0.757812, rating_acc 0.533989->0.525985, rate 0.00075\n",
      "2017-11-12T15:21:22.138283: step 47300, loss 0.291284, ranking_acc 0.4375->0.4375, rating_acc 0.52462->0.518707, rate 0.00075\n",
      "2017-11-12T15:21:23.587475: step 47400, loss 0.351617, ranking_acc 0.65625->0.65625, rating_acc 0.582057->0.57392, rate 0.00075\n",
      "2017-11-12T15:21:25.082591: step 47500, loss 0.163437, ranking_acc 0.390625->0.390625, rating_acc 0.379416->0.37574, rate 0.00075\n",
      "2017-11-12T15:21:26.536722: step 47600, loss 0.23796, ranking_acc 0.367188->0.367188, rating_acc 0.472175->0.464431, rate 0.00075\n",
      "2017-11-12T15:21:28.003672: step 47700, loss 0.257655, ranking_acc 0.804688->0.804688, rating_acc 0.49295->0.48514, rate 0.00075\n",
      "2017-11-12T15:21:29.443248: step 47800, loss 0.286856, ranking_acc 0.648438->0.65625, rating_acc 0.524216->0.514313, rate 0.00075\n",
      "2017-11-12T15:21:30.899934: step 47900, loss 0.286258, ranking_acc 0.601562->0.609375, rating_acc 0.524093->0.513722, rate 0.00075\n",
      "2017-11-12T15:21:32.368542: step 48000, loss 0.293724, ranking_acc 0.617188->0.617188, rating_acc 0.531926->0.52091, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:21:34.306299: step 48000, loss 0.837234, ranking_acc 0.567383, rating_acc 0.902694\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-48000\n",
      "\n",
      "2017-11-12T15:21:35.861771: step 48100, loss 0.568758, ranking_acc 0.59375->0.59375, rating_acc 0.747985->0.739159, rate 0.00075\n",
      "2017-11-12T15:21:37.341207: step 48200, loss 0.458819, ranking_acc 0.445312->0.445312, rating_acc 0.667217->0.660603, rate 0.00075\n",
      "2017-11-12T15:21:38.810419: step 48300, loss 0.158226, ranking_acc 0.625->0.617188, rating_acc 0.372757->0.368484, rate 0.00075\n",
      "2017-11-12T15:21:40.280559: step 48400, loss 0.335857, ranking_acc 0.601562->0.601562, rating_acc 0.571496->0.559812, rate 0.00075\n",
      "2017-11-12T15:21:41.762535: step 48500, loss 0.420361, ranking_acc 0.429688->0.429688, rating_acc 0.637041->0.63077, rate 0.00075\n",
      "2017-11-12T15:21:43.225056: step 48600, loss 0.436956, ranking_acc 0.609375->0.609375, rating_acc 0.65362->0.643768, rate 0.00075\n",
      "2017-11-12T15:21:44.722395: step 48700, loss 0.293436, ranking_acc 0.476562->0.476562, rating_acc 0.52885->0.520474, rate 0.00075\n",
      "2017-11-12T15:21:46.217919: step 48800, loss 0.234081, ranking_acc 0.570312->0.570312, rating_acc 0.468942->0.459909, rate 0.00075\n",
      "2017-11-12T15:21:47.682416: step 48900, loss 0.253233, ranking_acc 0.585938->0.578125, rating_acc 0.492039->0.480259, rate 0.00075\n",
      "2017-11-12T15:21:49.181812: step 49000, loss 0.390665, ranking_acc 0.484375->0.484375, rating_acc 0.614842->0.606674, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:21:51.106694: step 49000, loss 0.837533, ranking_acc 0.579102, rating_acc 0.902729\n",
      "\n",
      "2017-11-12T15:21:52.566934: step 49100, loss 0.320983, ranking_acc 0.507812->0.507812, rating_acc 0.550536->0.546217, rate 0.00075\n",
      "2017-11-12T15:21:54.025353: step 49200, loss 0.224097, ranking_acc 0.554688->0.554688, rating_acc 0.454959->0.448818, rate 0.00075\n",
      "2017-11-12T15:21:55.533448: step 49300, loss 0.42085, ranking_acc 0.5625->0.5625, rating_acc 0.637673->0.631, rate 0.00075\n",
      "2017-11-12T15:21:57.011335: step 49400, loss 0.259777, ranking_acc 0.523438->0.523438, rating_acc 0.493509->0.486904, rate 0.00075\n",
      "2017-11-12T15:21:58.493789: step 49500, loss 0.273465, ranking_acc 0.53125->0.539062, rating_acc 0.510098->0.500738, rate 0.00075\n",
      "2017-11-12T15:22:00.009977: step 49600, loss 0.340755, ranking_acc 0.5625->0.5625, rating_acc 0.573578->0.563909, rate 0.00075\n",
      "2017-11-12T15:22:01.480095: step 49700, loss 0.351807, ranking_acc 0.757812->0.757812, rating_acc 0.579512->0.573604, rate 0.00075\n",
      "2017-11-12T15:22:02.944682: step 49800, loss 0.168054, ranking_acc 0.664062->0.648438, rating_acc 0.389155->0.381122, rate 0.00075\n",
      "2017-11-12T15:22:04.454849: step 49900, loss 0.131847, ranking_acc 0.578125->0.570312, rating_acc 0.344204->0.330184, rate 0.00075\n",
      "2017-11-12T15:22:05.923163: step 50000, loss 0.194207, ranking_acc 0.65625->0.65625, rating_acc 0.416876->0.413925, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:22:07.850201: step 50000, loss 0.838507, ranking_acc 0.558594, rating_acc 0.903125\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-50000\n",
      "\n",
      "2017-11-12T15:22:09.402196: step 50100, loss 0.348315, ranking_acc 0.53125->0.53125, rating_acc 0.579129->0.570451, rate 0.00075\n",
      "2017-11-12T15:22:10.870669: step 50200, loss 0.266392, ranking_acc 0.507812->0.507812, rating_acc 0.500311->0.493391, rate 0.00075\n",
      "2017-11-12T15:22:12.337072: step 50300, loss 0.239362, ranking_acc 0.570312->0.570312, rating_acc 0.469872->0.465161, rate 0.00075\n",
      "2017-11-12T15:22:13.786998: step 50400, loss 0.231219, ranking_acc 0.546875->0.546875, rating_acc 0.46502->0.45629, rate 0.00075\n",
      "2017-11-12T15:22:15.258515: step 50500, loss 0.263818, ranking_acc 0.523438->0.53125, rating_acc 0.494388->0.490688, rate 0.00075\n",
      "2017-11-12T15:22:16.737974: step 50600, loss 0.458636, ranking_acc 0.367188->0.367188, rating_acc 0.669412->0.659981, rate 0.00075\n",
      "2017-11-12T15:22:18.223514: step 50700, loss 0.372883, ranking_acc 0.53125->0.523438, rating_acc 0.597474->0.591429, rate 0.00075\n",
      "2017-11-12T15:22:19.682242: step 50800, loss 0.786422, ranking_acc 0.570312->0.570312, rating_acc 0.887718->0.873679, rate 0.00075\n",
      "2017-11-12T15:22:21.172073: step 50900, loss 0.414805, ranking_acc 0.523438->0.53125, rating_acc 0.632662->0.625835, rate 0.00075\n",
      "2017-11-12T15:22:22.654339: step 51000, loss 0.292956, ranking_acc 0.554688->0.5625, rating_acc 0.523928->0.519411, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:22:24.575931: step 51000, loss 0.838548, ranking_acc 0.5625, rating_acc 0.902984\n",
      "\n",
      "2017-11-12T15:22:26.051841: step 51100, loss 0.443108, ranking_acc 0.5625->0.554688, rating_acc 0.658532->0.648015, rate 0.00075\n",
      "2017-11-12T15:22:27.554735: step 51200, loss 0.391089, ranking_acc 0.5625->0.5625, rating_acc 0.611283->0.606535, rate 0.00075\n",
      "2017-11-12T15:22:29.013147: step 51300, loss 0.211206, ranking_acc 0.671875->0.648438, rating_acc 0.440916->0.43357, rate 0.00075\n",
      "2017-11-12T15:22:30.446668: step 51400, loss 0.341915, ranking_acc 0.617188->0.625, rating_acc 0.572559->0.564512, rate 0.00075\n",
      "2017-11-12T15:22:31.927731: step 51500, loss 0.556454, ranking_acc 0.65625->0.65625, rating_acc 0.739874->0.730203, rate 0.00075\n",
      "2017-11-12T15:22:33.378307: step 51600, loss 0.365712, ranking_acc 0.585938->0.585938, rating_acc 0.590283->0.58518, rate 0.00075\n",
      "2017-11-12T15:22:34.863390: step 51700, loss 0.222071, ranking_acc 0.578125->0.585938, rating_acc 0.455041->0.445853, rate 0.00075\n",
      "2017-11-12T15:22:36.352246: step 51800, loss 0.646572, ranking_acc 0.625->0.625, rating_acc 0.796562->0.789475, rate 0.00075\n",
      "2017-11-12T15:22:37.757118: step 51900, loss 0.298539, ranking_acc 0.398438->0.398438, rating_acc 0.528183->0.524602, rate 0.00075\n",
      "2017-11-12T15:22:39.174896: step 52000, loss 0.576511, ranking_acc 0.335938->0.335938, rating_acc 0.749457->0.743744, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:22:41.102248: step 52000, loss 0.837983, ranking_acc 0.555664, rating_acc 0.902567\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-52000\n",
      "\n",
      "2017-11-12T15:22:42.633752: step 52100, loss 0.774974, ranking_acc 0.453125->0.460938, rating_acc 0.876492->0.866948, rate 0.00075\n",
      "2017-11-12T15:22:44.117903: step 52200, loss 0.30454, ranking_acc 0.4375->0.4375, rating_acc 0.53547->0.530233, rate 0.00075\n",
      "2017-11-12T15:22:45.590921: step 52300, loss 0.665599, ranking_acc 0.554688->0.554688, rating_acc 0.809113->0.801364, rate 0.00075\n",
      "2017-11-12T15:22:47.047318: step 52400, loss 0.271536, ranking_acc 0.351562->0.351562, rating_acc 0.503635->0.4981, rate 0.00075\n",
      "2017-11-12T15:22:48.518099: step 52500, loss 0.218497, ranking_acc 0.609375->0.609375, rating_acc 0.447124->0.441642, rate 0.00075\n",
      "2017-11-12T15:22:49.962886: step 52600, loss 0.358109, ranking_acc 0.546875->0.546875, rating_acc 0.584747->0.578473, rate 0.00075\n",
      "2017-11-12T15:22:51.446606: step 52700, loss 0.250279, ranking_acc 0.664062->0.664062, rating_acc 0.480342->0.476208, rate 0.00075\n",
      "2017-11-12T15:22:52.951064: step 52800, loss 0.215189, ranking_acc 0.632812->0.625, rating_acc 0.449174->0.437806, rate 0.00075\n",
      "2017-11-12T15:22:54.430868: step 52900, loss 0.244543, ranking_acc 0.453125->0.453125, rating_acc 0.475775->0.47011, rate 0.00075\n",
      "2017-11-12T15:22:55.947369: step 53000, loss 0.272254, ranking_acc 0.570312->0.570312, rating_acc 0.50458->0.498688, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:22:57.878387: step 53000, loss 0.834873, ranking_acc 0.571289, rating_acc 0.900726\n",
      "\n",
      "2017-11-12T15:22:59.344645: step 53100, loss 0.33398, ranking_acc 0.671875->0.664062, rating_acc 0.567425->0.557127, rate 0.00075\n",
      "2017-11-12T15:23:00.798377: step 53200, loss 0.318258, ranking_acc 0.609375->0.609375, rating_acc 0.546244->0.542799, rate 0.00075\n",
      "2017-11-12T15:23:02.296341: step 53300, loss 0.27038, ranking_acc 0.59375->0.59375, rating_acc 0.505438->0.496729, rate 0.00075\n",
      "2017-11-12T15:23:03.758023: step 53400, loss 0.156536, ranking_acc 0.585938->0.578125, rating_acc 0.367528->0.364508, rate 0.00075\n",
      "2017-11-12T15:23:07.551723: step 53500, loss 0.283723, ranking_acc 0.460938->0.460938, rating_acc 0.517451->0.509924, rate 0.00075\n",
      "2017-11-12T15:23:09.021262: step 53600, loss 0.313415, ranking_acc 0.476562->0.476562, rating_acc 0.544205->0.538233, rate 0.00075\n",
      "2017-11-12T15:23:10.522883: step 53700, loss 0.182851, ranking_acc 0.625->0.625, rating_acc 0.406619->0.398882, rate 0.00075\n",
      "2017-11-12T15:23:11.988423: step 53800, loss 0.165891, ranking_acc 0.820312->0.820312, rating_acc 0.382398->0.376996, rate 0.00075\n",
      "2017-11-12T15:23:13.470970: step 53900, loss 0.439506, ranking_acc 0.453125->0.46875, rating_acc 0.651659->0.644753, rate 0.00075\n",
      "2017-11-12T15:23:14.967226: step 54000, loss 0.190844, ranking_acc 0.757812->0.757812, rating_acc 0.413415->0.408672, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:23:16.894438: step 54000, loss 0.842288, ranking_acc 0.553711, rating_acc 0.904686\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-54000\n",
      "\n",
      "2017-11-12T15:23:18.524514: step 54100, loss 0.173998, ranking_acc 0.703125->0.710938, rating_acc 0.392818->0.387485, rate 0.00075\n",
      "2017-11-12T15:23:20.001467: step 54200, loss 0.225125, ranking_acc 0.523438->0.523438, rating_acc 0.454648->0.448609, rate 0.00075\n",
      "2017-11-12T15:23:21.484552: step 54300, loss 0.383519, ranking_acc 0.570312->0.570312, rating_acc 0.608582->0.599662, rate 0.00075\n",
      "2017-11-12T15:23:22.963811: step 54400, loss 0.298192, ranking_acc 0.710938->0.703125, rating_acc 0.532475->0.523664, rate 0.00075\n",
      "2017-11-12T15:23:24.451938: step 54500, loss 0.313262, ranking_acc 0.703125->0.703125, rating_acc 0.543854->0.537829, rate 0.00075\n",
      "2017-11-12T15:23:25.911971: step 54600, loss 0.160679, ranking_acc 0.640625->0.648438, rating_acc 0.375377->0.369677, rate 0.00075\n",
      "2017-11-12T15:23:27.413644: step 54700, loss 0.411078, ranking_acc 0.757812->0.757812, rating_acc 0.630829->0.622116, rate 0.00075\n",
      "2017-11-12T15:23:28.851858: step 54800, loss 0.721449, ranking_acc 0.34375->0.34375, rating_acc 0.844131->0.835096, rate 0.00075\n",
      "2017-11-12T15:23:30.308131: step 54900, loss 0.274171, ranking_acc 0.46875->0.46875, rating_acc 0.507097->0.500089, rate 0.00075\n",
      "2017-11-12T15:23:31.806605: step 55000, loss 0.392763, ranking_acc 0.539062->0.539062, rating_acc 0.614925->0.607175, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:23:33.733748: step 55000, loss 0.852871, ranking_acc 0.55957, rating_acc 0.910368\n",
      "\n",
      "2017-11-12T15:23:35.208836: step 55100, loss 0.284985, ranking_acc 0.664062->0.664062, rating_acc 0.517385->0.510741, rate 0.00075\n",
      "2017-11-12T15:23:36.669576: step 55200, loss 0.216041, ranking_acc 0.578125->0.5625, rating_acc 0.445999->0.438054, rate 0.00075\n",
      "2017-11-12T15:23:38.135154: step 55300, loss 0.127234, ranking_acc 0.648438->0.648438, rating_acc 0.327333->0.321036, rate 0.00075\n",
      "2017-11-12T15:23:39.596795: step 55400, loss 0.241254, ranking_acc 0.695312->0.6875, rating_acc 0.471785->0.465885, rate 0.00075\n",
      "2017-11-12T15:23:41.082877: step 55500, loss 0.448789, ranking_acc 0.46875->0.476562, rating_acc 0.658695->0.651591, rate 0.00075\n",
      "2017-11-12T15:23:42.569737: step 55600, loss 0.207477, ranking_acc 0.859375->0.859375, rating_acc 0.432371->0.428063, rate 0.00075\n",
      "2017-11-12T15:23:44.014094: step 55700, loss 0.248436, ranking_acc 0.367188->0.367188, rating_acc 0.477853->0.473466, rate 0.00075\n",
      "2017-11-12T15:23:45.495369: step 55800, loss 0.427231, ranking_acc 0.414062->0.40625, rating_acc 0.646969->0.634781, rate 0.00075\n",
      "2017-11-12T15:23:46.965052: step 55900, loss 0.19132, ranking_acc 0.53125->0.53125, rating_acc 0.415829->0.40868, rate 0.00075\n",
      "2017-11-12T15:23:48.459247: step 56000, loss 0.213576, ranking_acc 0.492188->0.492188, rating_acc 0.43902->0.435042, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:23:50.411773: step 56000, loss 0.851334, ranking_acc 0.558594, rating_acc 0.909406\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-56000\n",
      "\n",
      "2017-11-12T15:23:51.908488: step 56100, loss 0.220911, ranking_acc 0.585938->0.585938, rating_acc 0.451255->0.443361, rate 0.00075\n",
      "2017-11-12T15:23:53.337010: step 56200, loss 0.24047, ranking_acc 0.53125->0.546875, rating_acc 0.473606->0.464882, rate 0.00075\n",
      "2017-11-12T15:23:54.748299: step 56300, loss 0.244787, ranking_acc 0.585938->0.585938, rating_acc 0.475115->0.469476, rate 0.00075\n",
      "2017-11-12T15:23:56.246527: step 56400, loss 0.158355, ranking_acc 0.5625->0.5625, rating_acc 0.369217->0.365985, rate 0.00075\n",
      "2017-11-12T15:23:57.642325: step 56500, loss 0.265032, ranking_acc 0.507812->0.507812, rating_acc 0.495245->0.490518, rate 0.00075\n",
      "2017-11-12T15:23:59.105919: step 56600, loss 0.176346, ranking_acc 0.585938->0.585938, rating_acc 0.39347->0.389745, rate 0.00075\n",
      "2017-11-12T15:24:00.546953: step 56700, loss 0.299532, ranking_acc 0.585938->0.585938, rating_acc 0.532012->0.52447, rate 0.00075\n",
      "2017-11-12T15:24:02.008295: step 56800, loss 0.2048, ranking_acc 0.351562->0.351562, rating_acc 0.431402->0.42464, rate 0.00075\n",
      "2017-11-12T15:24:03.421361: step 56900, loss 0.24512, ranking_acc 0.554688->0.539062, rating_acc 0.475658->0.469693, rate 0.00075\n",
      "2017-11-12T15:24:04.861235: step 57000, loss 0.552633, ranking_acc 0.578125->0.585938, rating_acc 0.736992->0.726708, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:24:06.810690: step 57000, loss 0.853984, ranking_acc 0.568359, rating_acc 0.910744\n",
      "\n",
      "2017-11-12T15:24:08.267619: step 57100, loss 0.31981, ranking_acc 0.546875->0.546875, rating_acc 0.549574->0.543379, rate 0.00075\n",
      "2017-11-12T15:24:09.784175: step 57200, loss 0.178899, ranking_acc 0.507812->0.507812, rating_acc 0.404122->0.39284, rate 0.00075\n",
      "2017-11-12T15:24:11.267492: step 57300, loss 0.218411, ranking_acc 0.4375->0.4375, rating_acc 0.447239->0.440229, rate 0.00075\n",
      "2017-11-12T15:24:12.786400: step 57400, loss 0.543863, ranking_acc 0.695312->0.695312, rating_acc 0.72997->0.720584, rate 0.00075\n",
      "2017-11-12T15:24:14.285689: step 57500, loss 0.303146, ranking_acc 0.5->0.515625, rating_acc 0.536908->0.527732, rate 0.00075\n",
      "2017-11-12T15:24:15.774882: step 57600, loss 0.324714, ranking_acc 0.695312->0.695312, rating_acc 0.554091->0.54775, rate 0.00075\n",
      "2017-11-12T15:24:17.294906: step 57700, loss 0.585379, ranking_acc 0.570312->0.570312, rating_acc 0.75773->0.748773, rate 0.00075\n",
      "2017-11-12T15:24:18.779498: step 57800, loss 0.29716, ranking_acc 0.664062->0.664062, rating_acc 0.531485->0.521925, rate 0.00075\n",
      "2017-11-12T15:24:20.238612: step 57900, loss 0.123142, ranking_acc 0.6875->0.6875, rating_acc 0.317533->0.313583, rate 0.00075\n",
      "2017-11-12T15:24:21.730590: step 58000, loss 0.319322, ranking_acc 0.507812->0.5, rating_acc 0.550174->0.542666, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:24:23.698475: step 58000, loss 0.850609, ranking_acc 0.56543, rating_acc 0.90872\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-58000\n",
      "\n",
      "2017-11-12T15:24:25.290071: step 58100, loss 0.209681, ranking_acc 0.640625->0.640625, rating_acc 0.435431->0.429906, rate 0.00075\n",
      "2017-11-12T15:24:26.756672: step 58200, loss 0.385425, ranking_acc 0.414062->0.414062, rating_acc 0.60677->0.600456, rate 0.00075\n",
      "2017-11-12T15:24:28.260682: step 58300, loss 0.283696, ranking_acc 0.414062->0.421875, rating_acc 0.518841->0.508712, rate 0.00075\n",
      "2017-11-12T15:24:29.813166: step 58400, loss 0.314583, ranking_acc 0.414062->0.421875, rating_acc 0.54225->0.538191, rate 0.00075\n",
      "2017-11-12T15:24:31.274686: step 58500, loss 0.18653, ranking_acc 0.78125->0.789062, rating_acc 0.406388->0.401955, rate 0.00075\n",
      "2017-11-12T15:24:32.732783: step 58600, loss 0.253438, ranking_acc 0.460938->0.460938, rating_acc 0.484735->0.477958, rate 0.00075\n",
      "2017-11-12T15:24:34.264541: step 58700, loss 0.276674, ranking_acc 0.5->0.5, rating_acc 0.505172->0.501655, rate 0.00075\n",
      "2017-11-12T15:24:35.790262: step 58800, loss 0.192576, ranking_acc 0.5625->0.570312, rating_acc 0.417963->0.409318, rate 0.00075\n",
      "2017-11-12T15:24:37.304421: step 58900, loss 0.205341, ranking_acc 0.679688->0.679688, rating_acc 0.433037->0.424605, rate 0.00075\n",
      "2017-11-12T15:24:38.824671: step 59000, loss 0.392987, ranking_acc 0.390625->0.390625, rating_acc 0.613727->0.606559, rate 0.00075\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:24:40.765349: step 59000, loss 0.851809, ranking_acc 0.561523, rating_acc 0.90925\n",
      "\n",
      "2017-11-12T15:24:42.270169: step 59100, loss 0.448673, ranking_acc 0.4375->0.453125, rating_acc 0.660189->0.650829, rate 0.00075\n",
      "2017-11-12T15:24:43.757936: step 59200, loss 0.388528, ranking_acc 0.640625->0.640625, rating_acc 0.614584->0.602845, rate 0.00075\n",
      "2017-11-12T15:24:45.244701: step 59300, loss 0.259536, ranking_acc 0.59375->0.59375, rating_acc 0.489355->0.484167, rate 0.00075\n",
      "2017-11-12T15:24:46.734935: step 59400, loss 0.276864, ranking_acc 0.75->0.75, rating_acc 0.507846->0.501729, rate 0.00075\n",
      "2017-11-12T15:24:48.241386: step 59500, loss 0.334366, ranking_acc 0.648438->0.648438, rating_acc 0.561532->0.556064, rate 0.00075\n",
      "2017-11-12T15:24:49.750269: step 59600, loss 0.274912, ranking_acc 0.664062->0.664062, rating_acc 0.506162->0.499726, rate 0.00075\n",
      "2017-11-12T15:24:51.225928: step 59700, loss 0.394284, ranking_acc 0.601562->0.617188, rating_acc 0.617239->0.607521, rate 0.00075\n",
      "2017-11-12T15:24:52.731134: step 59800, loss 0.212685, ranking_acc 0.507812->0.507812, rating_acc 0.441156->0.432968, rate 0.00075\n",
      "2017-11-12T15:24:54.220710: step 59900, loss 0.273804, ranking_acc 0.578125->0.578125, rating_acc 0.507805->0.498555, rate 0.00075\n",
      "2017-11-12T15:24:55.668231: step 60000, loss 0.525662, ranking_acc 0.625->0.625, rating_acc 0.714587->0.707391, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:24:57.602466: step 60000, loss 0.851006, ranking_acc 0.570312, rating_acc 0.908705\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-60000\n",
      "\n",
      "2017-11-12T15:24:59.162357: step 60100, loss 0.449534, ranking_acc 0.5625->0.5625, rating_acc 0.655236->0.651354, rate 0.000375\n",
      "2017-11-12T15:25:00.596340: step 60200, loss 0.189812, ranking_acc 0.539062->0.546875, rating_acc 0.408799->0.405626, rate 0.000375\n",
      "2017-11-12T15:25:02.063311: step 60300, loss 0.431546, ranking_acc 0.632812->0.632812, rating_acc 0.641455->0.637383, rate 0.000375\n",
      "2017-11-12T15:25:03.537412: step 60400, loss 0.2266, ranking_acc 0.734375->0.734375, rating_acc 0.452838->0.448672, rate 0.000375\n",
      "2017-11-12T15:25:05.002449: step 60500, loss 0.459593, ranking_acc 0.429688->0.4375, rating_acc 0.663303->0.65901, rate 0.000375\n",
      "2017-11-12T15:25:06.437368: step 60600, loss 0.333928, ranking_acc 0.570312->0.570312, rating_acc 0.560179->0.555533, rate 0.000375\n",
      "2017-11-12T15:25:07.824115: step 60700, loss 0.301408, ranking_acc 0.53125->0.53125, rating_acc 0.528643->0.525443, rate 0.000375\n",
      "2017-11-12T15:25:09.194603: step 60800, loss 0.273339, ranking_acc 0.390625->0.390625, rating_acc 0.501778->0.498003, rate 0.000375\n",
      "2017-11-12T15:25:10.667912: step 60900, loss 0.362159, ranking_acc 0.65625->0.65625, rating_acc 0.584762->0.580363, rate 0.000375\n",
      "2017-11-12T15:25:12.121122: step 61000, loss 0.277632, ranking_acc 0.539062->0.539062, rating_acc 0.508006->0.502281, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:25:14.051919: step 61000, loss 0.849188, ranking_acc 0.556641, rating_acc 0.907658\n",
      "\n",
      "2017-11-12T15:25:17.840545: step 61100, loss 0.213204, ranking_acc 0.585938->0.585938, rating_acc 0.4382->0.433411, rate 0.000375\n",
      "2017-11-12T15:25:19.230862: step 61200, loss 0.49402, ranking_acc 0.351562->0.351562, rating_acc 0.687729->0.684582, rate 0.000375\n",
      "2017-11-12T15:25:20.659328: step 61300, loss 0.373461, ranking_acc 0.578125->0.578125, rating_acc 0.594916->0.589988, rate 0.000375\n",
      "2017-11-12T15:25:22.112993: step 61400, loss 0.269773, ranking_acc 0.695312->0.695312, rating_acc 0.497071->0.494359, rate 0.000375\n",
      "2017-11-12T15:25:23.624191: step 61500, loss 0.310522, ranking_acc 0.421875->0.414062, rating_acc 0.536456->0.533974, rate 0.000375\n",
      "2017-11-12T15:25:24.979378: step 61600, loss 0.347999, ranking_acc 0.476562->0.492188, rating_acc 0.570993->0.567974, rate 0.000375\n",
      "2017-11-12T15:25:26.425922: step 61700, loss 0.308207, ranking_acc 0.484375->0.484375, rating_acc 0.535247->0.531782, rate 0.000375\n",
      "2017-11-12T15:25:27.762110: step 61800, loss 0.336978, ranking_acc 0.6875->0.6875, rating_acc 0.563683->0.558175, rate 0.000375\n",
      "2017-11-12T15:25:29.154149: step 61900, loss 0.307666, ranking_acc 0.492188->0.492188, rating_acc 0.534508->0.531246, rate 0.000375\n",
      "2017-11-12T15:25:30.528650: step 62000, loss 0.215119, ranking_acc 0.515625->0.515625, rating_acc 0.437797->0.435503, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:25:32.446599: step 62000, loss 0.850542, ranking_acc 0.579102, rating_acc 0.908342\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-62000\n",
      "\n",
      "2017-11-12T15:25:33.963406: step 62100, loss 0.24888, ranking_acc 0.570312->0.570312, rating_acc 0.477248->0.472661, rate 0.000375\n",
      "2017-11-12T15:25:35.380231: step 62200, loss 0.550382, ranking_acc 0.421875->0.414062, rating_acc 0.727244->0.724503, rate 0.000375\n",
      "2017-11-12T15:25:36.760092: step 62300, loss 0.414275, ranking_acc 0.640625->0.640625, rating_acc 0.6289->0.62353, rate 0.000375\n",
      "2017-11-12T15:25:38.198140: step 62400, loss 0.360436, ranking_acc 0.640625->0.648438, rating_acc 0.583945->0.578743, rate 0.000375\n",
      "2017-11-12T15:25:39.636637: step 62500, loss 0.160232, ranking_acc 0.570312->0.570312, rating_acc 0.369774->0.367063, rate 0.000375\n",
      "2017-11-12T15:25:41.053248: step 62600, loss 0.369199, ranking_acc 0.546875->0.546875, rating_acc 0.58866->0.58626, rate 0.000375\n",
      "2017-11-12T15:25:42.495837: step 62700, loss 0.15275, ranking_acc 0.601562->0.601562, rating_acc 0.360649->0.356712, rate 0.000375\n",
      "2017-11-12T15:25:43.960017: step 62800, loss 0.244661, ranking_acc 0.492188->0.492188, rating_acc 0.472397->0.468133, rate 0.000375\n",
      "2017-11-12T15:25:45.366786: step 62900, loss 0.44458, ranking_acc 0.585938->0.570312, rating_acc 0.64981->0.64735, rate 0.000375\n",
      "2017-11-12T15:25:46.827654: step 63000, loss 0.194292, ranking_acc 0.648438->0.648438, rating_acc 0.414071->0.410808, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:25:48.750266: step 63000, loss 0.851537, ranking_acc 0.540039, rating_acc 0.90885\n",
      "\n",
      "2017-11-12T15:25:50.184734: step 63100, loss 0.478288, ranking_acc 0.5625->0.5625, rating_acc 0.676492->0.672864, rate 0.000375\n",
      "2017-11-12T15:25:51.629884: step 63200, loss 0.201362, ranking_acc 0.742188->0.742188, rating_acc 0.421706->0.419308, rate 0.000375\n",
      "2017-11-12T15:25:53.060480: step 63300, loss 0.329298, ranking_acc 0.703125->0.703125, rating_acc 0.55466->0.55113, rate 0.000375\n",
      "2017-11-12T15:25:54.513146: step 63400, loss 0.272636, ranking_acc 0.585938->0.585938, rating_acc 0.499918->0.497069, rate 0.000375\n",
      "2017-11-12T15:25:55.928028: step 63500, loss 0.314792, ranking_acc 0.640625->0.640625, rating_acc 0.541284->0.537801, rate 0.000375\n",
      "2017-11-12T15:25:57.355928: step 63600, loss 0.338623, ranking_acc 0.59375->0.59375, rating_acc 0.56434->0.559515, rate 0.000375\n",
      "2017-11-12T15:25:58.766803: step 63700, loss 0.163234, ranking_acc 0.664062->0.664062, rating_acc 0.375048->0.371023, rate 0.000375\n",
      "2017-11-12T15:26:00.164585: step 63800, loss 0.242436, ranking_acc 0.664062->0.664062, rating_acc 0.470085->0.465681, rate 0.000375\n",
      "2017-11-12T15:26:01.640814: step 63900, loss 0.2633, ranking_acc 0.5->0.5, rating_acc 0.489282->0.487557, rate 0.000375\n",
      "2017-11-12T15:26:03.145833: step 64000, loss 0.272766, ranking_acc 0.609375->0.609375, rating_acc 0.502075->0.497161, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:26:05.066777: step 64000, loss 0.851205, ranking_acc 0.578125, rating_acc 0.90863\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-64000\n",
      "\n",
      "2017-11-12T15:26:06.667863: step 64100, loss 0.46399, ranking_acc 0.578125->0.570312, rating_acc 0.666341->0.66211, rate 0.000375\n",
      "2017-11-12T15:26:08.116975: step 64200, loss 0.245147, ranking_acc 0.523438->0.53125, rating_acc 0.474163->0.468552, rate 0.000375\n",
      "2017-11-12T15:26:09.606103: step 64300, loss 0.316473, ranking_acc 0.585938->0.585938, rating_acc 0.541135->0.539314, rate 0.000375\n",
      "2017-11-12T15:26:11.069595: step 64400, loss 0.207975, ranking_acc 0.515625->0.515625, rating_acc 0.429528->0.427039, rate 0.000375\n",
      "2017-11-12T15:26:12.571287: step 64500, loss 0.382843, ranking_acc 0.664062->0.664062, rating_acc 0.600857->0.597675, rate 0.000375\n",
      "2017-11-12T15:26:14.017979: step 64600, loss 0.162841, ranking_acc 0.507812->0.507812, rating_acc 0.372431->0.370414, rate 0.000375\n",
      "2017-11-12T15:26:15.477123: step 64700, loss 0.385298, ranking_acc 0.679688->0.679688, rating_acc 0.602722->0.59972, rate 0.000375\n",
      "2017-11-12T15:26:16.988175: step 64800, loss 0.285859, ranking_acc 0.789062->0.796875, rating_acc 0.513609->0.510116, rate 0.000375\n",
      "2017-11-12T15:26:18.437612: step 64900, loss 0.273562, ranking_acc 0.429688->0.429688, rating_acc 0.501458->0.497906, rate 0.000375\n",
      "2017-11-12T15:26:19.905694: step 65000, loss 0.244828, ranking_acc 0.46875->0.46875, rating_acc 0.470897->0.468158, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:26:21.821807: step 65000, loss 0.85111, ranking_acc 0.569336, rating_acc 0.908545\n",
      "\n",
      "2017-11-12T15:26:23.318941: step 65100, loss 0.23509, ranking_acc 0.585938->0.59375, rating_acc 0.460173->0.457633, rate 0.000375\n",
      "2017-11-12T15:26:24.818566: step 65200, loss 0.493498, ranking_acc 0.523438->0.523438, rating_acc 0.688984->0.683979, rate 0.000375\n",
      "2017-11-12T15:26:26.294874: step 65300, loss 0.249183, ranking_acc 0.679688->0.679688, rating_acc 0.475931->0.472758, rate 0.000375\n",
      "2017-11-12T15:26:27.762309: step 65400, loss 0.279387, ranking_acc 0.59375->0.59375, rating_acc 0.506298->0.503679, rate 0.000375\n",
      "2017-11-12T15:26:29.233641: step 65500, loss 0.19234, ranking_acc 0.445312->0.445312, rating_acc 0.411513->0.408202, rate 0.000375\n",
      "2017-11-12T15:26:30.725831: step 65600, loss 0.369666, ranking_acc 0.539062->0.539062, rating_acc 0.590899->0.586469, rate 0.000375\n",
      "2017-11-12T15:26:32.138741: step 65700, loss 0.278124, ranking_acc 0.59375->0.609375, rating_acc 0.50679->0.502386, rate 0.000375\n",
      "2017-11-12T15:26:33.479803: step 65800, loss 0.364794, ranking_acc 0.625->0.625, rating_acc 0.585044->0.582287, rate 0.000375\n",
      "2017-11-12T15:26:34.925696: step 65900, loss 0.251262, ranking_acc 0.523438->0.523438, rating_acc 0.480414->0.474886, rate 0.000375\n",
      "2017-11-12T15:26:36.447796: step 66000, loss 0.238631, ranking_acc 0.726562->0.726562, rating_acc 0.465272->0.461385, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:26:38.357670: step 66000, loss 0.849518, ranking_acc 0.558594, rating_acc 0.907614\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-66000\n",
      "\n",
      "2017-11-12T15:26:39.903945: step 66100, loss 0.154017, ranking_acc 0.523438->0.523438, rating_acc 0.36112->0.358125, rate 0.000375\n",
      "2017-11-12T15:26:41.395887: step 66200, loss 0.376376, ranking_acc 0.5625->0.5625, rating_acc 0.59706->0.592121, rate 0.000375\n",
      "2017-11-12T15:26:42.888889: step 66300, loss 0.607079, ranking_acc 0.445312->0.445312, rating_acc 0.766038->0.762432, rate 0.000375\n",
      "2017-11-12T15:26:44.373787: step 66400, loss 0.318786, ranking_acc 0.632812->0.632812, rating_acc 0.545068->0.541298, rate 0.000375\n",
      "2017-11-12T15:26:45.836431: step 66500, loss 0.163282, ranking_acc 0.78125->0.78125, rating_acc 0.374158->0.370803, rate 0.000375\n",
      "2017-11-12T15:26:47.325381: step 66600, loss 0.287591, ranking_acc 0.414062->0.414062, rating_acc 0.514041->0.511663, rate 0.000375\n",
      "2017-11-12T15:26:48.823944: step 66700, loss 0.269185, ranking_acc 0.59375->0.59375, rating_acc 0.497556->0.493344, rate 0.000375\n",
      "2017-11-12T15:26:50.298449: step 66800, loss 0.327632, ranking_acc 0.46875->0.46875, rating_acc 0.551762->0.549393, rate 0.000375\n",
      "2017-11-12T15:26:51.736946: step 66900, loss 0.289381, ranking_acc 0.554688->0.554688, rating_acc 0.51666->0.513398, rate 0.000375\n",
      "2017-11-12T15:26:53.229301: step 67000, loss 0.218742, ranking_acc 0.65625->0.65625, rating_acc 0.442187->0.439243, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:26:55.149244: step 67000, loss 0.848961, ranking_acc 0.569336, rating_acc 0.907278\n",
      "\n",
      "2017-11-12T15:26:56.643713: step 67100, loss 0.46096, ranking_acc 0.414062->0.414062, rating_acc 0.664935->0.659655, rate 0.000375\n",
      "2017-11-12T15:26:58.119359: step 67200, loss 0.204212, ranking_acc 0.585938->0.585938, rating_acc 0.424661->0.422357, rate 0.000375\n",
      "2017-11-12T15:26:59.629120: step 67300, loss 0.190761, ranking_acc 0.546875->0.546875, rating_acc 0.40878->0.406115, rate 0.000375\n",
      "2017-11-12T15:27:01.094781: step 67400, loss 0.154106, ranking_acc 0.742188->0.742188, rating_acc 0.360316->0.35815, rate 0.000375\n",
      "2017-11-12T15:27:02.546019: step 67500, loss 0.637773, ranking_acc 0.523438->0.523438, rating_acc 0.787225->0.78226, rate 0.000375\n",
      "2017-11-12T15:27:04.049527: step 67600, loss 0.318341, ranking_acc 0.5->0.507812, rating_acc 0.544401->0.540827, rate 0.000375\n",
      "2017-11-12T15:27:05.432206: step 67700, loss 0.181436, ranking_acc 0.726562->0.726562, rating_acc 0.39698->0.394437, rate 0.000375\n",
      "2017-11-12T15:27:06.856665: step 67800, loss 0.528788, ranking_acc 0.703125->0.703125, rating_acc 0.712983->0.709172, rate 0.000375\n",
      "2017-11-12T15:27:08.333815: step 67900, loss 0.175453, ranking_acc 0.585938->0.585938, rating_acc 0.388641->0.386755, rate 0.000375\n",
      "2017-11-12T15:27:09.780541: step 68000, loss 0.53728, ranking_acc 0.460938->0.460938, rating_acc 0.717948->0.715121, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:27:11.718121: step 68000, loss 0.847259, ranking_acc 0.557617, rating_acc 0.906298\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-68000\n",
      "\n",
      "2017-11-12T15:27:13.299585: step 68100, loss 0.423029, ranking_acc 0.484375->0.476562, rating_acc 0.633915->0.630195, rate 0.000375\n",
      "2017-11-12T15:27:14.761859: step 68200, loss 0.495117, ranking_acc 0.664062->0.664062, rating_acc 0.690998->0.685, rate 0.000375\n",
      "2017-11-12T15:27:16.150951: step 68300, loss 0.295154, ranking_acc 0.398438->0.398438, rating_acc 0.522785->0.518894, rate 0.000375\n",
      "2017-11-12T15:27:17.592286: step 68400, loss 0.232893, ranking_acc 0.632812->0.625, rating_acc 0.456826->0.454945, rate 0.000375\n",
      "2017-11-12T15:27:19.076599: step 68500, loss 0.219525, ranking_acc 0.703125->0.710938, rating_acc 0.443118->0.439999, rate 0.000375\n",
      "2017-11-12T15:27:20.525260: step 68600, loss 0.414217, ranking_acc 0.6875->0.6875, rating_acc 0.625392->0.623128, rate 0.000375\n",
      "2017-11-12T15:27:24.286095: step 68700, loss 0.289013, ranking_acc 0.742188->0.742188, rating_acc 0.515284->0.512902, rate 0.000375\n",
      "2017-11-12T15:27:25.746939: step 68800, loss 0.404827, ranking_acc 0.476562->0.484375, rating_acc 0.619846->0.615527, rate 0.000375\n",
      "2017-11-12T15:27:27.180122: step 68900, loss 0.164111, ranking_acc 0.648438->0.648438, rating_acc 0.37418->0.371684, rate 0.000375\n",
      "2017-11-12T15:27:28.619556: step 69000, loss 0.26617, ranking_acc 0.460938->0.460938, rating_acc 0.495199->0.490105, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:27:30.544773: step 69000, loss 0.848126, ranking_acc 0.561523, rating_acc 0.90673\n",
      "\n",
      "2017-11-12T15:27:32.026299: step 69100, loss 0.501973, ranking_acc 0.523438->0.523438, rating_acc 0.693517->0.689922, rate 0.000375\n",
      "2017-11-12T15:27:33.466058: step 69200, loss 0.194441, ranking_acc 0.421875->0.421875, rating_acc 0.412084->0.410425, rate 0.000375\n",
      "2017-11-12T15:27:34.924084: step 69300, loss 0.215933, ranking_acc 0.296875->0.304688, rating_acc 0.440192->0.435809, rate 0.000375\n",
      "2017-11-12T15:27:36.348389: step 69400, loss 0.178632, ranking_acc 0.453125->0.453125, rating_acc 0.392654->0.390671, rate 0.000375\n",
      "2017-11-12T15:27:37.783909: step 69500, loss 0.35045, ranking_acc 0.640625->0.640625, rating_acc 0.572122->0.569588, rate 0.000375\n",
      "2017-11-12T15:27:39.230306: step 69600, loss 0.222686, ranking_acc 0.75->0.75, rating_acc 0.446496->0.443443, rate 0.000375\n",
      "2017-11-12T15:27:40.708448: step 69700, loss 0.174484, ranking_acc 0.65625->0.65625, rating_acc 0.38764->0.385255, rate 0.000375\n",
      "2017-11-12T15:27:42.165367: step 69800, loss 0.179601, ranking_acc 0.390625->0.390625, rating_acc 0.393596->0.391824, rate 0.000375\n",
      "2017-11-12T15:27:43.595984: step 69900, loss 0.489648, ranking_acc 0.375->0.375, rating_acc 0.686071->0.680859, rate 0.000375\n",
      "2017-11-12T15:27:45.060866: step 70000, loss 0.473188, ranking_acc 0.65625->0.65625, rating_acc 0.671361->0.668653, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:27:46.981669: step 70000, loss 0.854968, ranking_acc 0.551758, rating_acc 0.910427\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-70000\n",
      "\n",
      "2017-11-12T15:27:48.508159: step 70100, loss 0.234662, ranking_acc 0.5625->0.5625, rating_acc 0.459314->0.456693, rate 0.000375\n",
      "2017-11-12T15:27:49.926415: step 70200, loss 0.279873, ranking_acc 0.609375->0.609375, rating_acc 0.5071->0.503762, rate 0.000375\n",
      "2017-11-12T15:27:51.377938: step 70300, loss 0.228824, ranking_acc 0.484375->0.484375, rating_acc 0.45682->0.450246, rate 0.000375\n",
      "2017-11-12T15:27:52.850825: step 70400, loss 0.34282, ranking_acc 0.476562->0.484375, rating_acc 0.56674->0.562767, rate 0.000375\n",
      "2017-11-12T15:27:54.296152: step 70500, loss 0.195365, ranking_acc 0.625->0.625, rating_acc 0.413949->0.411397, rate 0.000375\n",
      "2017-11-12T15:27:55.750012: step 70600, loss 0.243563, ranking_acc 0.664062->0.65625, rating_acc 0.467906->0.466298, rate 0.000375\n",
      "2017-11-12T15:27:57.194230: step 70700, loss 0.135256, ranking_acc 0.5->0.492188, rating_acc 0.333918->0.33032, rate 0.000375\n",
      "2017-11-12T15:27:58.651338: step 70800, loss 0.317507, ranking_acc 0.585938->0.59375, rating_acc 0.543814->0.53978, rate 0.000375\n",
      "2017-11-12T15:28:00.115668: step 70900, loss 0.281544, ranking_acc 0.53125->0.53125, rating_acc 0.507419->0.505359, rate 0.000375\n",
      "2017-11-12T15:28:01.567960: step 71000, loss 0.30708, ranking_acc 0.460938->0.460938, rating_acc 0.534675->0.530015, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:28:03.476741: step 71000, loss 0.853731, ranking_acc 0.561523, rating_acc 0.909707\n",
      "\n",
      "2017-11-12T15:28:04.925132: step 71100, loss 0.355665, ranking_acc 0.625->0.625, rating_acc 0.578179->0.574017, rate 0.000375\n",
      "2017-11-12T15:28:06.364185: step 71200, loss 0.45004, ranking_acc 0.492188->0.492188, rating_acc 0.654479->0.651049, rate 0.000375\n",
      "2017-11-12T15:28:07.812778: step 71300, loss 0.396265, ranking_acc 0.65625->0.65625, rating_acc 0.613907->0.608344, rate 0.000375\n",
      "2017-11-12T15:28:09.281285: step 71400, loss 0.262032, ranking_acc 0.601562->0.601562, rating_acc 0.489573->0.485638, rate 0.000375\n",
      "2017-11-12T15:28:10.763036: step 71500, loss 0.377317, ranking_acc 0.664062->0.664062, rating_acc 0.595363->0.592553, rate 0.000375\n",
      "2017-11-12T15:28:12.222212: step 71600, loss 0.209481, ranking_acc 0.765625->0.765625, rating_acc 0.429921->0.428107, rate 0.000375\n",
      "2017-11-12T15:28:13.672283: step 71700, loss 0.200863, ranking_acc 0.609375->0.609375, rating_acc 0.422179->0.417911, rate 0.000375\n",
      "2017-11-12T15:28:15.137106: step 71800, loss 0.271828, ranking_acc 0.789062->0.789062, rating_acc 0.498625->0.495589, rate 0.000375\n",
      "2017-11-12T15:28:16.610460: step 71900, loss 0.293538, ranking_acc 0.445312->0.445312, rating_acc 0.521098->0.517018, rate 0.000375\n",
      "2017-11-12T15:28:18.068391: step 72000, loss 0.243851, ranking_acc 0.601562->0.601562, rating_acc 0.468244->0.466495, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:28:19.987620: step 72000, loss 0.854287, ranking_acc 0.579102, rating_acc 0.909974\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-72000\n",
      "\n",
      "2017-11-12T15:28:21.508507: step 72100, loss 0.382061, ranking_acc 0.640625->0.640625, rating_acc 0.600228->0.596504, rate 0.000375\n",
      "2017-11-12T15:28:22.964677: step 72200, loss 0.26906, ranking_acc 0.492188->0.5, rating_acc 0.494887->0.492752, rate 0.000375\n",
      "2017-11-12T15:28:24.391602: step 72300, loss 0.167922, ranking_acc 0.710938->0.710938, rating_acc 0.378106->0.376384, rate 0.000375\n",
      "2017-11-12T15:28:25.844255: step 72400, loss 0.230086, ranking_acc 0.484375->0.476562, rating_acc 0.454504->0.451463, rate 0.000375\n",
      "2017-11-12T15:28:27.326328: step 72500, loss 0.162038, ranking_acc 0.5625->0.5625, rating_acc 0.372627->0.368454, rate 0.000375\n",
      "2017-11-12T15:28:28.759028: step 72600, loss 0.283918, ranking_acc 0.671875->0.671875, rating_acc 0.510708->0.50757, rate 0.000375\n",
      "2017-11-12T15:28:30.208146: step 72700, loss 0.283652, ranking_acc 0.554688->0.554688, rating_acc 0.511673->0.507305, rate 0.000375\n",
      "2017-11-12T15:28:31.676453: step 72800, loss 0.188644, ranking_acc 0.601562->0.601562, rating_acc 0.408368->0.402916, rate 0.000375\n",
      "2017-11-12T15:28:33.102066: step 72900, loss 0.150275, ranking_acc 0.65625->0.65625, rating_acc 0.357847->0.352069, rate 0.000375\n",
      "2017-11-12T15:28:34.514911: step 73000, loss 0.285628, ranking_acc 0.367188->0.367188, rating_acc 0.514799->0.509211, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:28:36.428571: step 73000, loss 0.854115, ranking_acc 0.548828, rating_acc 0.909826\n",
      "\n",
      "2017-11-12T15:28:37.883815: step 73100, loss 0.29339, ranking_acc 0.46875->0.46875, rating_acc 0.520809->0.516754, rate 0.000375\n",
      "2017-11-12T15:28:39.340613: step 73200, loss 0.162103, ranking_acc 0.703125->0.695312, rating_acc 0.373463->0.368423, rate 0.000375\n",
      "2017-11-12T15:28:40.814785: step 73300, loss 0.210534, ranking_acc 0.734375->0.734375, rating_acc 0.434661->0.42913, rate 0.000375\n",
      "2017-11-12T15:28:42.284190: step 73400, loss 0.222068, ranking_acc 0.460938->0.460938, rating_acc 0.445224->0.442354, rate 0.000375\n",
      "2017-11-12T15:28:43.749229: step 73500, loss 0.408155, ranking_acc 0.507812->0.507812, rating_acc 0.623363->0.617866, rate 0.000375\n",
      "2017-11-12T15:28:45.232064: step 73600, loss 0.373902, ranking_acc 0.5->0.5, rating_acc 0.594803->0.589482, rate 0.000375\n",
      "2017-11-12T15:28:46.750297: step 73700, loss 0.220695, ranking_acc 0.5->0.5, rating_acc 0.448131->0.440768, rate 0.000375\n",
      "2017-11-12T15:28:48.215172: step 73800, loss 0.170895, ranking_acc 0.476562->0.476562, rating_acc 0.385015->0.380086, rate 0.000375\n",
      "2017-11-12T15:28:49.663388: step 73900, loss 0.223222, ranking_acc 0.554688->0.554688, rating_acc 0.445292->0.443596, rate 0.000375\n",
      "2017-11-12T15:28:51.103601: step 74000, loss 0.295508, ranking_acc 0.664062->0.664062, rating_acc 0.523495->0.518707, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:28:53.161269: step 74000, loss 0.854102, ranking_acc 0.552734, rating_acc 0.909753\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-74000\n",
      "\n",
      "2017-11-12T15:28:54.710243: step 74100, loss 0.258419, ranking_acc 0.578125->0.578125, rating_acc 0.484058->0.481622, rate 0.000375\n",
      "2017-11-12T15:28:56.174682: step 74200, loss 0.337006, ranking_acc 0.65625->0.648438, rating_acc 0.560151->0.557259, rate 0.000375\n",
      "2017-11-12T15:28:57.612894: step 74300, loss 0.257072, ranking_acc 0.523438->0.523438, rating_acc 0.483663->0.480205, rate 0.000375\n",
      "2017-11-12T15:28:59.049365: step 74400, loss 0.491471, ranking_acc 0.734375->0.734375, rating_acc 0.686455->0.681901, rate 0.000375\n",
      "2017-11-12T15:29:00.510511: step 74500, loss 0.333183, ranking_acc 0.671875->0.671875, rating_acc 0.557035->0.553798, rate 0.000375\n",
      "2017-11-12T15:29:01.988979: step 74600, loss 0.313518, ranking_acc 0.585938->0.585938, rating_acc 0.538456->0.535747, rate 0.000375\n",
      "2017-11-12T15:29:03.449283: step 74700, loss 0.513465, ranking_acc 0.679688->0.679688, rating_acc 0.701213->0.69783, rate 0.000375\n",
      "2017-11-12T15:29:04.882048: step 74800, loss 0.24122, ranking_acc 0.5->0.5, rating_acc 0.465687->0.463366, rate 0.000375\n",
      "2017-11-12T15:29:06.372458: step 74900, loss 0.450463, ranking_acc 0.515625->0.515625, rating_acc 0.656421->0.651107, rate 0.000375\n",
      "2017-11-12T15:29:07.809263: step 75000, loss 0.550738, ranking_acc 0.554688->0.554688, rating_acc 0.728105->0.724022, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:29:09.744056: step 75000, loss 0.852562, ranking_acc 0.551758, rating_acc 0.908863\n",
      "\n",
      "2017-11-12T15:29:11.208237: step 75100, loss 0.15925, ranking_acc 0.648438->0.648438, rating_acc 0.370973->0.364295, rate 0.000375\n",
      "2017-11-12T15:29:12.647815: step 75200, loss 0.319926, ranking_acc 0.609375->0.609375, rating_acc 0.545433->0.541643, rate 0.000375\n",
      "2017-11-12T15:29:14.103756: step 75300, loss 0.369815, ranking_acc 0.453125->0.460938, rating_acc 0.590957->0.585884, rate 0.000375\n",
      "2017-11-12T15:29:15.551978: step 75400, loss 0.487152, ranking_acc 0.5625->0.5625, rating_acc 0.683546->0.678668, rate 0.000375\n",
      "2017-11-12T15:29:17.004030: step 75500, loss 0.473864, ranking_acc 0.429688->0.421875, rating_acc 0.673646->0.668799, rate 0.000375\n",
      "2017-11-12T15:29:18.456837: step 75600, loss 0.286835, ranking_acc 0.492188->0.492188, rating_acc 0.512741->0.51015, rate 0.000375\n",
      "2017-11-12T15:29:19.895837: step 75700, loss 0.184673, ranking_acc 0.5625->0.5625, rating_acc 0.402525->0.397604, rate 0.000375\n",
      "2017-11-12T15:29:21.367510: step 75800, loss 0.279187, ranking_acc 0.640625->0.640625, rating_acc 0.506019->0.502586, rate 0.000375\n",
      "2017-11-12T15:29:22.786111: step 75900, loss 0.250097, ranking_acc 0.765625->0.765625, rating_acc 0.476758->0.472751, rate 0.000375\n",
      "2017-11-12T15:29:24.233196: step 76000, loss 0.929582, ranking_acc 0.539062->0.539062, rating_acc 0.955781->0.950247, rate 0.000375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-12T15:29:26.151660: step 76000, loss 0.851383, ranking_acc 0.557617, rating_acc 0.908169\n",
      "\n",
      "Saved model checkpoint to /home/tvromen/research/subtitles2/runs/1510492085/checkpoints/model-76000\n",
      "\n",
      "2017-11-12T15:29:27.668284: step 76100, loss 0.315775, ranking_acc 0.585938->0.585938, rating_acc 0.540443->0.537724, rate 0.000375\n",
      "2017-11-12T15:29:29.101606: step 76200, loss 0.390145, ranking_acc 0.585938->0.585938, rating_acc 0.607374->0.602921, rate 0.000375\n",
      "2017-11-12T15:29:30.549776: step 76300, loss 0.140348, ranking_acc 0.585938->0.585938, rating_acc 0.339743->0.3372, rate 0.000375\n",
      "0...\n",
      "MRR is 0.0034739869467947577\n",
      "Precision@10 is 0.0\n",
      "MRR@10 is 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d28c945f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...\n",
      "MRR is 0.0034739869467947577\n",
      "Precision@10 is 0.0\n",
      "MRR@10 is 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEUNJREFUeJzt3X+sX3V9x/Hna2WwZMmsGzekUrqWWJYVYxg2gH/oTJyj\nBeNVE13RBERn04QuWfaHtnF/mZBhTJalglxZRoBE1zEJehdKKjNR/1ljy0KIBauXiqGkzgoJZqsB\nK+/98T3Mr3e3n++5v/q99D4fyTf9ns/5vM/5nE/uva+ec+733FQVkiSdzW+NewCSpJXNoJAkNRkU\nkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDVdMO4BLIWLL764Nm7cOO5hSNLryuOP\nP/6zqpoY1e+8CIqNGzdy5MiRcQ9Dkl5Xkvy4Tz8vPUmSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1\nGRSSpCaDQpLUdF584E7jtXHPI4uqf/aOG5doJJKWg2cUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigk\nSU0GhSSpqVdQJNmW5FiSmSR75lifJPu69U8muXpUbZLPJ/l+1//hJGu79o1JfpHkie41tRQHKkla\nmJFBkWQNcBewHdgC3JRky6xu24HN3WsncHeP2seAt1TVW4EfAHuHtvdMVV3VvXYt9OAkSYvX54zi\nGmCmqo5X1SvAfmByVp9J4IEaOASsTbKuVVtV36iqM139IWD9EhyPJGmJ9QmKS4HnhpZPdG19+vSp\nBfg48OjQ8qbustO3k7xjrkEl2ZnkSJIjp06d6nEYkqSFGPvN7CSfAc4AX+6aTgIbquoq4G+AryT5\nvdl1VXVPVW2tqq0TExPnbsCStMr0eSjg88BlQ8vru7Y+fX67VZvkY8B7gXdXVQFU1cvAy937x5M8\nA1wBHOkxVknSEutzRnEY2JxkU5ILgR3A9Kw+08DN3W8/XQe8VFUnW7VJtgGfAt5XVadf21CSie4m\nOEkuZ3CD/PiijlKStGAjzyiq6kyS3cBBYA1wb1UdTbKrWz8FHABuAGaA08Ctrdpu03cCFwGPJQE4\n1P2G0zuBzyb5JfAqsKuqXlyqA5YkzU+vv0dRVQcYhMFw29TQ+wJu61vbtb/5LP0fAh7qMy5J0vIb\n+81sSdLKZlBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlq\nMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaD\nQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKmpV1Ak2ZbkWJKZJHvmWJ8k+7r1Tya5elRtks8n+X7X\n/+Eka4fW7e36H0ty/WIPUpK0cCODIska4C5gO7AFuCnJllndtgObu9dO4O4etY8Bb6mqtwI/APZ2\nNVuAHcCVwDbgi912JElj0OeM4hpgpqqOV9UrwH5gclafSeCBGjgErE2yrlVbVd+oqjNd/SFg/dC2\n9lfVy1X1I2Cm244kaQz6BMWlwHNDyye6tj59+tQCfBx4dB77kySdI2O/mZ3kM8AZ4MvzrNuZ5EiS\nI6dOnVqewUmSegXF88BlQ8vru7Y+fZq1ST4GvBf4aFXVPPZHVd1TVVurauvExESPw5AkLcQFPfoc\nBjYn2cTgB/YO4COz+kwDu5PsB64FXqqqk0lOna02yTbgU8CfVtXpWdv6SpK/B97E4Ab5dxd6gDr/\nbdzzyKLqn73jxiUaiXR+GhkUVXUmyW7gILAGuLeqjibZ1a2fAg4ANzC48XwauLVV2236TuAi4LEk\nAIeqale37QeBpxhckrqtqn61ZEcsSZqXPmcUVNUBBmEw3DY19L6A2/rWdu1vbuzvduD2PmOTJC2v\nsd/MliStbAaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKk\nJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoy\nKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaegVFkm1JjiWZSbJnjvVJsq9b/2SSq0fVJvlQkqNJ\nXk2ydah9Y5JfJHmie00t9iAlSQt3wagOSdYAdwHvAU4Ah5NMV9VTQ922A5u717XA3cC1I2q/B3wQ\n+NIcu32mqq5a+GFJkpZKnzOKa4CZqjpeVa8A+4HJWX0mgQdq4BCwNsm6Vm1VPV1Vx5bsSCRJy6JP\nUFwKPDe0fKJr69OnT+1cNnWXnb6d5B09+kuSlsnIS09jcBLYUFUvJHkb8LUkV1bVz4c7JdkJ7ATY\nsGHDGIYpSatDnzOK54HLhpbXd219+vSp/Q1V9XJVvdC9fxx4Brhijn73VNXWqto6MTHR4zAkSQvR\nJygOA5uTbEpyIbADmJ7VZxq4ufvtp+uAl6rqZM/a35BkorsJTpLLGdwgPz6vo5IkLZmRl56q6kyS\n3cBBYA1wb1UdTbKrWz8FHABuAGaA08CtrVqAJB8AvgBMAI8keaKqrgfeCXw2yS+BV4FdVfXiUh60\nJKm/XvcoquoAgzAYbpsael/AbX1ru/aHgYfnaH8IeKjPuCRJy89PZkuSmgwKSVKTQSFJajIoJElN\nBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQ\nSJKaDApJUlOvP4Uqnc827nlkUfXP3nHjEo1EWpk8o5AkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlq\nMigkSU0GhSSpyaCQJDUZFJKkJoNCktTUKyiSbEtyLMlMkj1zrE+Sfd36J5NcPao2yYeSHE3yapKt\ns7a3t+t/LMn1izlASdLijAyKJGuAu4DtwBbgpiRbZnXbDmzuXjuBu3vUfg/4IPCdWfvbAuwArgS2\nAV/stiNJGoM+ZxTXADNVdbyqXgH2A5Oz+kwCD9TAIWBtknWt2qp6uqqOzbG/SWB/Vb1cVT8CZrrt\nSJLGoE9QXAo8N7R8omvr06dP7UL2J0k6R163N7OT7ExyJMmRU6dOjXs4knTe6hMUzwOXDS2v79r6\n9OlTu5D9UVX3VNXWqto6MTExYpOSpIXqExSHgc1JNiW5kMGN5ulZfaaBm7vffroOeKmqTvasnW0a\n2JHkoiSbGNwg/+48jkmStIRG/inUqjqTZDdwEFgD3FtVR5Ps6tZPAQeAGxjceD4N3NqqBUjyAeAL\nwATwSJInqur6btsPAk8BZ4DbqupXS3rUkqTeev3N7Ko6wCAMhtumht4XcFvf2q79YeDhs9TcDtze\nZ2ySpOX1ur2ZLUk6NwwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0Eh\nSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKk\nJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ19QqKJNuSHEsyk2TPHOuTZF+3/skkV4+q\nTfL7SR5L8sPu3zd27RuT/CLJE91raikOVJK0MCODIska4C5gO7AFuCnJllndtgObu9dO4O4etXuA\nb1bVZuCb3fJrnqmqq7rXroUenCRp8fqcUVwDzFTV8ap6BdgPTM7qMwk8UAOHgLVJ1o2onQTu797f\nD7x/kcciSVoGfYLiUuC5oeUTXVufPq3aS6rqZPf+J8AlQ/02dZedvp3kHT3GKElaJheMewAAVVVJ\nqls8CWyoqheSvA34WpIrq+rnwzVJdjK4zMWGDRvO7YAlaRXpc0bxPHDZ0PL6rq1Pn1btf3WXp+j+\n/SlAVb1cVS907x8HngGumD2oqrqnqrZW1daJiYkehyFJWog+QXEY2JxkU5ILgR3A9Kw+08DN3W8/\nXQe81F1WatVOA7d0728Bvg6QZKK7CU6SyxncID++4COUJC3KyEtPVXUmyW7gILAGuLeqjibZ1a2f\nAg4ANwAzwGng1lZtt+k7gAeTfAL4MfDhrv2dwGeT/BJ4FdhVVS8uydFKkuat1z2KqjrAIAyG26aG\n3hdwW9/arv0F4N1ztD8EPNRnXNL5YOOeRxZV/+wdNy7RSKS5+clsSVKTQSFJajIoJElNBoUkqcmg\nkCQ1rYhPZkuvZ4v9rSVppfOMQpLUZFBIkpq89CStcn7gT6N4RiFJajIoJElNBoUkqcmgkCQ1eTNb\nep3zcxxabp5RSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktTkB+4kjZVPr135PKOQ\nJDUZFJKkJoNCktTkPQpJi+JDCc9/nlFIkpoMCklSk5eeJK1q4/713HHvv49eZxRJtiU5lmQmyZ45\n1ifJvm79k0muHlWb5PeTPJbkh92/bxxat7frfyzJ9Ys9SEnSwo08o0iyBrgLeA9wAjicZLqqnhrq\nth3Y3L2uBe4Grh1Ruwf4ZlXd0QXIHuDTSbYAO4ArgTcB/57kiqr61dIcsqTzybhvpo97/+dCnzOK\na4CZqjpeVa8A+4HJWX0mgQdq4BCwNsm6EbWTwP3d+/uB9w+176+ql6vqR8BMtx1J0hj0CYpLgeeG\nlk90bX36tGovqaqT3fufAJfMY3+SpHNkRdzMrqpKUvOpSbIT2Nkt/neSY0Or3wC8NI/li4GfzWf/\nPc3ez1LVjOpztvVztc9nrpZlnvK5s45tlMXO1WLmaXbbSv6a6lu3XHPl99/Z2xf9/dd9/yzUH/bq\nVVXNF/B24ODQ8l5g76w+XwJuGlo+Bqxr1b7Wp3u/Djg21/aBg8DbR41z1njumefykflsf6HjWKqa\nUX3Otn6u9vnM1XLN07jmajHzNMfcrNivqXHPld9/SzNXy/n9N+rV59LTYWBzkk1JLmRwo3l6Vp9p\n4Obut5+uA16qwWWlVu00cEv3/hbg60PtO5JclGQTgxvk3+0xzmH/Ns/l5bKQ/fSpGdXnbOvnal/N\nc7WYeZrdtpLnqW/dcs3Vavqaaq1fyXPVlC6p2p2SG4B/ANYA91bV7Ul2AVTVVJIAdwLbgNPArVV1\n5Gy1XfsfAA8CG4AfAx+uqhe7dZ8BPg6cAf66qh5dukOe8/iOVNXW5dzH+cB56s+56s+56mec89Qr\nKM53SXZW1T3jHsdK5zz151z151z1M855MigkSU0+60mS1GRQSJKaDApJUpNBMUuS301yf5J/TPLR\ncY9nJUtyeZJ/SvLVcY9lpUvy/u5r6l+S/Pm4x7NSJfnjJFNJ/jXJX457PCtd9/PqSJL3Lud+VkVQ\nJLk3yU+TfG9W+1xPtv0g8NWq+iTwvnM+2DGbz1zV4BlenxjPSMdvnnP1te5rahfwF+MY77jMc56e\nrqrX5mjVPTl6nj+rAD7N4GMGy2pVBAVwH4PPePyfoSfbbge2ADd1T65dz6+fNbUan1h7H/3narW7\nj/nP1d9261eT+5jHPCV5H3CAwUNEV5v76DlXSd4DPAX8dLkHtSqCoqq+A7w4q/lsT7Y9wSAsYJXM\nz7B5ztWqNp+56p5a8Dng0ar6z3M91nGa79dUVU1X1TZ+/eSGVWOec/Uu4DrgI8Ankyzbz6sV8VDA\nMZnrKbXXAvuAO5PcyAr5+PwKMOdcdZ+uvx34kyR7q+rvxjK6leVsX1d/BfwZ8IYkb66qqXEMbgU5\n29fUuxhc/v0d4Fvnflgr0pxzVVW7AZJ8DPhZVb26XANYzUExp6r6H+DWcY/j9aCqXmBwzV0jVNU+\nBv8JUUNVfQsDYl6q6r7l3sequ7Qy5HngsqHl9V2b/j/nqj/nqh/nqb+xz9VqDoo+T8XVgHPVn3PV\nj/PU39jnalUERZJ/Bv4D+KMkJ5J8oqrOALsZ/L2Lp4EHq+roOMe5EjhX/TlX/ThP/a3UufKhgJKk\nplVxRiFJWjiDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqSm/wVAoUVqDU31OgAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d0018aa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = runall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-12T15:29:50.261979\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.runall.<locals>.<lambda>>,\n",
       "            {1e-06: defaultdict(list,\n",
       "                         {'accuracy': [(0.24489795,\n",
       "                            0.55761719,\n",
       "                            0.37908566,\n",
       "                            0.90816867)],\n",
       "                          'mrr': [0.0034739869467947577,\n",
       "                           0.0034739869467947577],\n",
       "                          'mrr_at_10': [0.0, 0.0],\n",
       "                          'precision_at_10': [0.0, 0.0]})})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# loss: ranking 0%, rating 100%\n",
    "# all star ratings are considered watched\n",
    "# centered the rating data around 0\n",
    "# lambda = 5e-6\n",
    "# 10 epochs\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.runall.<locals>.<lambda>>,\n",
       "            {1e-06: defaultdict(list,\n",
       "                         {'accuracy': [(0.46938777,\n",
       "                            0.61328125,\n",
       "                            0.38906783,\n",
       "                            0.90203112)],\n",
       "                          'mrr': [0.0062781847316484632,\n",
       "                           0.0062781847316484632],\n",
       "                          'mrr_at_10': [0.0025000000000000001,\n",
       "                           0.0025000000000000001],\n",
       "                          'precision_at_10': [0.02, 0.02]})})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# ranking and rating 50% each\n",
    "# all star ratings are considered watched\n",
    "# centered the rating data around 0\n",
    "# lambda = 5e-6\n",
    "# 10 epochs\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.runall.<locals>.<lambda>>,\n",
       "            {1e-06: defaultdict(list,\n",
       "                         {'accuracy': [(0.75510204,\n",
       "                            0.70117188,\n",
       "                            4.3681498,\n",
       "                            2.5989952)],\n",
       "                          'mrr': [0.013684247847832686, 0.013684247847832686],\n",
       "                          'mrr_at_10': [0.0033333333333333331,\n",
       "                           0.0033333333333333331],\n",
       "                          'precision_at_10': [0.02, 0.02]})})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# loss: ranking 100%, rating 0%\n",
    "# all star ratings are considered watched\n",
    "# centered the rating data around 0\n",
    "# lambda = 5e-6\n",
    "# 10 epochs\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.runall.<locals>.<lambda>>,\n",
       "            {1e-06: defaultdict(list,\n",
       "                         {'accuracy': [(0.24489795,\n",
       "                            0.55761719,\n",
       "                            0.37908566,\n",
       "                            0.90816867)],\n",
       "                          'mrr': [0.0034739869467947577,\n",
       "                           0.0034739869467947577],\n",
       "                          'mrr_at_10': [0.0, 0.0],\n",
       "                          'precision_at_10': [0.0, 0.0]})})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1M lines\n",
    "# loss: ranking 0%, rating 100%\n",
    "# all star ratings are considered watched\n",
    "# centered the rating data around 0\n",
    "# lambda = 5e-6\n",
    "# 10 epochs\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
